{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a0780df-d9fa-43db-bb58-bdd0a48e19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Courses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8ea72d5-73b6-4035-b5b3-52f4d1b86dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Courses.csv\")\n",
    "\n",
    "# 1. Remove specified columns\n",
    "columns_to_remove = ['nplay_video', 'incomplete_flag', 'roles']\n",
    "df_cleaned = df.drop(columns=columns_to_remove)\n",
    "\n",
    "# 2. Handle missing values\n",
    "# For categorical variables (LoE_DI) - impute with mode\n",
    "df_cleaned['LoE_DI'] = df_cleaned['LoE_DI'].fillna(df_cleaned['LoE_DI'].mode()[0])\n",
    "\n",
    "# For gender in random way\n",
    "missing_indices = df_cleaned[df_cleaned['gender'].isnull()].index\n",
    "df_cleaned.loc[missing_indices, 'gender'] = np.random.choice(df_cleaned['gender'].dropna(), len(missing_indices))\n",
    "\n",
    "# For YoB - impute with median\n",
    "df_cleaned['YoB'] = df_cleaned['YoB'].fillna(df_cleaned['YoB'].median())\n",
    "\n",
    "# 3. Drop rows with missing grades\n",
    "df_cleaned = df_cleaned.dropna(subset=['grade'])\n",
    "\n",
    "# 5. Define function for nchapters imputation\n",
    "def impute_nchapters_simple(df):\n",
    "    # Create copy of original nchapters\n",
    "    df['nchapters_imputed'] = df['nchapters'].copy()\n",
    "    \n",
    "    # Imputation logic based on certification and activity days\n",
    "    mask = df['nchapters'].isna()\n",
    "    \n",
    "    conditions = [\n",
    "        # Condition 1: Certified students\n",
    "        (mask) & (df['certified'] == 1),\n",
    "        # Condition 2: Not certified but active (ndays_act > 3)\n",
    "        (mask) & (df['certified'] == 0) & (df['ndays_act'] > 3),\n",
    "        # Condition 3: Not certified and some activity (ndays_act <= 3)\n",
    "        (mask) & (df['certified'] == 0) & (df['ndays_act'] <= 3),\n",
    "        # Condition 4: No activity recorded (ndays_act is NaN) but viewed course\n",
    "        (mask) & (df['ndays_act'].isna()) & (df['viewed'] == 1),\n",
    "        # Condition 5: No activity and never viewed (complete non-engagement)\n",
    "        (mask) & (df['ndays_act'].isna()) & (df['viewed'] == 0)\n",
    "    ]\n",
    "    \n",
    "    values = [\n",
    "        16,  # Average for certified students\n",
    "        3,   # Average for non-certified active students\n",
    "        1,   # Minimal engagement\n",
    "        1,   # Viewed but no sustained activity\n",
    "        0    # Never engaged with content\n",
    "    ]\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Apply imputation\n",
    "    df['nchapters_imputed'] = np.select(conditions, values, df['nchapters_imputed'])\n",
    "    \n",
    "    return df['nchapters_imputed']\n",
    "\n",
    "# Apply nchapters imputation\n",
    "df_cleaned['nchapters_imputed'] = impute_nchapters_simple(df_cleaned)\n",
    "\n",
    "# 6. nevents missing data\n",
    "def impute_nevents(row):\n",
    "    if pd.isna(row['nevents']):  # Only impute if the value is missing\n",
    "        if row['nchapters'] == 0:\n",
    "            return 0\n",
    "        elif row['nchapters'] == 1:\n",
    "            return np.random.choice([0, 1], p=[0.7, 0.3])\n",
    "        elif 2 <= row['nchapters'] <= 5:\n",
    "            return np.random.choice([0, 1, 2], p=[0.5, 0.3, 0.2])\n",
    "        elif 6 <= row['nchapters'] <= 12:\n",
    "            return np.random.choice([1, 2, 3], p=[0.4, 0.4, 0.2])\n",
    "        else:\n",
    "            return np.random.choice([2, 3, 4], p=[0.3, 0.4, 0.3])\n",
    "    else:\n",
    "        return row['nevents']  # Return original value if not missing\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create new column with imputed values\n",
    "df_cleaned['nevents_imputed'] = df_cleaned.apply(impute_nevents, axis=1)\n",
    "\n",
    "# 7. Save the cleaned dataset with all changes\n",
    "df_cleaned.to_csv('cleaned_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e5a553ce-f4c6-45cd-ad64-bf78c3be2921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflowNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading flatbuffers-24.12.23-py2.py3-none-any.whl.metadata (876 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading grpcio-1.69.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\asus\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.3.5)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading optree-0.13.1-cp312-cp312-win_amd64.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.7/48.7 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl (390.3 MB)\n",
      "   ---------------------------------------- 0.0/390.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.4/390.3 MB 8.9 MB/s eta 0:00:44\n",
      "   ---------------------------------------- 1.1/390.3 MB 13.4 MB/s eta 0:00:30\n",
      "   ---------------------------------------- 1.7/390.3 MB 12.0 MB/s eta 0:00:33\n",
      "   ---------------------------------------- 2.3/390.3 MB 12.4 MB/s eta 0:00:32\n",
      "   ---------------------------------------- 3.0/390.3 MB 12.6 MB/s eta 0:00:31\n",
      "   ---------------------------------------- 3.6/390.3 MB 12.7 MB/s eta 0:00:31\n",
      "   ---------------------------------------- 4.2/390.3 MB 13.4 MB/s eta 0:00:29\n",
      "   ---------------------------------------- 4.8/390.3 MB 12.9 MB/s eta 0:00:30\n",
      "    --------------------------------------- 5.5/390.3 MB 12.9 MB/s eta 0:00:30\n",
      "    --------------------------------------- 6.1/390.3 MB 13.0 MB/s eta 0:00:30\n",
      "    --------------------------------------- 6.7/390.3 MB 13.0 MB/s eta 0:00:30\n",
      "    --------------------------------------- 7.3/390.3 MB 13.0 MB/s eta 0:00:30\n",
      "    --------------------------------------- 8.0/390.3 MB 13.0 MB/s eta 0:00:30\n",
      "    --------------------------------------- 8.6/390.3 MB 13.1 MB/s eta 0:00:30\n",
      "    --------------------------------------- 9.2/390.3 MB 13.1 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 9.8/390.3 MB 13.1 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 10.5/390.3 MB 13.4 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 11.1/390.3 MB 13.4 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 11.7/390.3 MB 13.4 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 12.3/390.3 MB 13.4 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 13.0/390.3 MB 13.1 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 13.6/390.3 MB 13.4 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 14.2/390.3 MB 13.4 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 14.8/390.3 MB 12.8 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 15.4/390.3 MB 13.4 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 16.1/390.3 MB 13.1 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 16.7/390.3 MB 13.4 MB/s eta 0:00:28\n",
      "   - -------------------------------------- 17.4/390.3 MB 13.1 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 18.0/390.3 MB 13.1 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 18.6/390.3 MB 13.1 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 19.3/390.3 MB 13.1 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 19.9/390.3 MB 13.4 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 20.5/390.3 MB 13.1 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 21.1/390.3 MB 12.9 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 21.6/390.3 MB 13.1 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 22.2/390.3 MB 13.1 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 22.8/390.3 MB 13.4 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 23.4/390.3 MB 13.4 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 24.1/390.3 MB 13.4 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 24.7/390.3 MB 13.4 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 25.3/390.3 MB 13.4 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 25.9/390.3 MB 13.4 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 26.6/390.3 MB 13.4 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 27.2/390.3 MB 13.4 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 27.8/390.3 MB 13.4 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 28.4/390.3 MB 13.4 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 29.1/390.3 MB 13.1 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 29.7/390.3 MB 13.4 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 30.3/390.3 MB 13.1 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 30.9/390.3 MB 13.1 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 31.6/390.3 MB 13.4 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 32.2/390.3 MB 13.1 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 32.8/390.3 MB 13.4 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 33.4/390.3 MB 13.1 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 34.0/390.3 MB 12.8 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 34.7/390.3 MB 13.4 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 35.3/390.3 MB 13.1 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 35.9/390.3 MB 13.4 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 36.5/390.3 MB 13.1 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 37.2/390.3 MB 13.1 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 37.8/390.3 MB 13.1 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 38.4/390.3 MB 13.1 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 39.0/390.3 MB 13.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 39.6/390.3 MB 13.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 40.3/390.3 MB 13.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 40.9/390.3 MB 13.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 41.5/390.3 MB 13.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 42.1/390.3 MB 13.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 42.6/390.3 MB 13.4 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 43.3/390.3 MB 13.4 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 43.9/390.3 MB 13.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 44.5/390.3 MB 13.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 45.1/390.3 MB 13.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 45.8/390.3 MB 13.4 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 46.4/390.3 MB 13.4 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 47.0/390.3 MB 13.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 47.6/390.3 MB 13.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 48.2/390.3 MB 13.4 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 48.8/390.3 MB 13.1 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 49.4/390.3 MB 13.4 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 50.0/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 50.7/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 51.3/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 51.9/390.3 MB 13.4 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 52.5/390.3 MB 13.4 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 53.2/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 53.8/390.3 MB 13.4 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 54.4/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 55.1/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 55.7/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 56.3/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 56.9/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 57.5/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 58.2/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 58.8/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 59.4/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 60.0/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 60.6/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 61.3/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 61.9/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 62.5/390.3 MB 13.1 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 63.2/390.3 MB 13.1 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 63.8/390.3 MB 13.4 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 64.4/390.3 MB 13.1 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 65.0/390.3 MB 13.4 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 65.7/390.3 MB 13.4 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 66.3/390.3 MB 13.4 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 66.9/390.3 MB 13.4 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 67.5/390.3 MB 13.4 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 68.2/390.3 MB 13.1 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 68.8/390.3 MB 13.4 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 69.4/390.3 MB 13.1 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 70.1/390.3 MB 13.1 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 70.7/390.3 MB 13.4 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 71.3/390.3 MB 13.1 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 71.9/390.3 MB 13.1 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 72.6/390.3 MB 13.1 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 73.2/390.3 MB 13.1 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 73.7/390.3 MB 13.1 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 74.4/390.3 MB 13.4 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 75.0/390.3 MB 13.4 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 75.6/390.3 MB 13.1 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 76.2/390.3 MB 13.4 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 76.9/390.3 MB 13.4 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 77.5/390.3 MB 13.4 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 78.1/390.3 MB 13.4 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 78.7/390.3 MB 13.1 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 79.3/390.3 MB 13.4 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 80.0/390.3 MB 13.1 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 80.6/390.3 MB 13.1 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 81.2/390.3 MB 13.4 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 81.8/390.3 MB 13.1 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 82.5/390.3 MB 13.1 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 83.1/390.3 MB 13.1 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 83.7/390.3 MB 13.1 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 84.3/390.3 MB 13.1 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 85.0/390.3 MB 13.1 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 85.6/390.3 MB 13.1 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 86.2/390.3 MB 13.4 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 86.9/390.3 MB 13.4 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 87.5/390.3 MB 13.1 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 88.1/390.3 MB 13.4 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 88.7/390.3 MB 13.1 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 89.4/390.3 MB 13.4 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 90.0/390.3 MB 13.4 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 90.6/390.3 MB 13.4 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 91.2/390.3 MB 13.1 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 91.9/390.3 MB 13.4 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 92.5/390.3 MB 13.4 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 93.1/390.3 MB 13.1 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 93.9/390.3 MB 13.1 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 94.6/390.3 MB 13.4 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 95.2/390.3 MB 13.1 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 95.8/390.3 MB 13.4 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 96.4/390.3 MB 13.1 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 97.0/390.3 MB 13.1 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 97.7/390.3 MB 13.1 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 98.3/390.3 MB 13.1 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 98.9/390.3 MB 13.1 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 99.6/390.3 MB 13.1 MB/s eta 0:00:23\n",
      "   ---------- ---------------------------- 100.2/390.3 MB 13.1 MB/s eta 0:00:23\n",
      "   ---------- ---------------------------- 100.7/390.3 MB 13.4 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 101.3/390.3 MB 13.1 MB/s eta 0:00:23\n",
      "   ---------- ---------------------------- 101.9/390.3 MB 13.1 MB/s eta 0:00:23\n",
      "   ---------- ---------------------------- 102.6/390.3 MB 13.4 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 103.2/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 103.8/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 104.4/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 105.1/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 105.7/390.3 MB 13.4 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 106.3/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 107.0/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 107.6/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 108.2/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 108.8/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 109.5/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ----------- --------------------------- 110.1/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ----------- --------------------------- 110.7/390.3 MB 13.4 MB/s eta 0:00:21\n",
      "   ----------- --------------------------- 111.3/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ----------- --------------------------- 112.0/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ----------- --------------------------- 112.6/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ----------- --------------------------- 113.2/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ----------- --------------------------- 113.8/390.3 MB 13.4 MB/s eta 0:00:21\n",
      "   ----------- --------------------------- 114.5/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ----------- --------------------------- 115.1/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ----------- --------------------------- 115.7/390.3 MB 13.4 MB/s eta 0:00:21\n",
      "   ----------- --------------------------- 116.3/390.3 MB 13.1 MB/s eta 0:00:21\n",
      "   ----------- --------------------------- 116.9/390.3 MB 13.4 MB/s eta 0:00:21\n",
      "   ----------- --------------------------- 117.6/390.3 MB 13.1 MB/s eta 0:00:21\n",
      "   ----------- --------------------------- 118.2/390.3 MB 13.1 MB/s eta 0:00:21\n",
      "   ----------- --------------------------- 118.8/390.3 MB 13.1 MB/s eta 0:00:21\n",
      "   ----------- --------------------------- 119.4/390.3 MB 13.4 MB/s eta 0:00:21\n",
      "   ----------- --------------------------- 120.1/390.3 MB 13.4 MB/s eta 0:00:21\n",
      "   ------------ -------------------------- 120.7/390.3 MB 13.4 MB/s eta 0:00:21\n",
      "   ------------ -------------------------- 121.3/390.3 MB 13.1 MB/s eta 0:00:21\n",
      "   ------------ -------------------------- 121.9/390.3 MB 13.4 MB/s eta 0:00:21\n",
      "   ------------ -------------------------- 122.6/390.3 MB 13.4 MB/s eta 0:00:21\n",
      "   ------------ -------------------------- 123.2/390.3 MB 13.1 MB/s eta 0:00:21\n",
      "   ------------ -------------------------- 123.8/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------ -------------------------- 124.4/390.3 MB 13.1 MB/s eta 0:00:21\n",
      "   ------------ -------------------------- 125.1/390.3 MB 13.1 MB/s eta 0:00:21\n",
      "   ------------ -------------------------- 125.7/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------ -------------------------- 126.3/390.3 MB 13.1 MB/s eta 0:00:21\n",
      "   ------------ -------------------------- 126.8/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------ -------------------------- 127.5/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------ -------------------------- 128.1/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------ -------------------------- 128.7/390.3 MB 13.1 MB/s eta 0:00:20\n",
      "   ------------ -------------------------- 129.3/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------ -------------------------- 130.0/390.3 MB 13.1 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 130.6/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 131.2/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 131.8/390.3 MB 13.1 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 132.4/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 133.1/390.3 MB 13.1 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 133.7/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 134.3/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 134.9/390.3 MB 13.1 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 135.5/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 136.2/390.3 MB 13.1 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 136.8/390.3 MB 13.1 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 137.4/390.3 MB 13.4 MB/s eta 0:00:19\n",
      "   ------------- ------------------------- 138.1/390.3 MB 13.1 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 138.7/390.3 MB 13.1 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 139.3/390.3 MB 13.1 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 139.8/390.3 MB 13.4 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 140.4/390.3 MB 13.1 MB/s eta 0:00:20\n",
      "   -------------- ------------------------ 141.0/390.3 MB 13.1 MB/s eta 0:00:20\n",
      "   -------------- ------------------------ 141.6/390.3 MB 13.1 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 142.2/390.3 MB 13.1 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 142.9/390.3 MB 13.4 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 143.5/390.3 MB 13.1 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 144.1/390.3 MB 13.4 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 144.7/390.3 MB 13.4 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 145.4/390.3 MB 13.4 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 146.0/390.3 MB 13.4 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 146.6/390.3 MB 13.1 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 147.3/390.3 MB 13.4 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 147.9/390.3 MB 13.4 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 148.4/390.3 MB 13.4 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 149.0/390.3 MB 13.4 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 149.6/390.3 MB 13.1 MB/s eta 0:00:19\n",
      "   --------------- ----------------------- 150.3/390.3 MB 13.4 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 150.9/390.3 MB 13.1 MB/s eta 0:00:19\n",
      "   --------------- ----------------------- 151.5/390.3 MB 13.1 MB/s eta 0:00:19\n",
      "   --------------- ----------------------- 152.2/390.3 MB 13.4 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 152.8/390.3 MB 13.1 MB/s eta 0:00:19\n",
      "   --------------- ----------------------- 153.3/390.3 MB 13.1 MB/s eta 0:00:19\n",
      "   --------------- ----------------------- 154.0/390.3 MB 13.1 MB/s eta 0:00:19\n",
      "   --------------- ----------------------- 154.6/390.3 MB 13.4 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 155.2/390.3 MB 13.4 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 155.9/390.3 MB 13.1 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 156.5/390.3 MB 13.4 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 157.1/390.3 MB 13.4 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 157.7/390.3 MB 13.1 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 158.4/390.3 MB 13.1 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 159.0/390.3 MB 13.1 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 159.6/390.3 MB 13.1 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 160.2/390.3 MB 13.4 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 160.9/390.3 MB 13.1 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 161.5/390.3 MB 13.4 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 162.1/390.3 MB 13.4 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 162.8/390.3 MB 13.1 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 163.4/390.3 MB 13.4 MB/s eta 0:00:17\n",
      "   ---------------- ---------------------- 164.0/390.3 MB 13.1 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 164.6/390.3 MB 13.1 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 165.2/390.3 MB 13.1 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 165.9/390.3 MB 13.1 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 166.6/390.3 MB 13.4 MB/s eta 0:00:17\n",
      "   ---------------- ---------------------- 167.2/390.3 MB 13.4 MB/s eta 0:00:17\n",
      "   ---------------- ---------------------- 167.9/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ---------------- ---------------------- 168.5/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ---------------- ---------------------- 169.1/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ---------------- ---------------------- 169.7/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 170.4/390.3 MB 13.4 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 171.0/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 171.6/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 172.2/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 172.9/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 173.5/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 174.1/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 174.6/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 175.3/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 176.0/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 176.6/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 177.2/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 177.8/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 178.5/390.3 MB 13.4 MB/s eta 0:00:16\n",
      "   ----------------- --------------------- 179.1/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 179.7/390.3 MB 13.4 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 180.3/390.3 MB 13.1 MB/s eta 0:00:17\n",
      "   ------------------ -------------------- 181.0/390.3 MB 13.1 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 181.6/390.3 MB 13.1 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 182.2/390.3 MB 13.4 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 182.8/390.3 MB 13.4 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 183.5/390.3 MB 13.1 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 184.1/390.3 MB 13.1 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 184.7/390.3 MB 13.1 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 185.4/390.3 MB 13.1 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 186.0/390.3 MB 13.1 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 186.6/390.3 MB 13.4 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 187.2/390.3 MB 13.1 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 187.9/390.3 MB 13.1 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 188.5/390.3 MB 13.1 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 189.1/390.3 MB 13.4 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 189.7/390.3 MB 13.1 MB/s eta 0:00:16\n",
      "   ------------------- ------------------- 190.3/390.3 MB 13.1 MB/s eta 0:00:16\n",
      "   ------------------- ------------------- 191.0/390.3 MB 13.1 MB/s eta 0:00:16\n",
      "   ------------------- ------------------- 191.6/390.3 MB 13.1 MB/s eta 0:00:16\n",
      "   ------------------- ------------------- 192.2/390.3 MB 13.1 MB/s eta 0:00:16\n",
      "   ------------------- ------------------- 192.7/390.3 MB 13.4 MB/s eta 0:00:15\n",
      "   ------------------- ------------------- 193.4/390.3 MB 13.4 MB/s eta 0:00:15\n",
      "   ------------------- ------------------- 194.0/390.3 MB 13.1 MB/s eta 0:00:15\n",
      "   ------------------- ------------------- 194.6/390.3 MB 13.1 MB/s eta 0:00:15\n",
      "   ------------------- ------------------- 195.2/390.3 MB 13.1 MB/s eta 0:00:15\n",
      "   ------------------- ------------------- 195.9/390.3 MB 13.4 MB/s eta 0:00:15\n",
      "   ------------------- ------------------- 196.5/390.3 MB 13.1 MB/s eta 0:00:15\n",
      "   ------------------- ------------------- 197.1/390.3 MB 13.1 MB/s eta 0:00:15\n",
      "   ------------------- ------------------- 197.7/390.3 MB 13.1 MB/s eta 0:00:15\n",
      "   ------------------- ------------------- 198.4/390.3 MB 13.1 MB/s eta 0:00:15\n",
      "   ------------------- ------------------- 199.0/390.3 MB 13.4 MB/s eta 0:00:15\n",
      "   ------------------- ------------------- 199.6/390.3 MB 13.1 MB/s eta 0:00:15\n",
      "   -------------------- ------------------ 200.3/390.3 MB 13.1 MB/s eta 0:00:15\n",
      "   -------------------- ------------------ 200.9/390.3 MB 13.1 MB/s eta 0:00:15\n",
      "   -------------------- ------------------ 201.4/390.3 MB 13.1 MB/s eta 0:00:15\n",
      "   -------------------- ------------------ 202.1/390.3 MB 13.1 MB/s eta 0:00:15\n",
      "   -------------------- ------------------ 202.8/390.3 MB 13.4 MB/s eta 0:00:15\n",
      "   -------------------- ------------------ 203.4/390.3 MB 13.1 MB/s eta 0:00:15\n",
      "   -------------------- ------------------ 204.0/390.3 MB 13.1 MB/s eta 0:00:15\n",
      "   -------------------- ------------------ 204.6/390.3 MB 13.4 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 205.3/390.3 MB 13.4 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 205.9/390.3 MB 13.1 MB/s eta 0:00:15\n",
      "   -------------------- ------------------ 206.5/390.3 MB 13.4 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 207.2/390.3 MB 13.4 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 207.8/390.3 MB 13.4 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 208.4/390.3 MB 13.1 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 209.0/390.3 MB 13.1 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 209.6/390.3 MB 13.4 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 210.3/390.3 MB 13.4 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 210.9/390.3 MB 13.4 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 211.5/390.3 MB 13.1 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 212.2/390.3 MB 13.1 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 212.8/390.3 MB 13.4 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 213.4/390.3 MB 13.1 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 214.0/390.3 MB 13.1 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 214.7/390.3 MB 13.1 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 215.3/390.3 MB 13.1 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 215.9/390.3 MB 13.1 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 216.5/390.3 MB 13.1 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 217.1/390.3 MB 13.1 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 217.8/390.3 MB 13.1 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 218.4/390.3 MB 13.1 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 219.1/390.3 MB 13.4 MB/s eta 0:00:13\n",
      "   --------------------- ----------------- 219.8/390.3 MB 13.1 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 220.4/390.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 221.0/390.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 221.6/390.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 222.3/390.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 222.9/390.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 223.5/390.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 224.1/390.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 224.8/390.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 225.4/390.3 MB 13.4 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 226.0/390.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 226.6/390.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 227.3/390.3 MB 13.4 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 228.0/390.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 228.7/390.3 MB 13.4 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 229.4/390.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 230.0/390.3 MB 13.4 MB/s eta 0:00:12\n",
      "   ----------------------- --------------- 230.6/390.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ----------------------- --------------- 231.2/390.3 MB 13.4 MB/s eta 0:00:12\n",
      "   ----------------------- --------------- 231.8/390.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ----------------------- --------------- 232.5/390.3 MB 13.1 MB/s eta 0:00:13\n",
      "   ----------------------- --------------- 233.1/390.3 MB 13.4 MB/s eta 0:00:12\n",
      "   ----------------------- --------------- 233.7/390.3 MB 13.4 MB/s eta 0:00:12\n",
      "   ----------------------- --------------- 234.4/390.3 MB 13.1 MB/s eta 0:00:12\n",
      "   ----------------------- --------------- 235.0/390.3 MB 13.4 MB/s eta 0:00:12\n",
      "   ----------------------- --------------- 235.6/390.3 MB 13.4 MB/s eta 0:00:12\n",
      "   ----------------------- --------------- 236.2/390.3 MB 13.4 MB/s eta 0:00:12\n",
      "   ----------------------- --------------- 236.8/390.3 MB 13.4 MB/s eta 0:00:12\n",
      "   ----------------------- --------------- 237.5/390.3 MB 13.1 MB/s eta 0:00:12\n",
      "   ----------------------- --------------- 238.1/390.3 MB 13.1 MB/s eta 0:00:12\n",
      "   ----------------------- --------------- 238.7/390.3 MB 13.4 MB/s eta 0:00:12\n",
      "   ----------------------- --------------- 239.3/390.3 MB 13.1 MB/s eta 0:00:12\n",
      "   ----------------------- --------------- 240.0/390.3 MB 13.4 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 240.6/390.3 MB 13.1 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 241.2/390.3 MB 13.4 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 241.9/390.3 MB 13.4 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 242.5/390.3 MB 13.1 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 243.1/390.3 MB 13.4 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 243.8/390.3 MB 13.4 MB/s eta 0:00:11\n",
      "   ------------------------ -------------- 244.4/390.3 MB 13.1 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 245.0/390.3 MB 13.4 MB/s eta 0:00:11\n",
      "   ------------------------ -------------- 245.6/390.3 MB 13.4 MB/s eta 0:00:11\n",
      "   ------------------------ -------------- 246.3/390.3 MB 13.4 MB/s eta 0:00:11\n",
      "   ------------------------ -------------- 246.9/390.3 MB 13.4 MB/s eta 0:00:11\n",
      "   ------------------------ -------------- 247.5/390.3 MB 13.1 MB/s eta 0:00:11\n",
      "   ------------------------ -------------- 248.1/390.3 MB 13.1 MB/s eta 0:00:11\n",
      "   ------------------------ -------------- 248.8/390.3 MB 13.4 MB/s eta 0:00:11\n",
      "   ------------------------ -------------- 249.4/390.3 MB 13.4 MB/s eta 0:00:11\n",
      "   ------------------------ -------------- 250.0/390.3 MB 13.1 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 250.6/390.3 MB 13.1 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 251.3/390.3 MB 13.1 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 251.9/390.3 MB 13.1 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 252.5/390.3 MB 13.1 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 253.1/390.3 MB 13.1 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 253.8/390.3 MB 13.1 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 254.4/390.3 MB 13.1 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 255.1/390.3 MB 13.1 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 255.8/390.3 MB 13.4 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 256.4/390.3 MB 13.4 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 257.0/390.3 MB 13.4 MB/s eta 0:00:10\n",
      "   ------------------------- ------------- 257.6/390.3 MB 13.4 MB/s eta 0:00:10\n",
      "   ------------------------- ------------- 258.2/390.3 MB 13.1 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 258.9/390.3 MB 13.1 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 259.5/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   ------------------------- ------------- 260.1/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 260.8/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 261.4/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 262.0/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 262.6/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 263.2/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 263.9/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 264.5/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 265.1/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 265.7/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 266.4/390.3 MB 13.4 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 267.0/390.3 MB 13.4 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 267.6/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 268.2/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 268.9/390.3 MB 13.4 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 269.4/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 270.1/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 270.7/390.3 MB 13.4 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 271.3/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 271.9/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 272.5/390.3 MB 13.1 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 273.1/390.3 MB 13.4 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 273.7/390.3 MB 13.1 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 274.4/390.3 MB 13.1 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 275.0/390.3 MB 13.1 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 275.6/390.3 MB 13.1 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 276.2/390.3 MB 13.1 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 276.9/390.3 MB 13.1 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 277.5/390.3 MB 13.1 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 278.1/390.3 MB 13.4 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 278.7/390.3 MB 13.1 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 279.4/390.3 MB 13.1 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 280.0/390.3 MB 13.4 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 280.6/390.3 MB 13.4 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 281.1/390.3 MB 13.1 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 281.9/390.3 MB 13.1 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 282.5/390.3 MB 13.4 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 283.1/390.3 MB 13.4 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 283.7/390.3 MB 13.1 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 284.4/390.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ---------------------------- ---------- 284.9/390.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ---------------------------- ---------- 285.7/390.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ---------------------------- ---------- 286.3/390.3 MB 13.1 MB/s eta 0:00:08\n",
      "   ---------------------------- ---------- 286.9/390.3 MB 13.1 MB/s eta 0:00:08\n",
      "   ---------------------------- ---------- 287.5/390.3 MB 13.1 MB/s eta 0:00:08\n",
      "   ---------------------------- ---------- 288.1/390.3 MB 13.1 MB/s eta 0:00:08\n",
      "   ---------------------------- ---------- 288.8/390.3 MB 13.1 MB/s eta 0:00:08\n",
      "   ---------------------------- ---------- 289.6/390.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 290.2/390.3 MB 13.1 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 290.8/390.3 MB 13.1 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 291.4/390.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 292.1/390.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 292.7/390.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 293.3/390.3 MB 13.1 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 293.9/390.3 MB 13.1 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 294.6/390.3 MB 13.1 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 295.2/390.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 295.8/390.3 MB 13.1 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 296.5/390.3 MB 13.4 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 297.1/390.3 MB 13.1 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 297.7/390.3 MB 13.4 MB/s eta 0:00:07\n",
      "   ----------------------------- --------- 298.3/390.3 MB 13.4 MB/s eta 0:00:07\n",
      "   ----------------------------- --------- 299.0/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ----------------------------- --------- 299.6/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 300.2/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 300.9/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 301.5/390.3 MB 13.4 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 302.1/390.3 MB 13.4 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 302.7/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 303.4/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 304.0/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 304.6/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 305.2/390.3 MB 13.4 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 305.9/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 306.5/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 307.1/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 307.7/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 308.5/390.3 MB 13.4 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 309.1/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 309.7/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 310.3/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 311.0/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 311.6/390.3 MB 13.1 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 312.2/390.3 MB 13.4 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 312.8/390.3 MB 13.1 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 313.5/390.3 MB 13.1 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 314.1/390.3 MB 13.1 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 314.7/390.3 MB 13.1 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 315.4/390.3 MB 13.1 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 316.0/390.3 MB 13.1 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 316.6/390.3 MB 13.1 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 317.2/390.3 MB 13.4 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 317.8/390.3 MB 13.4 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 318.5/390.3 MB 13.1 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 319.1/390.3 MB 13.4 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 319.7/390.3 MB 13.4 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 320.4/390.3 MB 13.1 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 321.0/390.3 MB 13.4 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 321.6/390.3 MB 13.4 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 322.3/390.3 MB 13.1 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 322.9/390.3 MB 13.4 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 323.5/390.3 MB 13.1 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 324.1/390.3 MB 13.4 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 324.8/390.3 MB 13.4 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 325.5/390.3 MB 13.4 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 326.1/390.3 MB 13.4 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 326.7/390.3 MB 13.4 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 327.4/390.3 MB 13.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 328.0/390.3 MB 13.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 328.6/390.3 MB 13.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 329.3/390.3 MB 13.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 329.9/390.3 MB 13.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 330.5/390.3 MB 13.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 331.1/390.3 MB 13.4 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 331.8/390.3 MB 13.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 332.3/390.3 MB 13.4 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 333.0/390.3 MB 13.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 333.6/390.3 MB 13.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 334.3/390.3 MB 13.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 334.9/390.3 MB 13.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 335.5/390.3 MB 13.4 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 336.1/390.3 MB 13.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 336.8/390.3 MB 13.4 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 337.3/390.3 MB 13.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 337.9/390.3 MB 13.4 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 338.5/390.3 MB 13.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 339.2/390.3 MB 13.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 339.8/390.3 MB 13.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 340.4/390.3 MB 13.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 341.0/390.3 MB 13.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 341.7/390.3 MB 13.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 342.3/390.3 MB 13.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 342.9/390.3 MB 13.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 343.6/390.3 MB 13.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 344.2/390.3 MB 13.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 344.8/390.3 MB 13.4 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 345.5/390.3 MB 13.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 346.1/390.3 MB 13.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 346.4/390.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 346.4/390.3 MB 12.8 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 346.4/390.3 MB 11.7 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 346.9/390.3 MB 11.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 347.6/390.3 MB 11.3 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 348.3/390.3 MB 11.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 348.9/390.3 MB 11.3 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 349.5/390.3 MB 11.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 350.2/390.3 MB 11.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 350.8/390.3 MB 11.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 351.4/390.3 MB 11.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 352.0/390.3 MB 11.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 352.7/390.3 MB 11.3 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 353.3/390.3 MB 11.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 353.9/390.3 MB 11.3 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 354.5/390.3 MB 11.3 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 355.1/390.3 MB 11.3 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 355.8/390.3 MB 11.3 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 356.4/390.3 MB 11.3 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 357.0/390.3 MB 13.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 357.6/390.3 MB 13.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 358.3/390.3 MB 13.4 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 358.9/390.3 MB 13.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 359.4/390.3 MB 13.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 360.1/390.3 MB 13.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 360.7/390.3 MB 13.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 361.3/390.3 MB 13.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 361.9/390.3 MB 13.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 362.5/390.3 MB 13.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 363.1/390.3 MB 13.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 363.7/390.3 MB 13.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 364.3/390.3 MB 13.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 365.0/390.3 MB 13.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 365.6/390.3 MB 13.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 366.2/390.3 MB 13.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 366.8/390.3 MB 13.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 367.5/390.3 MB 13.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 368.1/390.3 MB 13.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 368.7/390.3 MB 13.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 369.3/390.3 MB 13.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 370.0/390.3 MB 13.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 370.6/390.3 MB 13.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 371.2/390.3 MB 13.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 371.8/390.3 MB 13.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 372.5/390.3 MB 13.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 373.1/390.3 MB 13.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 373.7/390.3 MB 13.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 374.3/390.3 MB 13.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 375.0/390.3 MB 13.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 375.6/390.3 MB 13.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 376.2/390.3 MB 13.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 376.8/390.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 377.4/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 378.1/390.3 MB 13.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 378.7/390.3 MB 13.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 379.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 379.9/390.3 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  380.6/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  381.2/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  381.8/390.3 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  382.5/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  383.1/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  383.7/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  384.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.0/390.3 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.6/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  386.2/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  386.8/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  387.4/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  388.0/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  388.5/390.3 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  389.2/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  389.8/390.3 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 390.3/390.3 MB 5.2 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.7/133.7 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.12.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.5/57.5 kB ? eta 0:00:00\n",
      "Downloading grpcio-1.69.0-cp312-cp312-win_amd64.whl (4.4 MB)\n",
      "   ---------------------------------------- 0.0/4.4 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.6/4.4 MB 18.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.2/4.4 MB 15.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.8/4.4 MB 14.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.4/4.4 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.0/4.4 MB 13.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.6/4.4 MB 13.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 4.2/4.4 MB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.4/4.4 MB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.4/4.4 MB 11.7 MB/s eta 0:00:00\n",
      "Downloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 0.7/1.3 MB 14.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.3/1.3 MB 13.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 10.3 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.6/26.4 MB 17.5 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 1.2/26.4 MB 14.7 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 1.8/26.4 MB 14.4 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 2.4/26.4 MB 14.2 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 3.1/26.4 MB 14.0 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 3.7/26.4 MB 13.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 4.3/26.4 MB 13.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 4.9/26.4 MB 13.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 5.6/26.4 MB 13.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 6.2/26.4 MB 13.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 6.8/26.4 MB 13.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 7.4/26.4 MB 13.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 8.1/26.4 MB 13.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.7/26.4 MB 13.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 9.3/26.4 MB 13.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.9/26.4 MB 13.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 10.6/26.4 MB 13.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 11.2/26.4 MB 13.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 11.8/26.4 MB 13.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 12.5/26.4 MB 13.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 13.1/26.4 MB 13.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 13.7/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 14.3/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 15.0/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 15.6/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 16.2/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 16.8/26.4 MB 13.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 17.5/26.4 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 18.1/26.4 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 18.7/26.4 MB 13.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 20.0/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 20.6/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 21.1/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.8/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 22.3/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.9/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 23.5/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 24.2/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.8/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.4/26.4 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.0/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)\n",
      "   ---------------------------------------- 0.0/127.5 kB ? eta -:--:--\n",
      "   -------------------------------------- - 122.9/127.5 kB 7.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 122.9/127.5 kB 7.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- 127.5/127.5 kB 831.4 kB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.9/71.9 kB 3.9 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.6/5.5 MB 13.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.3/5.5 MB 13.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.9/5.5 MB 13.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.5/5.5 MB 13.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.1/5.5 MB 13.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.8/5.5 MB 12.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.3/5.5 MB 13.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.8/5.5 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.5 MB 12.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.5 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.5 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.5 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.5 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 9.0 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp312-cp312-win_amd64.whl (292 kB)\n",
      "   ---------------------------------------- 0.0/292.0 kB ? eta -:--:--\n",
      "   --------------------------------------  286.7/292.0 kB 17.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 292.0/292.0 kB 6.0 MB/s eta 0:00:00\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.12.23 gast-0.6.0 google-pasta-0.2.0 grpcio-1.69.0 keras-3.8.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.13.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 termcolor-2.5.0\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d570c691-5614-4855-9699-29aa2ef906d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Number of users: 447584, Number of items: 16\n",
      "Generating negative samples...\n",
      "Final dataset shape: (1181292, 3)\n",
      "Epoch 1/10\n",
      "\u001b[1m462/462\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 122ms/step - accuracy: 0.6714 - loss: 0.6010 - val_accuracy: 0.6779 - val_loss: 0.5826\n",
      "Epoch 2/10\n",
      "\u001b[1m462/462\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 116ms/step - accuracy: 0.7440 - loss: 0.5008 - val_accuracy: 0.6784 - val_loss: 0.6007\n",
      "Epoch 3/10\n",
      "\u001b[1m462/462\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 114ms/step - accuracy: 0.9091 - loss: 0.2244 - val_accuracy: 0.6787 - val_loss: 0.7507\n",
      "\n",
      "Recommendations: [('HarvardX/CS50x/2012', 0.8521104), ('MITx/6.00x/2012_Fall', 0.62702733), ('MITx/6.00x/2013_Spring', 0.54378307), ('HarvardX/ER22x/2013_Spring', 0.5138786), ('MITx/6.002x/2012_Fall', 0.43203673)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class EfficientNCF:\n",
    "    def __init__(self):\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.item_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        \n",
    "    def preprocess_data(self, df, user_col='userid_DI', item_col='course_id'):\n",
    "        \"\"\"\n",
    "        Preprocess the data efficiently\n",
    "        \"\"\"\n",
    "        print(\"Starting preprocessing...\")\n",
    "        \n",
    "        # Encode users and items\n",
    "        df['user_idx'] = self.user_encoder.fit_transform(df[user_col])\n",
    "        df['item_idx'] = self.item_encoder.fit_transform(df[item_col])\n",
    "        \n",
    "        # Get dimensions\n",
    "        self.n_users = len(self.user_encoder.classes_)\n",
    "        self.n_items = len(self.item_encoder.classes_)\n",
    "        \n",
    "        print(f\"Number of users: {self.n_users}, Number of items: {self.n_items}\")\n",
    "        return df\n",
    "\n",
    "    def generate_negative_samples(self, df, neg_ratio=1):\n",
    "        \"\"\"\n",
    "        Generate negative samples more efficiently\n",
    "        \"\"\"\n",
    "        print(\"Generating negative samples...\")\n",
    "        \n",
    "        # Create positive samples dataframe\n",
    "        pos_samples = df[['user_idx', 'item_idx']].copy()\n",
    "        pos_samples['label'] = 1\n",
    "        \n",
    "        # Create set of positive interactions for faster lookup\n",
    "        pos_interactions = set(zip(df['user_idx'], df['item_idx']))\n",
    "        \n",
    "        # Generate negative samples\n",
    "        neg_samples = []\n",
    "        for user in df['user_idx'].unique():\n",
    "            n_neg = min(int(df[df['user_idx'] == user].shape[0] * neg_ratio), 10)\n",
    "            neg_items = np.random.randint(0, self.n_items, size=n_neg * 2)\n",
    "            valid_negs = [(user, item) for item in neg_items \n",
    "                         if (user, item) not in pos_interactions][:n_neg]\n",
    "            neg_samples.extend(valid_negs)\n",
    "        \n",
    "        # Convert negative samples to dataframe\n",
    "        neg_df = pd.DataFrame(neg_samples, columns=['user_idx', 'item_idx'])\n",
    "        neg_df['label'] = 0\n",
    "        \n",
    "        # Combine positive and negative samples\n",
    "        final_df = pd.concat([pos_samples, neg_df], ignore_index=True)\n",
    "        print(f\"Final dataset shape: {final_df.shape}\")\n",
    "        \n",
    "        return final_df\n",
    "\n",
    "    def build_model(self, embedding_dim=32):\n",
    "        \"\"\"\n",
    "        Build a simplified NCF model\n",
    "        \"\"\"\n",
    "        # Input layers\n",
    "        user_input = Input(shape=(1,), name='user_input')\n",
    "        item_input = Input(shape=(1,), name='item_input')\n",
    "        \n",
    "        # Embedding layers\n",
    "        user_embedding = Embedding(self.n_users, embedding_dim, name='user_embedding')(user_input)\n",
    "        item_embedding = Embedding(self.n_items, embedding_dim, name='item_embedding')(item_input)\n",
    "        \n",
    "        # Flatten embeddings\n",
    "        user_flat = Flatten()(user_embedding)\n",
    "        item_flat = Flatten()(item_embedding)\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        concat = Concatenate()([user_flat, item_flat])\n",
    "        \n",
    "        # Dense layers\n",
    "        dense1 = Dense(64, activation='relu')(concat)\n",
    "        dense2 = Dense(32, activation='relu')(dense1)\n",
    "        output = Dense(1, activation='sigmoid')(dense2)\n",
    "        \n",
    "        model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def train(self, train_data, validation_split=0.2, batch_size=2048, epochs=10):\n",
    "        \"\"\"\n",
    "        Train the model with early stopping\n",
    "        \"\"\"\n",
    "        # Prepare training data\n",
    "        X = train_data[['user_idx', 'item_idx']].values\n",
    "        y = train_data['label'].values\n",
    "        \n",
    "        # Split training data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=validation_split, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Early stopping callback\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=2,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            [X_train[:, 0], X_train[:, 1]], \n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=([X_val[:, 0], X_val[:, 1]], y_val),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "\n",
    "    def get_recommendations(self, user_id, top_n=5):\n",
    "        \"\"\"\n",
    "        Get recommendations for a specific user\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_encoder.classes_:\n",
    "            return []\n",
    "        \n",
    "        user_idx = self.user_encoder.transform([user_id])[0]\n",
    "        \n",
    "        # Create user-item pairs for prediction\n",
    "        items = np.arange(self.n_items)\n",
    "        user_items = np.array([[user_idx, item_idx] for item_idx in items])\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = self.model.predict(\n",
    "            [user_items[:, 0], user_items[:, 1]], \n",
    "            batch_size=2048,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Get top N items\n",
    "        top_item_indices = np.argsort(predictions.flatten())[-top_n:][::-1]\n",
    "        recommended_items = self.item_encoder.inverse_transform(top_item_indices)\n",
    "        \n",
    "        return list(zip(recommended_items, predictions[top_item_indices].flatten()))\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Sample usage code\n",
    "    # Load your data\n",
    "    df = pd.read_csv('cleaned_dataset.csv')\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    recommender = EfficientNCF()\n",
    "    \n",
    "    # Preprocess data\n",
    "    processed_df = recommender.preprocess_data(df)\n",
    "    \n",
    "    # Generate training data with negative samples\n",
    "    training_data = recommender.generate_negative_samples(processed_df, neg_ratio=1)\n",
    "    \n",
    "    # Build and train model\n",
    "    recommender.build_model(embedding_dim=32)\n",
    "    \n",
    "    # Train model\n",
    "    history = recommender.train(\n",
    "        training_data,\n",
    "        batch_size=2048,\n",
    "        epochs=10\n",
    "    )\n",
    "    \n",
    "    # Get recommendations for a user\n",
    "    user_id = df['userid_DI'].iloc[0]  # Example user\n",
    "    recommendations = recommender.get_recommendations(user_id, top_n=5)\n",
    "    print(\"\\nRecommendations:\", recommendations)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9f33366c-c5f3-41ea-a3f0-7090e9e7fa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Number of users: 447584, Number of items: 16\n",
      "Generating negative samples...\n",
      "Final dataset shape: (701014, 3)\n",
      "Epoch 1/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 190ms/step - accuracy: 0.8194 - loss: 37.2946 - val_accuracy: 0.8451 - val_loss: 0.5725 - learning_rate: 5.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 188ms/step - accuracy: 0.8468 - loss: 0.5183 - val_accuracy: 0.8451 - val_loss: 0.4296 - learning_rate: 5.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 184ms/step - accuracy: 0.8462 - loss: 0.4265 - val_accuracy: 0.8451 - val_loss: 0.4125 - learning_rate: 5.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 191ms/step - accuracy: 0.8461 - loss: 0.4147 - val_accuracy: 0.8451 - val_loss: 0.4081 - learning_rate: 5.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 179ms/step - accuracy: 0.8465 - loss: 0.4099 - val_accuracy: 0.8451 - val_loss: 0.4055 - learning_rate: 5.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 160ms/step - accuracy: 0.8457 - loss: 0.4091 - val_accuracy: 0.8451 - val_loss: 0.4035 - learning_rate: 5.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 165ms/step - accuracy: 0.8459 - loss: 0.4067 - val_accuracy: 0.8451 - val_loss: 0.4019 - learning_rate: 5.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 157ms/step - accuracy: 0.8460 - loss: 0.4059 - val_accuracy: 0.8451 - val_loss: 0.4006 - learning_rate: 5.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 157ms/step - accuracy: 0.8453 - loss: 0.4052 - val_accuracy: 0.8451 - val_loss: 0.3995 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 178ms/step - accuracy: 0.8466 - loss: 0.4016 - val_accuracy: 0.8451 - val_loss: 0.3982 - learning_rate: 5.0000e-04\n",
      "\n",
      "Sample recommendations for user: MHxPC130442623\n",
      "Course: HarvardX/CS50x/2012, Score: 0.9614\n",
      "Course: MITx/6.00x/2012_Fall, Score: 0.9019\n",
      "Course: MITx/6.00x/2013_Spring, Score: 0.8721\n",
      "Course: HarvardX/ER22x/2013_Spring, Score: 0.8660\n",
      "Course: HarvardX/PH278x/2013_Spring, Score: 0.8164\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ImprovedNCF:\n",
    "    def __init__(self):\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.item_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        \n",
    "    def preprocess_data(self, df, user_col='userid_DI', item_col='course_id'):\n",
    "        \"\"\"\n",
    "        Preprocess the data efficiently\n",
    "        \"\"\"\n",
    "        print(\"Starting preprocessing...\")\n",
    "        \n",
    "        # Encode users and items\n",
    "        df['user_idx'] = self.user_encoder.fit_transform(df[user_col])\n",
    "        df['item_idx'] = self.item_encoder.fit_transform(df[item_col])\n",
    "        \n",
    "        # Get dimensions\n",
    "        self.n_users = len(self.user_encoder.classes_)\n",
    "        self.n_items = len(self.item_encoder.classes_)\n",
    "        \n",
    "        print(f\"Number of users: {self.n_users}, Number of items: {self.n_items}\")\n",
    "        return df\n",
    "\n",
    "    def generate_negative_samples(self, df, neg_ratio=0.5):  # Reduced negative ratio\n",
    "        \"\"\"\n",
    "        Generate negative samples more efficiently\n",
    "        \"\"\"\n",
    "        print(\"Generating negative samples...\")\n",
    "        \n",
    "        # Create positive samples dataframe\n",
    "        pos_samples = df[['user_idx', 'item_idx']].copy()\n",
    "        pos_samples['label'] = 1\n",
    "        \n",
    "        # Create set of positive interactions for faster lookup\n",
    "        pos_interactions = set(zip(df['user_idx'], df['item_idx']))\n",
    "        \n",
    "        # Generate negative samples\n",
    "        neg_samples = []\n",
    "        for user in df['user_idx'].unique():\n",
    "            n_neg = min(int(df[df['user_idx'] == user].shape[0] * neg_ratio), 5)  # Reduced max negatives\n",
    "            neg_items = np.random.randint(0, self.n_items, size=n_neg * 2)\n",
    "            valid_negs = [(user, item) for item in neg_items \n",
    "                         if (user, item) not in pos_interactions][:n_neg]\n",
    "            neg_samples.extend(valid_negs)\n",
    "        \n",
    "        # Convert negative samples to dataframe\n",
    "        neg_df = pd.DataFrame(neg_samples, columns=['user_idx', 'item_idx'])\n",
    "        neg_df['label'] = 0\n",
    "        \n",
    "        # Combine positive and negative samples\n",
    "        final_df = pd.concat([pos_samples, neg_df], ignore_index=True)\n",
    "        print(f\"Final dataset shape: {final_df.shape}\")\n",
    "        \n",
    "        return final_df\n",
    "\n",
    "    def build_model(self, embedding_dim=32):\n",
    "        \"\"\"\n",
    "        Build improved NCF model with regularization\n",
    "        \"\"\"\n",
    "        # Input layers\n",
    "        user_input = Input(shape=(1,), name='user_input')\n",
    "        item_input = Input(shape=(1,), name='item_input')\n",
    "        \n",
    "        # Embedding layers with regularization\n",
    "        user_embedding = Embedding(\n",
    "            self.n_users, \n",
    "            embedding_dim,\n",
    "            embeddings_regularizer=l2(0.01),\n",
    "            name='user_embedding'\n",
    "        )(user_input)\n",
    "        \n",
    "        item_embedding = Embedding(\n",
    "            self.n_items, \n",
    "            embedding_dim,\n",
    "            embeddings_regularizer=l2(0.01),\n",
    "            name='item_embedding'\n",
    "        )(item_input)\n",
    "        \n",
    "        # Flatten embeddings\n",
    "        user_flat = Flatten()(user_embedding)\n",
    "        item_flat = Flatten()(item_embedding)\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        concat = Concatenate()([user_flat, item_flat])\n",
    "        \n",
    "        # Dense layers with dropout and regularization\n",
    "        dense1 = Dense(64, \n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=l2(0.01))(concat)\n",
    "        dropout1 = Dropout(0.2)(dense1)\n",
    "        \n",
    "        dense2 = Dense(32, \n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=l2(0.01))(dropout1)\n",
    "        dropout2 = Dropout(0.2)(dense2)\n",
    "        \n",
    "        output = Dense(1, activation='sigmoid')(dropout2)\n",
    "        \n",
    "        model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "        \n",
    "        # Use a lower learning rate\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def train(self, train_data, validation_split=0.2, batch_size=2048, epochs=10):\n",
    "        \"\"\"\n",
    "        Train the model with improved callbacks\n",
    "        \"\"\"\n",
    "        # Prepare training data\n",
    "        X = train_data[['user_idx', 'item_idx']].values\n",
    "        y = train_data['label'].values\n",
    "        \n",
    "        # Split training data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=validation_split, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Modified early stopping\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=0.001\n",
    "        )\n",
    "        \n",
    "        # Add learning rate reduction\n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=0.0001\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            [X_train[:, 0], X_train[:, 1]], \n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=([X_val[:, 0], X_val[:, 1]], y_val),\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "\n",
    "    def get_recommendations(self, user_id, top_n=5):\n",
    "        \"\"\"\n",
    "        Get recommendations for a specific user\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_encoder.classes_:\n",
    "            return \"User not found in training data\"\n",
    "        \n",
    "        user_idx = self.user_encoder.transform([user_id])[0]\n",
    "        \n",
    "        # Create test data for all items\n",
    "        test_user = np.array([user_idx] * self.n_items)\n",
    "        test_items = np.array(range(self.n_items))\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = self.model.predict([test_user, test_items], verbose=0)\n",
    "        \n",
    "        # Get top N recommendations\n",
    "        top_indices = predictions.flatten().argsort()[-top_n:][::-1]\n",
    "        \n",
    "        # Convert back to original item IDs and include scores\n",
    "        recommendations = [\n",
    "            (self.item_encoder.inverse_transform([idx])[0], float(predictions[idx]))\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Test the improved version\n",
    "def test_improved_model(df):\n",
    "    # Initialize the improved model\n",
    "    improved_recommender = ImprovedNCF()\n",
    "    \n",
    "    # Preprocess data\n",
    "    processed_df = improved_recommender.preprocess_data(df)\n",
    "    \n",
    "    # Generate training data with reduced negative samples\n",
    "    training_data = improved_recommender.generate_negative_samples(processed_df)\n",
    "    \n",
    "    # Build and train the model\n",
    "    improved_recommender.build_model()\n",
    "    history = improved_recommender.train(training_data)\n",
    "    \n",
    "    # Get recommendations for a sample user\n",
    "    sample_user = df['userid_DI'].iloc[0]\n",
    "    recommendations = improved_recommender.get_recommendations(sample_user)\n",
    "    \n",
    "    print(\"\\nSample recommendations for user:\", sample_user)\n",
    "    for course, score in recommendations:\n",
    "        print(f\"Course: {course}, Score: {score:.4f}\")\n",
    "    \n",
    "    return improved_recommender, history\n",
    "\n",
    "# Run the test\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    df = pd.read_csv('cleaned_dataset.csv')  # Replace with your data path\n",
    "    \n",
    "    # Test the improved model\n",
    "    improved_model, history = test_improved_model(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a2dd4f-87c6-483e-8930-90e8ebd7b908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Number of users: 447584, Number of items: 16\n",
      "Generating negative samples...\n",
      "Final dataset shape: (701019, 3)\n",
      "Epoch 1/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 152ms/step - accuracy: 0.8430 - loss: 42.7820 - val_accuracy: 0.8463 - val_loss: 0.6428 - learning_rate: 5.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 150ms/step - accuracy: 0.8456 - loss: 0.5765 - val_accuracy: 0.8463 - val_loss: 0.4501 - learning_rate: 5.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 152ms/step - accuracy: 0.8451 - loss: 0.4422 - val_accuracy: 0.8463 - val_loss: 0.4158 - learning_rate: 5.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 155ms/step - accuracy: 0.8452 - loss: 0.4184 - val_accuracy: 0.8463 - val_loss: 0.4093 - learning_rate: 5.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 151ms/step - accuracy: 0.8468 - loss: 0.4101 - val_accuracy: 0.8463 - val_loss: 0.4066 - learning_rate: 5.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 160ms/step - accuracy: 0.8460 - loss: 0.4094 - val_accuracy: 0.8463 - val_loss: 0.4047 - learning_rate: 5.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 166ms/step - accuracy: 0.8457 - loss: 0.4072 - val_accuracy: 0.8463 - val_loss: 0.4030 - learning_rate: 5.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 163ms/step - accuracy: 0.8462 - loss: 0.4044 - val_accuracy: 0.8463 - val_loss: 0.4013 - learning_rate: 5.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 159ms/step - accuracy: 0.8452 - loss: 0.4062 - val_accuracy: 0.8463 - val_loss: 0.4001 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 160ms/step - accuracy: 0.8456 - loss: 0.4039 - val_accuracy: 0.8463 - val_loss: 0.3988 - learning_rate: 5.0000e-04\n",
      "\n",
      "Generating ROC curve...\n",
      "\n",
      "Calculating ROC curve...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1kAAAK7CAYAAAAA+kUOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAADFCklEQVR4nOzdd3xN9+PH8dfNHmKPiL33VpTWql1arb33KB2qUyk1frS6tF8tLWrVDGq0VpSWGrVLUXsTI4QQmff8/rh109uEJtzkZLyfj4eHez535J04ifvO55zPsRiGYSAiIiIiIiJO4WJ2ABERERERkfREJUtERERERMSJVLJEREREREScSCVLRERERETEiVSyREREREREnEglS0RERERExIlUskRERERERJxIJUtERERERMSJVLJEREREREScSCVLJAOaNWsWFovF/sfNzY28efPSsWNHjh8/bnY8AAoXLkzPnj3NjhHP3bt3+fDDD6lSpQqZMmXC19eXypUrM378eO7evWt2vEQbP348y5cvjzf+yy+/YLFY+OWXX1I8032nTp3i5ZdfpmTJknh7e+Pj40O5cuUYMWIEFy9etD+ufv36lC9f3rScj2P+/PlMmjQp2V7/Ub5/tm3bxgcffEBoaGi8++rXr0/9+vWdku2+Z555hoEDB9q37+979/+4urqSK1cuWrVqxe7duxN8DcMwmD9/Pg0bNiRbtmx4enpStGhRBg8ezPnz5x/4sVetWkWrVq3IkycPHh4eZM+enWeeeYZ58+YRHR0NwM2bN8maNWuC3ycPk9j9V0TSOUNEMpyZM2cagDFz5kxj+/btxqZNm4xx48YZ3t7eRu7cuY0bN26YHdHYu3evceLECbNjOAgODjbKly9veHt7G++8846xfv16Y/369ca7775reHt7G+XLlzeCg4PNjpkovr6+Ro8ePeKN37p1y9i+fbtx69atlA9lGMaqVasMX19fo1ChQsbHH39sbNiwwfj555+NSZMmGRUrVjQqV65sf2y9evWMcuXKmZLzcT377LNGoUKFku31H+X75+OPPzYA4/Tp0/HuO3TokHHo0CEnpTOM5cuXG56ensaFCxfsY5s2bTIAY/z48cb27duNzZs3G1988YWRPXt2w8fHxzh27JjDa8TGxhodOnQwAKNTp07G8uXLjU2bNhlffPGFkT9/fiNr1qzGb7/95vAcq9Vq9OzZ0wCMFi1aGN9//73x66+/GitXrjRef/11I3PmzMakSZPsj//ggw+M4sWLG5GRkYn6vJKy/4pI+qaSJZIB3S9Zu3btchgfPXq0ARjfffedScnMFRMTY0RERDzw/iZNmhhubm7Gli1b4t23ZcsWw83NzWjatGlyRkzQf+VOyINKlplOnTpl+Pr6GlWqVDFCQ0Pj3W+1Wo2lS5fat1OiZFmtViM8PNzpr5tcJetxsj6sZDlbjRo1jI4dOzqM3S9ZgYGBDuOzZ882AGPkyJEO4+PHjzcA48MPP4z3+sHBwUahQoWMPHnyGDdv3rSPf/TRRwZgjB49OsFcly9fdvj+Dg4ONtzc3Ix58+b95+eU1P33cURFRRnR0dFOeS0RSR4qWSIZ0INK1k8//WQAxoQJExzGd+3aZbRq1crIli2b4enpaVSuXNlYtGhRvNe9cOGC0a9fPyN//vyGu7u7kTdvXqNNmzYOszu3bt0y3njjDaNw4cKGu7u7ERAQYLz22mvGnTt3HF6rUKFC9hJw9epVw93d3RgxYkS8j3nkyBEDML744gv72OXLl43+/fsb+fLlM9zd3Y3ChQsbH3zwgcObktOnTxuA8dFHHxljx441ChcubLi6uhpr1qxJ8Gu2a9cuAzAGDBjwgK+qYfTv398AjN27d9vHAGPw4MHG1KlTjRIlShgeHh5GmTJljAULFsR7/uPmvnfvnjF06FCjUqVKRubMmY1s2bIZtWrVMpYvX+7wcYB4f+rVq2cYRtwb3U2bNtkf36NHD8PX19c4fvy40bx5c8PX19fInz+/MXTo0Hjl7vz580abNm2MTJkyGVmyZDE6d+5s7Ny50z5z+jAvv/yyARjbt29/6OPuu1+ydu7caTz11FOGt7e3UaRIEWPChAlGbGys/XGJ/brc/9oMHjzYmDJlilG6dGnD3d3dmDJlimEYtlmNGjVqGNmyZTP8/PyMKlWqGNOnTzesVmu815k3b55Rq1Ytw9fX1/D19TUqVapkTJ8+3Z47oX+D+yIjI42xY8capUqVMjw8PIycOXMaPXv2NK5everwMQoVKmQ8++yzxtKlS43KlSsbnp6exjvvvGO/758lOjY21hg7dqxRsmRJw8vLy8iSJYtRoUIF+6zNqFGjEsx0fz+oV6+efR+5LyIiwhg9erRRunRpw9PT08iePbtRv359Y+vWrQ/9d9u7d68BGD/99JPD+INK1qFDh+J970VGRhrZsmUzypQpk+DX3zAMY/78+QZgfPLJJ4Zh2IpJ9uzZjdKlSz/wOQlp3ry58fTTT//n45K6//773+i+f3+t739d5syZYwwdOtQICAgwLBaLsX//fgOw71f/tHr1agMwVqxYYR87duyY0alTJyNXrlyGh4eHUbp0aWPy5MmJyioiSeeWDEcgikgadfr0aQBKlixpH9u0aRPNmjWjZs2aTJ06lSxZsrBw4UI6dOhAeHi4/byPixcv8sQTTxAdHc17771HxYoVCQkJYd26ddy8eZM8efIQHh5OvXr1uHDhgv0xhw4dYuTIkRw8eJANGzZgsVji5cqVKxctW7Zk9uzZjB49GheXuNNJZ86ciYeHB126dAEgODiYGjVq4OLiwsiRIylWrBjbt29n3LhxnDlzhpkzZzq89pdffknJkiX55JNPyJw5MyVKlEjwaxMUFARA69atH/j1a926Nd9++y1BQUFUq1bNPr5y5Uo2bdrEmDFj8PX15euvv6ZTp064ubnRtm1bp+WOjIzkxo0bvPnmm+TLl4+oqCg2bNjAiy++yMyZM+nevTsA27dvp2HDhjRo0ID3338fgMyZMz/w8wKIjo7mueeeo0+fPrzxxhts3ryZsWPHkiVLFkaOHAnYzldr0KABN27c4KOPPqJ48eKsXbuWDh06PPS171u/fj158uShVq1aiXr8/a9bly5deOONNxg1ahQ//PADw4YNIyAgwP75Jvbrct/y5cvZsmULI0eOxN/fn9y5cwNw5swZBgwYQMGCBQHYsWMHr7zyChcvXrR/DQBGjhzJ2LFjefHFF3njjTfIkiULf/75J2fPngXg66+/pn///pw8eZIffvjB4WNbrVaef/55tmzZwttvv03t2rU5e/Yso0aNon79+uzevRtvb2/74/fu3cuRI0cYMWIERYoUwdfXN8Gv08SJE/nggw8YMWIEdevWJTo6mr/++st+/lXfvn25ceMG//vf/1i2bBl58+YFoGzZsgm+XkxMDM2bN2fLli0MGTKEhg0bEhMTw44dOzh37hy1a9d+4L/Zjz/+iKurK3Xr1n3gY/4poZ9Le/bs4ebNm/Tv3z/BnxkArVq1wsXFhaCgIN544w12797NjRs36Nev3wOfk5D69eszbNgwQkNDyZo16wMf9yj7b1IMGzaMJ598kqlTp+Li4kKBAgWoUqUKM2fOpE+fPg6PnTVrFrlz56ZFixYAHD58mNq1a1OwYEE+/fRT/P39WbduHa+++irXr19n1KhRyZJZJEMzu+WJSMq7P5O1Y8cOIzo62ggLCzPWrl1r+Pv7G3Xr1nWYOSldurRRpUqVeIemtGzZ0sibN699xqB3796Gu7u7cfjw4Qd+3AkTJhguLi7xZtCWLFliAMbq1avtY//+Le/KlSsNwFi/fr19LCYmxggICDDatGljHxswYICRKVMm4+zZsw4f45NPPjEA+3kl92eEihUrZkRFRf3Xl8wYOHCgARh//fXXAx9zf1btpZdeso8Bhre3t8NsXkxMjFG6dGmjePHiyZo7JibGiI6ONvr06WNUqVLF4b4HHS74oJkswFi8eLHDY1u0aGGUKlXKvv3VV18ZQLzZwAEDBiRqJsvLy8uoVavWQx/zT/dnhH7//XeH8bJlyz70sM2HfV0AI0uWLP95XmJsbKwRHR1tjBkzxsiRI4d9ZuTUqVOGq6ur0aVLl4c+/0GHCy5YsMAA4h1Wdn8m9euvv7aPFSpUyHB1dTWOHj0a73X+/f3TsmXL/zwf6GGHC/57dmXOnDkGYEybNu2hr5mQ5s2bG6VLl443fn/fW7RokREdHW2Eh4cbW7duNUqVKmWULVvW4bC/hQsXGoAxderUh36sPHnyGGXKlEnSc/4tKCgowf3635K6/yZ1Jqtu3brxHvvll18agMM+cOPGDcPT09N444037GNNmzY18ufPH+9cy5dfftnw8vJKFefhiqQ3Wl1QJAOrVasW7u7u+Pn50axZM7Jly8aKFStwc7NNcp84cYK//vrLPksUExNj/9OiRQsuX77M0aNHAVizZg0NGjSgTJkyD/x4P/74I+XLl6dy5coOr9W0adP/XNGuefPm+Pv7O8zorFu3jkuXLtG7d2+Hj9GgQQMCAgIcPkbz5s0B+PXXXx1e97nnnsPd3T1pX7gHMAwDIN5vyZ955hny5Mlj33Z1daVDhw6cOHGCCxcuODV3YGAgderUIVOmTLi5ueHu7s6MGTM4cuTIY31uFouFVq1aOYxVrFjRPjtzP+P9femfOnXq9Fgf+2H8/f2pUaPGQ3NB0r4u91eq+7eNGzfSqFEjsmTJgqurK+7u7owcOZKQkBCuXr0K2GY8Y2NjGTx48CN9Pj/++CNZs2alVatWDvtB5cqV8ff3j/c9UrFiRYcZngepUaMGf/zxB4MGDWLdunXcvn37kfLdt2bNGry8vBy+9xLr0qVL9tnBhHTo0AF3d3d8fHyoU6cOt2/f5qeffnroLNKDGIaRpFmrhNzPavbKgG3atIk31qVLFzw9PZk1a5Z9bMGCBURGRtKrVy8AIiIi+Pnnn3nhhRfw8fGJ93M8IiKCHTt2pNSnIZJhqGSJZGBz5sxh165dbNy4kQEDBnDkyBGHN8RXrlwB4M0338Td3d3hz6BBgwC4fv06ANeuXSN//vwP/XhXrlzhwIED8V7Lz88PwzDsr5UQNzc3unXrxg8//GA/xGnWrFnkzZuXpk2bOnyMVatWxfsY5cqVc8h73/3Dov7L/UPE7h+6lJAzZ84AUKBAAYdxf3//eI+9PxYSEuK03MuWLaN9+/bky5eP77//nu3bt7Nr1y569+5NREREoj7PB/Hx8cHLy8thzNPT0+F1Q0JCHMrkfQmNJaRgwYIP/fomJEeOHPHGPD09uXfvnn07qV+XhL62O3fupEmTJgBMmzaNrVu3smvXLoYPHw5g/3jXrl0D+M/vhQe5cuUKoaGheHh4xNsXgoODH3n/HTZsGJ988gk7duygefPm5MiRg2eeeeaBS6P/l2vXrhEQEOBw6G5i3bt3L96+9E8fffQRu3bt4tdff2X48OFcuXKF1q1bExkZaX9MYr4f7969y/Xr1+3fj4l5TkLuZ/3nPpWQR9l/kyKhf+vs2bPz3HPPMWfOHGJjYwHbz8UaNWrYf3aEhIQQExPD//73v3j71P3DCR/2s1dEHo3OyRLJwMqUKUP16tUBaNCgAbGxsUyfPp0lS5bQtm1bcubMCdjeoL344osJvkapUqUA23lT92dlHiRnzpx4e3vz3XffPfD+h+nVqxcff/yx/ZywlStXMmTIEFxdXR1eo2LFivzf//1fgq8REBDgsJ3Y33I3btyY9957j+XLl8ebqbnv/vV0Gjdu7DAeHBwc77H3x+6XBGfk/v777ylSpAiLFi1yuP+fb06TU44cOdi5c2e88YQ+/4Q0bdqU//3vf+zYscOp57Uk9euS0Nd24cKFuLu78+OPPzoUhH9fQylXrlwAXLhwIV7ZToycOXOSI0cO1q5dm+D9fn5+/5k1IW5ubgwdOpShQ4cSGhrKhg0beO+992jatCnnz5/Hx8cnSTlz5crFb7/9htVqTXLRypkzJzdu3Hjg/UWLFrX/XKpbty7e3t6MGDGC//3vf7z55psAVKtWjWzZsrFy5UomTJiQ4Ndh5cqVWK1W+/dj9erVyZ49OytWrHjgcxJyP+t//XxK6v7r5eWV4D54/fr1BD/Wg/L26tWLwMBAgoKCKFiwILt27WLKlCn2+7Nly4arqyvdunV74AxrkSJF/jOviCSNZrJExG7ixIlky5aNkSNHYrVaKVWqFCVKlOCPP/6gevXqCf65/6avefPmbNq0yX74YEJatmzJyZMnyZEjR4KvVbhw4YfmK1OmDDVr1mTmzJnMnz/f4ZCYf36MP//8k2LFiiX4Mf5dVhKrevXqNGnShBkzZrB169Z49//222989913NGvWzGHRC4Cff/7ZPisIEBsby6JFiyhWrJh9xsMZuS0WCx4eHg5vxoKDg1mxYkW8x/57tscZ6tWrR1hYGGvWrHEYX7hwYaKe//rrr+Pr68ugQYO4detWvPsNw4i3UERiJOXr8rDXcHNzcyj09+7dY+7cuQ6Pa9KkCa6urg5vchPyoK9/y5YtCQkJITY2NsH94P4vNR5H1qxZadu2LYMHD+bGjRv2GVhPT0/75/VfmjdvTkREhMNhaolVunRpTp06lejHv/322xQvXpwPP/yQsLAwADw8PHjrrbc4cuQIH3/8cbznXL16lWHDhpEnTx769u0LgLu7O++88w5//fUXY8eOTfBjXb16Nd739/2sD1oE5L6k7r+FCxfmwIEDDo85duzYQ3+GJqRJkybky5ePmTNnMnPmTLy8vByOSPDx8aFBgwbs27ePihUrJrhfJTQjLCKPRzNZImKXLVs2hg0bxttvv838+fPp2rUr33zzDc2bN6dp06b07NmTfPnycePGDY4cOcLevXsJDAwEYMyYMaxZs4a6devy3nvvUaFCBUJDQ1m7di1Dhw6ldOnSDBkyhKVLl1K3bl1ef/11KlasiNVq5dy5c6xfv5433niDmjVrPjRj7969GTBgAJcuXaJ27drx3nSOGTOGoKAgateuzauvvkqpUqWIiIjgzJkzrF69mqlTpz7yoVxz5syhUaNGNGnShFdffZVnnnkGsJ2r88UXX1C6dOkE33TmzJmThg0b8v7779tXF/zrr78cyoczcrds2ZJly5YxaNAg2rZty/nz5xk7dix58+bl+PHjDo+tUKECv/zyC6tWrSJv3rz4+fk99hv4Hj168Pnnn9O1a1fGjRtH8eLFWbNmDevWrQP4zxmPIkWK2GcpK1euzMsvv0yVKlUA2+po3333HYZh8MILLyQpV1K+Lg/y7LPP8tlnn9G5c2f69+9PSEgIn3zyib2Y3Fe4cGHee+89xo4dy7179+jUqRNZsmTh8OHDXL9+ndGjRwO2r/+yZcuYMmUK1apVw8XFherVq9OxY0fmzZtHixYteO2116hRowbu7u5cuHCBTZs28fzzzyf58wfbSnvly5enevXq5MqVi7NnzzJp0iQKFSpkX1GzQoUKAHzxxRf06NEDd3d3SpUqFW/2DGzn2c2cOZOBAwdy9OhRGjRogNVq5ffff6dMmTJ07NjxgVnq16/Pd999x7FjxxJ1Ppm7uzvjx4+nffv2fPHFF4wYMQKAd955hz/++MP+d4cOHciSJQsHDhzg448/JiwsjB9//JEsWbLYX+t+MRs1ahQ7d+6kc+fOFChQgFu3brF582a+/fZbRo8eTZ06dezP2bFjBzly5LB/fR4kqftvt27d6Nq1K4MGDaJNmzacPXuWiRMn2mdDE8vV1ZXu3bvz2WefkTlzZl588UWHzxls/6ZPPfUUTz/9NC+99BKFCxcmLCyMEydOsGrVKjZu3JikjykiiWDiohsiYpIHXSfLMGzXFCpYsKBRokQJIyYmxjAMw/jjjz+M9u3bG7lz5zbc3d0Nf39/o2HDhvFW6Tp//rzRu3dvw9/f334NrPbt2xtXrlyxP+bOnTvGiBEj7NcAun+9ntdff91hBb4Hrbx169Ytw9vb+6Erm127ds149dVXjSJFihju7u5G9uzZjWrVqhnDhw+3X4/r/ip9H3/8cZK+dnfu3DHGjx9vVK5c2fDx8TF8fHyMihUrGuPGjYt3rS/DiLvu0tdff20UK1bMcHd3N0qXLp3gxU2dkfvDDz80ChcubHh6ehplypQxpk2bZr8G0j/t37/fqFOnjuHj45Po62T9W0Kve+7cOePFF180MmXKZPj5+Rlt2rRJ8Jo9D3Py5Elj0KBBRvHixQ1PT0/D29vbKFu2rDF06FCHle8edDHiHj16xFu5L7Ffl/v/Xgn57rvvjFKlShmenp5G0aJFjQkTJhgzZsxIcEW+OXPmGE888YTh5eVlZMqUyahSpYrD6oo3btww2rZta2TNmtWwWCwOOaKjo41PPvnEqFSpkv35pUuXNgYMGGAcP37c/rj718lKyL+/fz799FOjdu3aRs6cOQ0PDw+jYMGCRp8+fYwzZ844PG/YsGFGQECA4eLi8p/Xybp3754xcuRI+/XfcuTIYTRs2NDYtm1bgpnuu3XrlpEpUyZj4sSJDuMPuk7WfTVr1jSyZcvmcKFfq9VqzJs3z6hfv76RNWtWw8PDwyhSpIjx0ksvxVup859WrFhhPPvss0auXLkMNzc3I1u2bEaDBg2MqVOnGpGRkQ6vX6hQIeOVV1556Of0T4ndf61WqzFx4kSjaNGihpeXl1G9enVj48aND1xd8EFfF8OwXQOLv69tFhQUlOBjTp8+bfTu3dt+Hb5cuXIZtWvXNsaNG5foz01EEs9iGH8vhyUiIk5nsVgYPHgwkydPNjuKacaPH8+IESM4d+7cI88iSvryyiuv8PPPP3Po0KHHXv0vOf388880adKEQ4cOUbp0abPjiEgaosMFRUTEae6XydKlSxMdHc3GjRv58ssv6dq1qwqW2I0YMYI5c+awdOlS+wW5U6Nx48bRu3dvFSwRSTKVLBERcRofHx8+//xzzpw5Q2RkJAULFuSdd96xn0cjArZl/efNm8fNmzfNjvJAN2/epF69evbLVYiIJIUOFxQREREREXEiLeEuIiIiIiLiRCpZIiIiIiIiTqSSJSIiIiIi4kQZbuELq9XKpUuX8PPzS9XLxoqIiIiISPIyDIOwsDACAgJwcXHe/FOGK1mXLl2iQIECZscQEREREZFU4vz580691EiGK1l+fn4AnD59muzZs5ucRtKz6Oho1q9fT5MmTXB3dzc7jqRj2tckpWhfk5SifU1Syo0bNyhSpIi9IzhLhitZ9w8R9PPzI3PmzCankfQsOjoaHx8fMmfOrP8gJFlpX5OUon1NUor2NUkp0dHRAE4/jUgLX4iIiIiIiDiRSpaIiIiIiIgTqWSJiIiIiIg4kUqWiIiIiIiIE6lkiYiIiIiIOJFKloiIiIiIiBOpZImIiIiIiDiRSpaIiIiIiIgTqWSJiIiIiIg4kUqWiIiIiIiIE6lkiYiIiIiIOJFKloiIiIiIiBOpZImIiIiIiDiRSpaIiIiIiIgTqWSJiIiIiIg4kUqWiIiIiIiIE6lkiYiIiIiIOJFKloiIiIiIiBOpZImIiIiIiDiRSpaIiIiIiIgTqWSJiIiIiIg4kUqWiIiIiIiIE5lasjZv3kyrVq0ICAjAYrGwfPny/3zOr7/+SrVq1fDy8qJo0aJMnTo1+YOKiIiIiIgkkqkl6+7du1SqVInJkycn6vGnT5+mRYsWPP300+zbt4/33nuPV199laVLlyZzUhERERERkcRxM/ODN2/enObNmyf68VOnTqVgwYJMmjQJgDJlyrB7924++eQT2rRpk0wpRUREREQk3TAMiLgJ4cFw/q9k+RCmlqyk2r59O02aNHEYa9q0KTNmzCA6Ohp3d/d4z4mMjCQyMtK+ffv2bQCio6OJjo5O3sCSod3fv7SfSXLTviYpRfuapBTta/JIosMhPBhL+BW4+/ff4cFY7tr+JvzK37evYLFGAbBmT8lkiZKmSlZwcDB58uRxGMuTJw8xMTFcv36dvHnzxnvOhAkTGD16dLzxTZs24ePjk2xZRe4LCgoyO4JkENrXJKVoX5OUon1NLEYMnrG38Iy5iWdsKF6xoXjG3LT9HXsTz5i4MXfjXpJfv1XZY8mQOo2VLACLxeKwbRhGguP3DRs2jKFDh9q3b9++TYECBWjQoAE5cuRIvqCS4UVHRxMUFETjxo0TnGUVcRbta5JStK9JStG+ls4ZBkTcSGDW6QqWu3/POIUHw90rWCKuO+/DYiHCPS9eWXJi+PiDbx5iYvyAr532Me5LUyXL39+f4OBgh7GrV6/i5ub2wMLk6emJp6dnvHF3d3d900qK0L4mKUX7mqQU7WuSUrSvpTFRd+Dv0mT/Ex7suP13icLqxENBPbOAjz/4/uPPv7d9/dmyK5wu3VYwa9bzPPNMUdtzQ0LI8CXrySefZNWqVQ5j69evp3r16voGFBERERFxttgoCL/68OJ0fzv6rvM+rqvngwvTP7d98oC790Nfymo1+PjjrQwfvpHYWIPOnZexb98AAgL8nJf3X0wtWXfu3OHEiRP27dOnT7N//36yZ89OwYIFGTZsGBcvXmTOnDkADBw4kMmTJzN06FD69evH9u3bmTFjBgsWLDDrUxARERERSVusMRB+LW7WKfwK3F8c4p9/3w2GiBDnfVyLC/jkTnCWKd6YR2Z4wOlASRESEk737stZvfq4faxcuVy4uj7+az+MqSVr9+7dNGjQwL59/9ypHj16MGvWLC5fvsy5c+fs9xcpUoTVq1fz+uuv89VXXxEQEMCXX36p5dtFREREJGOzxsK96/GLk/32P/6+dx0wnPexPbM+9DA9+5h3TnBxdd7H/Q87dlygfftAzp+3rS5uscD779dl5Mh6uLom7+WCTS1Z9evXty9ckZBZs2bFG6tXrx579+5NxlQiIiIiIqmAYYV7IYksTtdsj3cWNy/wzfsfs055bIfruXk57+M6gWEYfPHF77z1VhAxMbavSc6cPsyb9yJNmhRLkQxp6pwsEREREZE0zb6y3v0FIv55mN6/C9RVMGKd97FdPW2l6P65TP8sSv8c88kDHn5OOVwvpYWGRtC79wp++CHuIsNPPVWQhQvbkC9f5hTLoZIlIiIiIvI4DAMiQxM+rymh4uTMlfVc3P9VmP719/3S5JPHtgpfGixOSXH16l2Cgk7Zt99+uzbjxjXE3T3lDlMElSwRERERkYeLiYAbRyHkMNz4C+5ein/IXmyU8z6ei1tcMUqoMP3zb8+s6b44JUXJkjmYNq0Vgwb9xJw5L9CyZUlTcqhkiYiIiIgAxETCzWMQcsj25/rff4eeePzznSyuf6+sl9Cs07+Kk1c220p88p/CwiJxd3fFyyuu1nTsWJ4mTYqRPfvDl3ZPTipZIiIiIpKxxEbZytT9EnW/UIWeSNo5UBYX8M7134fp+fqDdw4VJyc7ePAKbdsG0qBBYaZObelwn5kFC1SyRERERCS9io2Cm8cdZ6VCDkPocdu1ohLDzRuyl4EcZSFHOchZDjIXspWnFF6SXOLMnLmPQYNWExERw7FjIdStW4jOnSuYHctOJUtERERE0rbYaFtxuv53ibo/O3XzWBLKlBdkK20rUTnK/aNQFVaRSkXu3o1i8ODVzJ79h32sShV/atbMZ2Kq+FSyRERERCRtiI2G0JPxz5m6eSzxK/a5ekL20nEl6n6hylJEZSqVO3LkGu3aBXLo0DX72MCB1fj882YO52SlBqkrjYiIiIiINYZMURexnPgBQo/GlaobR5NQpjziylSOcnGH+2Utalu9T9KUefMOMGDAj9y9a/v39/V159tvW6WqQwT/SXuYiIiIiJjDGpvgzJTbjaM8Y42Cc4l4DRd3yF7K8RC/HOUgazGVqXQgMjKGV19dw7ff7rWPlS+fm8DAdpQundPEZA+nPU9EREREkpc1Fm6dclx8IuSQ7ZpTsZHxHp7gVZ9c3CFbyfiH+WUrrjKVjrm5uXD8+A37dq9elZk8uQU+Pu4mpvpv2iNFRERExDkM419l6u/ZqZt/2S7omxgubhhZS3ApKhv+5Z7BNXdF26F+WUuAa+p+Yy3O5+rqwvz5bahT5ztGjHiaXr2qmB0pUVSyREREROTRGAbcOg3nNsL5jba/w68k7rkW179npso6zk5lK0GM1cLu1atpUbMFru4qVhlJZGQMFy7cplix7PYxf/9MHDkyGA+PtLMwiUqWiIiIiCRe2EU4vymuWN0++/DHW1wha/H4S6NnK2lbnCIhiV3cQtKVM2dCad8+kKtX77J37wCHCwqnpYIFKlkiIiIi8jDh1+HCL7ZSdW4j3Dz64Me6Z4L8dSFP1bgV/bKVAjfPFIsradPKlUfp0WM5oaG2w0r791/FkiXtTU716FSyRERERCRO5G24sDnu8L9rfzz4sW5eEFAHCjaEAg0hTzWdNyVJEh0dy/DhG/n44232saJFs/Hee0+bmOrxqWSJiIiIZGTR4XBpW9zhf8G7wYhN+LEubuBf01aqCjaEvLVsRUvkEVy4cJsOHZawbdt5+9iLL5bhu++eI0uWtL1fqWSJiIiIZCSxURC8K65UXdpmG0uQxXboX4G/S1W+p8AjU4rGlfRp3boTdO36A9evhwPg7u7Cxx835tVXa2KxJLiIf5qikiUiIiKSnllj4dr+uHOqLm6B6LsPfnyOsnGlKn898M7+4MeKPIJx4zYzcuQmDMO2XbBgFhYvbkvNmvnNDeZEKlkiIiIi6Ylh2C72e3+m6vwvEBn64MdnKRp3TlXBBuDrn0JBJaPKlcvHXrBatizJ7NmtHVYSTA9UskRERETSsvsXAL4/U3V+08OvVeWbFwo+83exagBZCqdYVBGA/v2rsW3bBcqVy8Wbb9bGxSXtHx74bypZIiIiImlN2MW41f/ObYSwcw9+rFcO2wzV/UMAs5WEdHDOi6QNVqvBL7+coWHDIvYxi8XCrFnPp4tzrx5EJUtEREQktXO4VtXPcPPYgx/rngkK1IsrVbkqgsUlxaKK3Hft2l26dfuBdetOsmpVJ1q2LGm/Lz0XLFDJEhEREUl9dK0qSeO2bj1Hhw5LuHgxDIBevVZw+vRrZMrkYXKylKGSJSIiImI2XatK0gnDMPj00+28++4GYmNtq1vkzu3L/PkvZpiCBSpZIiIiIinPGgOXdsTNVF3ermtVSZp348Y9evZczqpVcYez1qtXiAUL2pA3r5+JyVKeSpaIiIhISjAMuLwDjsyDo4vh3rUHPzZHubjD//LX1bWqJNXbufMi7dsHcvbsLfvY8OFP88EH9XFzy3jnBKpkiYiIiCSnkMNwZD78NR9unU74MbpWlaRhgYGH6NJlGdHRVgBy5PBm7twXaN68hMnJzKOSJSIiIuJsYRfgrwW2cnVtf/z73bygaEso0kLXqpI074kn8uHr60FoaARPPpmfRYvaUqBAFrNjmUolS0RERMQZ7t2A40tthwNe2AwYjvdbXGwXAS7TBYq/AJ6ZTYkp4myFC2dl9uzWbN58lgkTnsHd3dXsSKZTyRIRERF5VNH34NQq24zV6dVgjY7/GP8aUKYzlOqgwwAlzTMMg3nzDtK6dWmH1QKfe64Uzz1XysRkqYtKloiIiEhSWGNsKwIemQcnfoCosPiPyVbSNmNVuhNky7jnpUj6cudOFAMG/Mj8+Qfp3LkC33//Qrq/qPCjUskSERER+S+GAcG7/l4ZcBGEX4n/GN+8ULqjrVzlrgp68ynpyMGDV2jXLpCjR0MAmD//IIMGVadOnYImJ0udVLJEREREHuTG0biVAUNPxL/fIzOUbAulO0OB+uCic1Ek/Zk1az+DBv3EvXsxAPj5eTBjxnMqWA+hkiUiIiLyT3cuwV8LbcXqyp7497t62FYGLN0Zij5rWylQJB0KD4/m5ZdXM3PmfvtYpUp5CAxsR4kSOcwLlgaoZImIiIhE3oJjS+GveXBuE/FWBsRiu35V6S5Q4kXwympCSJGUc/Toddq2DeTPP6/ax/r3r8qkSc3w9nY3MVnaoJIlIiIiGVNMhG1FwCPz4NRPEBsZ/zF5qtlmrEp3hEwBKZ9RxASHD1+jZs3p3LkTBYCvrzvffNOSLl0qmpws7VDJEhERkYzDGgvnf7EdCnh8qW0G69+yFvu7WHWGHKVTOqGI6UqXzkm9eoX46afjlCuXi8DAdpQpk8vsWGmKSpaIiIikb4YBV/faZqz+Wgh3L8d/jE9uKNXRdj0r/xpaGVAyNBcXC7Nnt2b8+C2MGdMAX1+P/36SOFDJEhERkfTp5gnbjNWR+XDzaPz73TPZzq8q0wUKNgQXvS2SjGnZsiPkyOFNvXqF7WM5cvjw6adNzQuVxumniYiIiKQfd4Ph6GLbrFXwzvj3u7hDkRa2GauircDdO+UziqQSUVGxvP12EF988Tv+/pnYv38AefJkMjtWuqCSJSIiImlb5G048YNtxurcBjCs8R+Tv55txqpEG/DOnvIZRVKZs2dDad9+CTt3XgQgOPgOM2fu5913nzI5WfqgkiUiIiJpT0wknFn798qAq2wrBf5brsq2GatSHSFzgRSPKJJa/fjjMbp3/4GbN23fNx4ernz+eVNeeqm6ycnSD5UsERERSRsMK1zYbJuxOr4EIm7Gf0zmwrZiVaYL5Cib4hFFUrOYGCsjRmzko4+22seKFMlKYGA7qlXTJQqcSSVLREREUrfwa/Dnd3DgG7h1Ov793jmhVAfbkusBT2plQJEEXLx4m06dlrJlyzn7WOvWpZk583myZvUyMVn6pJIlIiIiqY9hwMXf4I8ptutZxUY53u/uC8Vb/70yYCNwdTclpkhaEB0dy9NPz+T06VAA3Nxc+Pjjxrz2Wk0s+qVEslDJEhERkdQj8hYcngt/TIWQQ/HvL9QYyvWC4s/ZipaI/Cd3d1fGjGlAt24/UKBAZhYvbketWvnNjpWuqWSJiIiI+a7ssRWrI/MhJtzxPq8cUL43VOwP2Yqbk08kjevatSK3b0fSoUM5cuTwMTtOuqeSJSIiIuaIDoe/FsKBqRC8K/79+Z6CSgNty6676ZwRkcT69dczbNx4mtGjGziMDxr0hEmJMh6VLBEREUlZIUdsi1gcmg2RoY73efhB2e5QcQDkqmBKPJG0ymo1+PDD33j//U1YrQZly+aiQ4fyZsfKkFSyREREJPnFRsHxH2yzVud/iX9/7ipQ6SUo3Qk8MqV0OpE07/r1cLp1+4G1a0/YxxYtOqSSZRKVLBEREUk+t87AgW/hzxkQftXxPjcv24WCK70E/k9o6XWRR7Rt23k6dFjChQu3Adu30qhR9Rgxoq7JyTIulSwRERFxLmssnF5jW3799BrAcLw/WynbuVZlu4N3dlMiiqQHhmHw+ec7eOedDcTEWAHIlcuH+fPb0KhRUZPTZWwqWSIiIuIcd4Ph4AzbzFXYOcf7XNyg+Au2WasC9TVrJfKYbt68R69eK1ix4qh9rG7dQixY0IaAAD8TkwmoZImIiMjjMAw4v8m2/PqJH8Aa43i/X0Hb0usV+oCvvzkZRdKhV19d61Cwhg17ijFjGuDm5mJiKrlPJUtERESS7t4NODzbVq5uHvvXnRYo2gIqDoQizcHF1ZSIIunZRx81Yt26E8TGGsyd+wItWpQwO5L8g0qWiIiIJI5hkC3iGK5BS+F4IMREON7vkxsq9IUK/SBLYVMiimQUAQF+rFjRkXz5MlOwYBaz48i/qGSJiIjIw0Xdgb/m47bva+pe/yP+/QUa2BayKN4aXD1SPJ5IevfHH8G8884GFi5sS9ascRfmfvLJAiamkodRyRIREZGEXTtoOxzwyFyICsNhqQrPrFCup+2iwTlKm5NPJJ0zDIMZM/bxyitriIiIoVevFSxb1h6LFo5J9VSyREREJE5MBBxbYitXl7bGu/umZwn8nn4bt7Kdwd3HhIAiGcPdu1G89NJPzJ17wD527twtQkMjyJbN28RkkhgqWSIiIgI3T8CBb+DPmRAR4nifmw+U6UJ0ub5s3n2ZFmVbgLu7OTlFMoDDh6/Rrl0ghw9fs48NGlSdTz9tipeX3r6nBfpXEhERyaisMXByle2iwWeD4t+fo5ztulZlu4JnFoiOBi6neEyRjGTu3D8YOPAnwsOjAciUyYNp01rRsWN5k5NJUqhkiYiIZDRhF+DgdDg4De5ccrzP1QNKtLWVq3x1dNFgkRRy7140r766hunT99nHKlTITWBgO0qVymliMnkUKlkiIiIZgWG1zVb9MdU2e2XEOt6fpahtEYvyvcAnlzkZRTKwpUuPOBSsPn2q8OWXzfHx0aG5aZFKloiISHoWeQsOfGs73yr0pON9Fhco9pxt+fVCjW3bImKKLl0qsHLlUX766ThTpjxL9+6VzI4kj0ElS0REJD2KvAV7v4Q9n0FkqON9mQJsFwyu0Bf88psSTySjs1oNXFziDse1WCxMn/4cFy7cpmxZzSandSpZIiIi6UnkLdj7Bez5PH65KtTYdq5VsVbgorcAImY5deomHTosYcyY+jRvXsI+njmzpwpWOqGfsCIiIulBRCjs+zJ+ubK4QrkeUONdyFbiQc8WkRSyfPlf9Oy5nFu3Iuna9Qf27x9AgQJZzI4lTqaSJSIikpb9V7mqORyyFjUrnYj8LTo6lnff3cBnn+2wj2XP7s3t25EmppLkopIlIiKSFkWE2g4L3Pu57RDB+1SuRFKd8+dv0aHDErZvv2Afa9euLNOnP0fmzJ4mJpPkopIlIiKSljy0XPWEmu+pXImkImvWHKdbtx8ICbkHgLu7C59/3pRBg57AouvQpVsqWSIiImnBg8qVixuU7aFyJZLKxMRYGTVqE+PH/2YfK1w4K4sXt+WJJ/KZmExSgkqWiIhIahYRCnsn2f6oXImkGZcuhfHVV7vs2889V4pZs54nWzZvE1NJSlHJEhERSY0eVq7uHxaYpYg52UTkPxUsmIWZM5+nffslfPjhMwwd+qQOD8xAVLJERERSk4ibsGeSrVxF3Y4bV7kSSdViY61ER1vx8op7e/3CC2U4ceIVChXKal4wMYVKloiISGqgciWSZl29epcuXZYREODHrFnPO8xYqWBlTCpZIiIiZnpouer1d7kqbFI4EfkvmzefpWPHJVy+fAeAevUK0bt3FZNTidlUskRERMwQcdN2AeG9X6hciaRBVqvBxx9vZfjwjcTGGgD4+2eiaNFsJieT1EAlS0REJCWpXImkeSEh4XTvvpzVq4/bxxo2LML8+S+SJ08mE5NJaqGSJSIikhLu3fh7tcAEylX53lBjmMqVSBqwY8cF2rcP5Px52/exxQLvv1+XkSPr4erqYnI6SS1UskRERJLTvRu2Cwjv/QKiwuLGVa5E0hTDMJg0aQdvv72BmBgrADlz+jBv3os0aVLM5HSS2qhkiYiIJAeVK5F0xTBg/fpT9oL11FMFWbiwDfnyZTY5maRGKlkiIiLO9MBy5W4rVzWHQeZC5uUTkUfi4mJh7twXqFr1Gzp3rsC4cQ1xc9PhgZIwlSwRERFnuHcD9nwG+75UuRJJBwzDIDj4Dnnz+tnHcub04c8/B5E5s6eJySQtUMkSERF5HCpXIulOWFgk/fqtYsuWc+zfP4BcuXzt96lgSWKoZImIiDyKeyG2pdgTKlcV+tjOucpc0Lx8IvJIDhy4Qrt2gRw7FgJA164/sHZtFywWi8nJJC1RyRIREUmKeyG2mau9X0L0nbhxlSuRNM0wDGbO3M/gwauJiIgBbLNW/ftXVcGSJFPJEhERSQyVK5F06+7dKAYPXs3s2X/Yx6pU8ScwsB3FimU3MZmkVSpZIiIiD/PQctUXaryrciWShh05co127QI5dOiafWzgwGp8/nkzvLz0VlkejfYcERGRhETchN2fqFyJpGMLF/5J374ruXs3GgBfX3emTWtFp04VTE4maZ1KloiIyD9F3bEtZrFrIkTeihtXuRJJdyIiYuwFq3z53AQGtqN06Zwmp5L0QCVLREQEICYSDnwLv4+D8Ktx4/ZyNQwyFzAvn4g4Xc+eldm8+SwAkye3wMfH3eREkl6oZImISMZmjYXDc2H7B3D7bNy4xQXK9YInR2rmSiSd+OOPYCpV8ncY+/bbVri5uZiUSNIr7VEiIpIxGQYcWwqzK8C6Xo4Fq2Q76HkYmk5XwRJJByIjY3j11TVUrvwNS5cedrhPBUuSg2ayREQkYzEMOBsEv70HV/Y43le4GTz1f5CnqjnZRMTpzpwJpX37QHbtugRA794rqV27AHnz+pmcTNIzlSwREck4Lm23lavzvziOB9SBp8dD/rpmpBKRZLJy5VF69FhOaGgEAB4erkyc2Ah//0wmJ5P0TiVLRETSv2sH4bfhcGqV43iuSvDUeCjSHCwWc7KJiNNFR8cyfPhGPv54m32saNFsBAa2o2rVvCYmk4xCJUtERNKv0JOwdST8tQAw4sazFoc6Y6FUe9sCFyKSbly4cJsOHZawbdt5+1ibNmWYMeM5smTxMjGZZCQqWSIikv6EXYQdY+HPGWCNiRvPlA+eHAXleoKrlmoWSW+2bTvP888v5Pr1cADc3V345JMmvPJKDSyarZYUpJIlIiLpx70Q2Pkh7J8MMRFx4145oOZ7UHkQuOk32SLpVcGCWTAMw3578eK21KyZ3+RUkhGpZImISNoXFQZ7JsHuTyDqdty4hx9UewOqvQ6emU2LJyIpI3/+zHz//YtMmbKbmTOfJ3t2b7MjSQalkiUiImlXTAT8MRV+Hw/3rsWNu3pC5Zehxrvgk9O8fCKSrDZvPkulSnkczrVq1qw4zZoVNzGViC5GLCIiaZE1Bg7OgO9Kwi+vxxUsiytU7A99TkD9T1SwRNIpq9Vg7NhfadBgNn36rLQfIiiSWmgmS0RE0g7DCseWwNb34eYxx/tKd4LaoyFbCXOyiUiKuHbtLl27/sD69ScBWLr0CMuWHaFNm7ImJxOJo5IlIiKpn2HAmbW2a11d3ed4X9Fnoc7/Qe5K5mQTkRTz22/n6NhxCRcvhgHg4mJh9Oj6vPBCGXODifyLSpaIiKRuF36D396Di1scx/PXtV1IOF8dc3KJSIqxWg0+/XQbw4b9TGys7dDAPHl8mT+/DQ0bFjE5nUh8pp+T9fXXX1OkSBG8vLyoVq0aW7Zseejj582bR6VKlfDx8SFv3rz06tWLkJCQFEorIiIp5up+WPYsLHrasWDlrgpt1kL7X1SwRDKAGzfu0br1Qt5+e4O9YNWvX5j9+weqYEmqZWrJWrRoEUOGDGH48OHs27ePp59+mubNm3Pu3LkEH//bb7/RvXt3+vTpw6FDhwgMDGTXrl307ds3hZOLiEiyuXkcfuwEc6vA6dVx49lKQatA6LoLCjcFXVhUJN27fPkOVat+w6pVtnMwLRYYMeJpgoK64e+fyeR0Ig9masn67LPP6NOnD3379qVMmTJMmjSJAgUKMGXKlAQfv2PHDgoXLsyrr75KkSJFeOqppxgwYAC7d+9O4eQiIuJ0YRdgfX+YWQaOLowb9ysATWZAzz+hZFuwmH4QhoikEH9/X6pWzQtAzpw+rFnThbFjG+Lmpp8DkrqZdk5WVFQUe/bs4d1333UYb9KkCdu2bUvwObVr12b48OGsXr2a5s2bc/XqVZYsWcKzzz77wI8TGRlJZGSkffv2bdtFKqOjo4mOjnbCZyKSsPv7l/YzSW5pfl8Lv4bLno9xOTAFS2zcz2vDOxfWJ97FWr4/uHlCrAGxafRzTCfS/L4macb9fSwmJoZvvmmBp6cr48c3IH/+zNr/xKmSa3+yGCZdWODSpUvky5ePrVu3Urt2bfv4+PHjmT17NkePHk3weUuWLKFXr15EREQQExPDc889x5IlS3B3d0/w8R988AGjR4+ONz5//nx8fHyc88mIiEiSuVnDKXZzBcVDV+BmRNjHo118OJG1NSeztiLWxdvEhCKS0k6eDCc8PJYKFfzMjiIZRHh4OJ07d+bWrVtkzpzZaa9r+uqCln8dU28YRryx+w4fPsyrr77KyJEjadq0KZcvX+att95i4MCBzJgxI8HnDBs2jKFDh9q3b9++TYECBWjQoAE5cuRw3ici8i/R0dEEBQXRuHHjB/4SQMQZ0ty+FnMPlwNTcdk9EUtE3MJFhqsX1sqDodpbFPfKTnETI0rC0ty+JmmGYRhMm7aP994LInNmT7Zta8yhQzu0r0myS64F9EwrWTlz5sTV1ZXg4GCH8atXr5InT54EnzNhwgTq1KnDW2+9BUDFihXx9fXl6aefZty4ceTNmzfeczw9PfH09Iw37u7urm9aSRHa1ySlpPp9LTYaDs2E7WPgzsW4cRc3qNAPS60RuGYKwNW8hJJIqX5fkzQlLCySAQN+ZMGCPwG4di2cTz/dSbNm2tck+SXX/mXaWYMeHh5Uq1aNoKAgh/GgoCCHwwf/KTw8HBcXx8iurrb/jk066lFERP6LYYUjC2BWWQga8I+CZYEyXaHXX9Doa8gUYGpMEUl5Bw9e4YknptkLFsArr9Tg448bmZhK5PGZerjg0KFD6datG9WrV+fJJ5/k22+/5dy5cwwcOBCwHep38eJF5syZA0CrVq3o168fU6ZMsR8uOGTIEGrUqEFAgP5zFhFJVQzDtgT7b8Ph2h+O9xV7HuqMhVwVzMkmIqabNWs/gwb9xL17MQD4+XkwY8ZztGtXTotbSJpnasnq0KEDISEhjBkzhsuXL1O+fHlWr15NoUKFALh8+bLDNbN69uxJWFgYkydP5o033iBr1qw0bNiQjz76yKxPQUREEnJhM2x5Dy5tdRwv0ACeGg8BtczJJSKmCw+P5uWXVzNz5n77WOXK/gQGtqN48ezmBRNxItMXvhg0aBCDBg1K8L5Zs2bFG3vllVd45ZVXkjmViIg8ktvnYMNLjhcRBvB/wlauCj6jiwiLZGCGYdCkyVy2bj1vH+vfvyqTJjXD21vnXkn6YXrJEhGRdMAw4M/v4JfXISosbjxHWagzDoq3VrkSESwWC6+9VpOtW8/j6+vON9+0pEuXimbHEnE6lSwREXk8YRdgfT84szZuLFM+eOr/bAtbuGi9QBGJ065dOSZODKVly5KUKZPL7DgiyUIlS0REHo1hwOE5sOk1iLwVN16+N9T/DDyzmJdNRFKFkydvEBh4mHfffcph/K236piUSCRlqGSJiEjS3bkMQf3h1I9xY755ock0KPqseblEJNVYtuwIvXqt4PbtSAIC/OjevZLZkURSjGnXyRIRkTTIMODIPJhdzrFgle0GPQ+pYIkIUVGxDBmyljZtFnP7diQAn3++g9hYq8nJRFKOZrJERCRx7l6xrRx44oe4MZ880PgbKP68eblEJNU4ezaU9u2XsHPnRftYx47l+fbblri66nf7knGoZImIyH87uhg2DIKIkLixUh3hmcngncO8XCKSavz44zG6d/+BmzcjAPDwcGXSpKYMHFgdi1YXlQxGJUtERB4s/Br8PBiOBcaNeeeERlOgZFvzcolIqhETY2XEiI189FHcxceLFMlKYGA7qlULMDGZiHlUskREJGHHl0HQQLh3LW6sRBto9DX45DYvl4ikKsOH/8zEidvs261bl2bmzOfJmtXLxFQi5tLBsSIi4uheCPzUBVa2iStYXtnh2YXQKlAFS0QcvPFGbfLmzYSbmwuff96UZcvaq2BJhqeZLBERiXNiJWwYAHeD48aKPQ+Np4Kvv3m5RCTVyp3bl8DAdri6ulCrVn6z44ikCprJEhERiLgJa3rAiufjCpZnVmg+F57/QQVLRAC4cuUO3br9QEhIuMN4nToFVbBE/kEzWSIiGd3pNbC+L9y5FDdWpIXtwsKZdNK6iNj88ssZOnVaSnDwHUJCwvnxx864uGjVQJGEaCZLRCSjirwF6/rCshZxBcsjMzSdCS/8qIIlIgBYrQbjx2/hmWfmEBx8B4D9+4M5d+6WyclEUi/NZImIZERngmB9Hwg7HzdWuCk0ngaZC5iXS0RSlevXw+nW7QfWrj1hH2vUqCjz5r1I7ty+JiYTSd1UskREMpKoMPj1LTjwTdyYeyao/xlU6Au6YKiI/G3btvN06LCECxduA7YfD6NG1WPEiLq4uupgKJGHUckSEckozm2Edb3h9tm4sYINoel3kLmQeblEJFUxDIPPPtvOu+/+TEyMFYBcuXyYP78NjRoVNTmdSNqgkiUikt5F34XN78D+r+LG3H2h7sdQaQBY9BtpEYmzbt1J3nwzyL5dt24hFixoQ0CAn4mpRNIW/c8qIpKeXdgCsys6Fqz89aD7Aaj8kgqWiMTTtGkxunatCMCwYU/x88/dVbBEkkgzWSIi6VF0OPw2HPZ+ARi2MTdvePpDqPKyypWIPJDFYmHKlGfp0aOSDg8UeUT6X1ZEJL25tB3mVoG9k7AXrIA60P0PqPqqCpaI2N26FUH79oEsX/6Xw3imTB4qWCKPQTNZIiLpRUwEbBsOez4Fw3ayOq6e8NR4qPoauLiam09EUpX9+4Np1y6QEydusH79SfbuHUDRotnMjiWSLqhkiYikA1kjjuG24B24eTRuMG9NaDoLcpQ2LZeIpD6GYTB9+l5eeWUNkZGx9vHTp2+qZIk4iUqWiEhaFhOJy7aR1L3wCRbuz155QO2xUH0ouOjHvIjEuXMnipde+onvvz9gH6tWLS+LF7dTwRJxIv3vKyKSVl3ZA2t74nr9z7ixPNWg2WzIWc68XCKSKh0+fI22bRdz5Mh1+9jgwU/w6adN8PTUW0IRZ9J3lIhIWhMbBTv+D37/PzBsh/pYccN48n1caw4DV3eTA4pIajN37h8MHPgT4eHRgG1hi+nTW9GhQ3mTk4mkTypZIiJpydU/YG0PuPaHfcjIWYlfvXvx1BODcFXBEpF/uXHjHkOGrLMXrAoVcrNkSXtKlsxhcjKR9Evr+IqIpAWx0bB9LMyrHlewXNzgyVHEdNjGbc/CpsYTkdQre3Zv5sxpDUCfPlX4/fe+KlgiyUwzWSIiqd31P2FtT9s5WPflrADNZkGeqhAdbVYyEUmlYmOtuLrG/S792WdLsmdPf6pWzWtiKpGMQyVLRCS1ssbArk9g+yjbeVgAFleo8S7Ueh/cPM3NJyKpTmRkDEOHruPGjQjmz38Ri8Viv08FSyTlqGSJiKRGIUdss1fBO+PGspeB5rPB/wnTYolI6nXq1E3atw9kz57LANStW5CXXtLPCxEzqGSJiKQm1ljY8zlsHQGxkbYxiwtUfxNqjwY3L3PziUiqtHz5X/TsuZxbt2w/Nzw9XbUsu4iJ9N0nIpJaXP0DgvpB8K64sWwlbedeBTxpWiwRSb2io2N5990NfPbZDvtYiRLZCQxsR6VK/iYmE8nYVLJERMwWHQ7bx8DuT+zXvQILVHsd6owDd29T44lI6nTu3C06dFjCjh0X7GPt2pVl+vTnyJxZ52yKmEklS0TETGeCYMNAuHUqbix7GWj8LeR/yrxcIpKqrV59nG7dfuDGjXsAuLu78PnnTRk06AmHxS5ExBwqWSIiZgi/Br++AYfnxo25ekDNEfDE21o5UEQeatq0vfaCVbhwVgID21G9eoDJqUTkPpUsEZGUZBhweA788gZEhMSN569rm73KXsq8bCKSZnz33XPs3x9MpUp5mDnzebJl02HFIqmJSpaISEq5eQI2DIBzG+PGPLNCvU+gfC/bKoIiIgkIC4vEzy9uhjtbNm+2bu1N3ryZdHigSCqk/9FFRJJbbDT8PgHmVHAsWKU6Qq8jUKGPCpaIJCg21soHH/xCqVKTuXw5zOG+gAA/FSyRVEozWSIiyenSDgjqD9cPxo1lLgTPfA1FW5iXS0RSvStX7tClyzJ+/vk0AJ06LWXDhu64uemXMiKpnUqWiEhyiLwNvw2H/V8Bhm3M4gJVh9guKuyRycx0IpLKbd58lo4dl3D58h0AXFwsNG1aDBcXzVyJpAUqWSIiznZiBfw8GO5cjBvLXQWaTIM81czLJSKpntVqMHHiVoYP34jVavsFjb9/JhYsaEP9+oXNDSciiaaSJSLiLGEXYdOrcHxZ3JibD9QZA1VfAxf9yBWRBwsJCad79+WsXn3cPtawYRHmz3+RPHk0+y2Sluh/fBGRx2VY4Y+psGUYRN2OGy/cDBpNgSyFTYsmImnDjh0XaN8+kPPnbT9DLBZ4//26jBxZD1dXnYMlktaoZImIPI7rh2B9P7i8PW7MJzc0+AJKdbC9UxIR+Q+nT9+0F6xcuXyYN+9FGjcuZnIqEXlUKlkiIo8iJgJ+/z/Y+RFYo+PGy/eBuhPBO7t52UQkzenUqQKbN5/lzz+vsXBhG/Lly2x2JBF5DCpZIiJJdW6T7aLCN+POmyBbSWj8DRSob1osEUk7zp27RcGCWRzGJk1qhquri5ZoF0kH9F0sIpJY90JgbW8IbBhXsFzcodb70P0PFSwR+U+GYfD117soUeJ/zJ9/0OE+T083FSyRdEIzWSIi/8Uw4K8FsGkI3LsWNx5QB5p8CznKmhZNRNKOsLBI+vVbxaJFhwDo338VTzwRQIkSOUxOJiLOppIlIvIwt07DhpfgzLq4MY/MUPcjqNjfdoFhEZH/cODAFdq1C+TYsRD7WL9+VSlUKKt5oUQk2ahkiYgkxBoDeybBtpEQcy9uvEQbaPglZAowLZqIpB2GYTBz5n4GD15NREQMAJkze/Ldd8/Rpo1mwUXSK5UsEZF/C95tW5b92v64sUz54ZmvoPhzpsUSkbTl7t0oBg9ezezZf9jHqlTxJzCwHcWKaQVSkfRMJUtE5L6oO7D1fdj3pe0CwwBYoMor8NQ48PAzNZ6IpB0nTtygdeuFHDoUdx7nSy9V57PPmuLlpbdfIumdvstFRABO/QQbBkHYubixXBWh8TTIW8O8XCKSJvn6unP9erj99rRprejUqYLJqUQkpeiMbRHJ2O4Gw6oO8EPLuILl5gVPfwhddqtgicgjyZvXjwUL2lCpUh527+6vgiWSwWgmS0QyJsMKB2fA5rchMjRuvGAjaDwVshYzLZqIpD3Hj4eQI4cP2bN728caNCjC3r0DcHGxmJhMRMygmSwRyXhC/oJF9SGof1zB8s4JzedA2/UqWCKSJIGBh6hW7Vt69FiO1Wo43KeCJZIxqWSJSMYREwnbRsPcSnBxS9x4uR7Q8wiU7QYWvSESkcSJjIzhlVdW0779EsLCovjxx2NMm7bH7FgikgrocEERyRgubLHNXN34K24sazFo9A0Uesa8XCKSJp0+fZP27Zewe/cl+1jnzhXo0qWiialEJLVQyRKR9C3iJmx+Bw5OixtzcYPqb0Gt98Hd+8HPFRFJwMqVR+nRYzmhoREAeHq68uWXzenXryoWzYaLCCpZIpKenVgBGwbaVhC8L29NaPytbXl2EZEkiI6O5b33fuaTT7bbx4oVy0ZgYDuqVMlrYjIRSW1UskQk/bl3Aza9CkfmxY15+MFT46HSS+Dial42EUmT7tyJolmz79m69bx9rE2bMsyY8RxZsniZmExEUiOVLBFJX06usp179c/Zq6ItodEU8MtvXi4RSdN8fd0pUCALcB53dxc+/bQJL79cQ4cHikiCVLJEJH2IuAmbXoPDc+PGPLNCgy+0aqCIPDaLxcK337bkxo17jB3bgBo18pkdSURSMZUsEUn7Tv749+zV5bixos/azr3KFGBeLhFJsy5fDuPkyZs89VRB+5ifnyfr1nU1MZWIpBUqWSKSdkXchE1D4PCcuDHPLH/PXnXX7JWIPJKNG0/TqdNSoqNj2bdvAIUKZTU7koikMboYsYikTad+gtnlHQtWkebQ45Dt4sIqWCKSRLGxVsaM+ZVGjeZw9epdbt6MYMiQdWbHEpE0SDNZIpK2RITCL6/DoVlxYx6ZocEkKNdT5UpEHsnVq3fp2nUZQUGn7GNNmhTj229bmphKRNIqlSwRSTtOr4H1/eDOxbixws2gyTStHCgij2zLlrN07LiUS5fCAHBxsTB6dH3ee+9pXFz0ixsRSTqVLBFJ/SJvwS9D4c/v4sY8MkP9z6F8L81eicgjsVoNPvlkG++99zOxsQYAefL4smBBGxo0KGJyOhFJy1SyRCR1O7MO1vWFOxfixgo3hcbTIHMB83KJSJrXs+dy5s49YN+uX78wCxa0wd8/k4mpRCQ90MIXIpI6Rd6ylaulzeIKloefrVy9uEYFS0QeW9u2ZQHbZPiIEU+zYUM3FSwRcQrNZIlI6nNmPazr4zh7VagxNJkOmQs++HkiIknw3HOlGDu2AU88EUDTpsXNjiMi6YhKloikHpG34dc34OD0uDH3TFD/U6jQT+deicgju3UrgrlzDzB48BNY/vGzZMSIuiamEpH0SiVLRFKHM0Gwvg+EnY8bK9gImk6HzIXMyyUiad7evZdp1y6QU6du4unpSr9+1cyOJCLpnM7JEhFzRd6GoAGwtElcwXLPBI2mQtv1Klgi8sgMw2Dq1N3Urj2DU6duAjBq1C/cuxdtcjIRSe80kyUi5jm7wXbuVdi5uLGCz9jOvcpS2LRYIpL2hYVFMmDAjyxY8Kd97IknAli8uB3e3u4mJhORjEAlS0RSXlQYbH4b/pgaN+buC/U+gYoDdO6ViDyWgwev0K5dIEePhtjHXn21Bh9/3AQPD1cTk4lIRqGSJSIp69xGWNcbbp+NGyvQAJrOgCy6+KeIPJ5Zs/YzaNBP3LsXA4Cfnwffffe8fbl2EZGUoJIlIikj6s7fs1dT4sbcfaHuRKg0ECw6RVREHs/kyTt55ZU19u3Klf0JDGxH8eLZTUwlIhmR3tWISPI7twlmV3AsWAXqQ/cDUHmQCpaIOEWnTuUpUCAzAP37V2Xbtt4qWCJiCs1kiUjyiboDm9+BP76OG3PzgbofqVyJiNPlyOHD4sXtOHnyBl26VDQ7johkYHqHIyLJ4/wvMKeiY8HKXxd6HIAqL6tgichjiYiI4Z13grhy5Y7DeK1a+VWwRMR0mskSEeeKvgub34X9k+PG3Hzg6Q+hymCVKxF5bCdP3qBdu0D27Qtmz57LrFvXFVdX/WwRkdRDJUtEnOfCZljbC26dihvL9zQ0mwlZi5mXS0TSjWXLjtCr1wpu344EYOvW8/zxxxWqVs1rcjIRkTgqWSLy+KLvwpb3YN+XcWNu3n/PXunQQBF5fFFRsbz9dhBffPG7faxkyRwEBrajYsU8JiYTEYlPJUtEHs+FLbCuF4SejBvL9xQ0/Q6ylTAvl4ikG2fPhtK+/RJ27rxoH+vYsTzfftsSPz9PE5OJiCRMJUtEHk10OPz2Huz9EjBsY27e8NR4qPIKuLiaGk9E0ocffzxG9+4/cPNmBAAeHq5MmtSUgQOrY7FYTE4nIpIwlSwRSboLv/09e3Uibiygju3cK81eiYiT7Nt3mVatFti3ixbNxuLFbalWLcDEVCIi/00nSohI4kWHw6bXYVHduILl5gX1PoUOv6pgiYhTVamSl759qwDwwgul2bOnvwqWiKQJmskSkcS5uA3W9YSbx+PG8j5pm73KXsq0WCKSvn35ZXPq1ClIjx6VdHigiKQZmskSkYeLvge/vAELn4orWG5eUO8T6LhFBUtEnCI21srIkZtYvPiQw7i3tzs9e1ZWwRKRNEUzWSLyYJe2w9qecPNY3FjeWtBslsqViDhNcPAdOndeyqZNZ/Dz86ByZX9KlsxhdiwRkUemkiUi8cVEwNaRsOdTMKy2MVdPqDMWqg3VyoEi4jS//HKGTp2WEhx8B4Dw8Gi2bz+vkiUiadojlayYmBh++eUXTp48SefOnfHz8+PSpUtkzpyZTJkyOTujiKSkK3tgTXcIORw3lrcmNJ0JOcqYl0tE0hWr1eDDD3/j/fc3YbXaLgMREODHwoVtePrpQianExF5PEkuWWfPnqVZs2acO3eOyMhIGjdujJ+fHxMnTiQiIoKpU6cmR04RSW6x0fD7/8GOcWDE2sZcPaD2GKj+Brho4ltEnOP69XC6dfuBtWvjLgPRqFFR5s17kdy5fU1MJiLiHEle+OK1116jevXq3Lx5E29vb/v4Cy+8wM8//+zUcCKSQq7/CfNrwfbRcQUrdxXougdqvKOCJSJOs23beapU+cZesCwWGD26PmvXdlHBEpF0I8nvnH777Te2bt2Kh4eHw3ihQoW4ePGi04KJSAqwxsLuT2Hb+xAbZRuzuEKtEVBzOLi6m5tPRNKViIgY2rUL5NKlMABy5/Zl/vwXeeaZoiYnExFxriTPZFmtVmJjY+ONX7hwAT8/P6eEEpEUcPM4LHwatrwTV7BylIUuv0PtD1SwRMTpvLzcmDXreSwWqFu3EPv2DVDBEpF0Kcklq3HjxkyaNMm+bbFYuHPnDqNGjaJFixbOzCYiycGwwr7JMKcSXN7+96AFqr9lOzwwTzVT44lI+mIYhsN248bFWL++Gz//3J2AAP1yVkTSpyQfLvj555/ToEEDypYtS0REBJ07d+b48ePkzJmTBQsWJEdGEXGW22dhXW84tzFuLGsxaDYb8tUxL5eIpDuGYTB58k62bbvA/PkvOlxMuFEjzV6JSPqW5JIVEBDA/v37WbhwIXv27MFqtdKnTx+6dOnisBCGiKQihgF/zoRfhkBUWNx45cFQ9yNw18nmIuI8t25F0LfvKpYssV0KokaNAF5//UmTU4mIpJwkl6zNmzdTu3ZtevXqRa9evezjMTExbN68mbp16zo1oIg8pjuXIagfnPopbsyvADT9Dgo1Mi+XiKRL+/cH065dICdO3LCPXbly18REIiIpL8klq0GDBly+fJncuXM7jN+6dYsGDRokuCiGiJjAMODoIvh5EETcjBsv1wsafA6eWczLJiLpjmEYTJ++l1deWUNkpO29QNasXsya9TzPP1/a5HQiIikrySXLMAyH46rvCwkJwddXhxyJpArh123l6lhg3JivPzSeBsVampdLRNKlO3eieOmln/j++wP2serVA1i8uC1FimQzMZmIiDkSXbJefPFFwLaaYM+ePfH09LTfFxsby4EDB6hdu7bzE4pI0pxYaTs8MPxq3FipDvDMV+Cdw7xcIpIuHTp0lXbtAjly5Lp97OWXn+CTT5rg6akLmYtIxpTon35ZstgOLTIMAz8/P4dFLjw8PKhVqxb9+vVzfkIRSZyIUNvCFodmx415ZYdnvobSHcxKJSLp3MSJ2+wFy8/Pg+nTn6N9+3ImpxIRMVeiS9bMmTMBKFy4MG+++abTDg38+uuv+fjjj7l8+TLlypVj0qRJPP300w98fGRkJGPGjOH7778nODiY/PnzM3z4cHr37u2UPCJp0pkg29Lsdy7EjRVtCU2m2Q4TFBFJJv/7X3O2bTuPr687gYHtKFFCM+YiIkmexx81apTTPviiRYsYMmQIX3/9NXXq1OGbb76hefPmHD58mIIFCyb4nPbt23PlyhVmzJhB8eLFuXr1KjExMU7LJJKmRN2BzW/DH1PixjwyQ4MvoFwPSOD8SRGRxxET43hx4cyZPVm3rit582bC29vdpFQiIqnLIx0svWTJEhYvXsy5c+eIiopyuG/v3r2Jfp3PPvuMPn360LdvXwAmTZrEunXrmDJlChMmTIj3+LVr1/Lrr79y6tQpsmfPDthm1kQypAtbYG1PuHUqbqxgI2g6AzIn/EsKEZHHsXjxYd544whVqjxN0aJxM1ZFi2pxCxGRf0pyyfryyy8ZPnw4PXr0YMWKFfTq1YuTJ0+ya9cuBg8enOjXiYqKYs+ePbz77rsO402aNGHbtm0JPmflypVUr16diRMnMnfuXHx9fXnuuecYO3bsAy+EHBkZSWRkpH379u3bAERHRxMdHZ3ovCJJdX//cvp+FhOBy/ZRuOybhAXbb5QNNx+sT32ItUJ/sLiA9u0MJdn2NZG/RUTE8NZbG/jmG9svUrt0WcbPP3fD3d3V5GSSXunnmqSU5NrHklyyvv76a7799ls6derE7NmzefvttylatCgjR47kxo0b//0Cf7t+/TqxsbHkyZPHYTxPnjwEBwcn+JxTp07x22+/4eXlxQ8//MD169cZNGgQN27c4LvvvkvwORMmTGD06NHxxjdt2oSPj0+i84o8qqCgIKe9VtaI41S98gV+0XHnXoV4lWFf7le5eyEvXFjrtI8laY8z9zWR+4KDI5k48QynTt2zj3l53ePHH9fg6eliYjLJCPRzTZJbeHh4srxukkvWuXPn7Eu1e3t7ExYWBkC3bt2oVasWkydPTtLr/fuaWw+6DheA1WrFYrEwb948+2qHn332GW3btuWrr75KcDZr2LBhDB061L59+/ZtChQoQIMGDciRQyfnSvKJjo4mKCiIxo0b4+7+mOcpxEbhsms8Lrs+wmLYLvJpuHpifXI0mSu/Rj0X/TY5I3PqvibyD8uXH+Wdd37k1i3bESFeXm706ZOXjz7qgIeHh8npJD3TzzVJKSEhIcnyukkuWf7+/oSEhFCoUCEKFSrEjh07qFSpEqdPn8YwjP9+gb/lzJkTV1fXeLNWV69ejTe7dV/evHnJly+fvWABlClTBsMwuHDhAiVKlIj3HE9PT4dret3n7u6ub1pJEY+9r107CGu6w7X9cWN5qmFpNhvXnOVQvZL79HNNnCUqKpZ3393A55/vsI+VKJGd+fNf4OLFPXh4eGhfkxShn2uS3JJr/0ryPH/Dhg1ZtWoVAH369OH111+ncePGdOjQgRdeeCHRr+Ph4UG1atXiTQMHBQU98KLGderU4dKlS9y5c8c+duzYMVxcXMifP39SPxWR1M0aA79/CN9XiytYLm5QezR02g45dR0aEXG+c+duUa/eLIeC1b59OXbv7k+lSgn/ElRERBwleSbr22+/xWq1AjBw4ECyZ8/Ob7/9RqtWrRg4cGCSXmvo0KF069aN6tWr8+STT/Ltt99y7tw5++sMGzaMixcvMmfOHAA6d+7M2LFj6dWrF6NHj+b69eu89dZb9O7d+4ELX4ikSTeOwdoecDnuTQ45ykHzOZCnqnm5RCTd27v3Mjt22M779PBw5fPPm/LSS9WxWCxahEBEJJGSXLJcXFxwcYmbAGvfvj3t27cH4OLFi+TLly/Rr9WhQwdCQkIYM2YMly9fpnz58qxevZpChQoBcPnyZc6dO2d/fKZMmQgKCuKVV16hevXq5MiRg/bt2zNu3LikfhoiqZNhhX2TYcu7EPP3SeYWF6j+lm0Gyy3+oa8iIs7UunVpXnutJitWHCUwsB3VqweYHUlEJM15pOtk/VtwcDD/93//x/Tp07l3795/P+EfBg0axKBBgxK8b9asWfHGSpcurZVmJH26dQbW9YLzv8SNZS0OzWZDvoQPoRUReVw3b94jWzbHo0EmTmzMqFH14o2LiEjiJPqcrNDQULp06UKuXLkICAjgyy+/xGq1MnLkSIoWLcqOHTseuIy6iDyEYcCB6TC7gmPBqvIKdN+vgiUiySYo6CSlSk1m5sx9DuMeHq4qWCIijyHRM1nvvfcemzdvpkePHqxdu5bXX3+dtWvXEhERwZo1a6hXr15y5hRJn+5cgvV94fSauDG/gtBsJhRsaF4uEUnXYmOtjB27mTFjfsUwYPDg1VSvHkCFClrYQkTEGRJdsn766SdmzpxJo0aNGDRoEMWLF6dkyZJMmjQpGeOJpFOGAX8tgI0vQ8TNuPHyfaD+Z+CZ2bxsIpKuXblyhy5dlvHzz6ftY/XqFSZvXj8TU4mIpC+JLlmXLl2ibNmyABQtWhQvLy/69u2bbMFE0q3wa7DhJTi+NG7MNy80mQZFnzUvl4ike5s3n6VjxyVcvmy7FIqLi4Vx4xrwzjtP4eJiMTmdiEj6keiSZbVaHS7W5erqiq+vb7KEEkm3ji+HoP5w71rcWOlO0HAyeGc3LZaIpG9Wq8HEiVsZPnwjVqsBgL9/JhYubEO9eoXNDScikg4lumQZhkHPnj3x9LQtIR0REcHAgQPjFa1ly5Y5N6FIehBxEza9Bofnxo155YBGU6BUO/NyiUi6FxISTvfuy1m9+rh97JlnijBv3ovkyZPJxGQiIulXoktWjx49HLa7du3q9DAi6ZHl7Hr4eQDcuRg3WOx5aPwN+OokcxFJXlarwR9/BANgscDIkfV4//26uLomeoFhERFJokSXrJkzZyZnDpH0JyqMilen4HZiXdyYZxZo8CWU7WZ7tyMiksxy5fJl0aK2tGsXyOzZrWncuJjZkURE0j2nXIxYRP7l/K+4re1Fkdtxq3dRqDE0mQGZC5iXS0TSvdDQCGJjreTI4WMfq1OnIKdOvYaXl/7bFxFJCTpWQMSZou/BptdhcQMsfxcsw93Xdu5Vm3UqWCKSrPbsuUTVqt/Qpcsy+wIX96lgiYikHJUsEWe5sgfmVoG9kwDbm5vrXmWJ6bQbKg3U4YEikmwMw+Drr3dRu/Z3nD4dyrp1J/n4461mxxIRybD0ay0RZ7j6ByxuAFFhtm1XT2KfHMvWy8VokVXnP4hI8gkLi6Rfv1UsWnTIPlarVn46dapgYioRkYxNM1kij+v2OfihRVzBylMduu3DWnUIWFxNjSYi6duBA1eoXn2aQ8F6/fVa/PprTwoWzGJiMhGRjO2RStbcuXOpU6cOAQEBnD17FoBJkyaxYsUKp4YTSfUibsKy5nDnkm07b03o8CvkKGNuLhFJ1wzD4Lvv9lGz5nSOHQsBIEsWT5Yta89nnzXFw0O/4BERMVOSS9aUKVMYOnQoLVq0IDQ0lNjYWACyZs3KpEmTnJ1PJPWKiYAVrSHksG07a3FovQrcfR76NBGRxxETY6VXrxX06bOSiIgYAKpWzcuePf154QX9gkdEJDVIcsn63//+x7Rp0xg+fDiurnG/KatevToHDx50ajiRVMuwwpoecGGzbds7F7RZCz65zM0lIumem5vjf90vvVSdrVt7U6xYdpMSiYjIvyV54YvTp09TpUqVeOOenp7cvXvXKaFEUr1f34Jji2233XzgxZ9AC1yISAr56qsWHD0awmuv1aRjx/JmxxERkX9JcskqUqQI+/fvp1ChQg7ja9asoWzZsk4LJpJq7ZkEez6z3ba4QqvF4P+EqZFEJP26dy+aP/+8yhNP5LOP+fp6sG1bbyy6NISISKqU5JL11ltvMXjwYCIiIjAMg507d7JgwQImTJjA9OnTkyOjSOpxNBB+GRq33WgqFH3WvDwikq4dOxZCu3aBnDkTyt69/R0OCVTBEhFJvZJcsnr16kVMTAxvv/024eHhdO7cmXz58vHFF1/QsWPH5Mgokjpc2AxrunL/QsPUGgkV+5oaSUTSr8WLD9G370rCwqIA6NFjOVu29FK5EhFJAx7pYsT9+vWjX79+XL9+HavVSu7cuZ2dSyR1CTkMy5+HWNubHcr1gtofmBpJRNKnyMgY3nxzPZMn77KPlS6dk6lTW6pgiYikEUleXXD06NGcPHkSgJw5c6pgSfp35xIsbQ6Robbtws2g8TegNzsi4mSnT9/kqadmOhSszp0rsGtXP8qX1/+3IiJpRZJL1tKlSylZsiS1atVi8uTJXLt2LTlyiaQOkbdhWQsIO2fbzl0VWgWCq7u5uUQk3Vmx4i+qVv2W3bttFzf39HTlm29a8v33L5Apk4fJ6UREJCmSXLIOHDjAgQMHaNiwIZ999hn58uWjRYsWzJ8/n/Dw8OTIKGKO2ChY2Qau/WHbzlzYtlS7RyZTY4lI+jN27K+0br2I0NAIAIoXz86OHX3p37+aDhEUEUmDklyyAMqVK8f48eM5deoUmzZtokiRIgwZMgR/f39n5xMxh2HA+r5wboNt2yu77WLDvtrHRcT5/rk8e9u2Zdm9ux+VK+vnjYhIWvVIC1/8k6+vL97e3nh4eBAWFuaMTCLm+204HJ5ru+3mBa1XQfZS5mYSkXSrWbPifPBBPbJn9+bll2to9kpEJI17pJms06dP83//93+ULVuW6tWrs3fvXj744AOCg4OdnU8k5e2fAjsn/L1hgRbzIV9tUyOJSPoRE2Nl/vyDGIbhMD5qVH1eeaWmCpaISDqQ5JmsJ598kp07d1KhQgV69eplv06WSLpwYgVsfDluu+GXUOIF8/KISLpy+XIYnTot5ddfz3Ljxj1efrmG2ZFERCQZJLlkNWjQgOnTp1OuXLnkyCNinks74KdOYFht20+8DVVefvhzREQSaePG03TqtJSrV+8C8M47G+jQoRy5cvmanExERJwtySVr/PjxyZFDxFw3jsEPLSHmnm27TBd4esLDnyMikgixsVb+7/+28MEHv3D/CMF8+fxYtKitCpaISDqVqJI1dOhQxo4di6+vL0OHDn3oYz/77DOnBBNJMXevwLLmEBFi2y7YEJp+B5ZHOmVRRMTu6tW7dO26jKCgU/axpk2LMXfuCypYIiLpWKJK1r59+4iOjrbfFkk3ou7YZrBu/f0GKGcFeG4ZuOrCnyLyeLZsOUvHjku5dMm28q6Li4UxY+ozbNjTuLhocQsRkfQsUSVr06ZNCd4WSdOsMfBjB7iy27adKT+8uBo8s5ibS0TSvBUr/qJNm8XExtqOD8yTx5cFC9rQoEERk5OJiEhKSPLxUL17907welh3796ld+/eTgklkuwMAza8BKdX27Y9s9guNuyX39xcIpIu1KtXmIIFbb+wqV+/MPv3D1TBEhHJQJJcsmbPns29e/fijd+7d485c+Y4JZRIstsxFg5Ot9129YDnl0NOrZgpIs6RNasXgYHteP/9umzY0A1//0xmRxIRkRSU6NUFb9++jWEYGIZBWFgYXl5e9vtiY2NZvXo1uXPnTpaQIk518DvYNipuu9lsKFDftDgikrYZhsGUKbtp3bo0AQF+9vFq1QKoVi3AxGQiImKWRJesrFmzYrFYsFgslCxZMt79FouF0aNHOzWciNOdXgtB/eO2630CpTual0dE0rRbtyLo3Xsly5YdYeHCP9m4sQdublqZVEQko0t0ydq0aROGYdCwYUOWLl1K9uzZ7fd5eHhQqFAhAgL0GztJxa7sgVVtwYi1bVd9Dao9/JIEIiIPsnfvZdq1C+TUqZsAbNlyjqCgkzRvXsLkZCIiYrZEl6x69eoBcPr0aQoWLIjFouVnJQ25dRqWPQvRd23bJdtC/c9A+7GIJJFhGHzzzR6GDFlLZKTtlzbZsnkxe3ZrFSwREQESWbIOHDhA+fLlcXFx4datWxw8ePCBj61YsaLTwok4xb0QWNoMwq/YtvM9Bc3n6mLDIpJkYWGRDBjwIwsW/Gkfe+KJABYvbkfhwlnNCyYiIqlKokpW5cqVCQ4OJnfu3FSuXBmLxYJhGPEeZ7FYiI2NdXpIkUcWfQ9+aAU3j9m2s5eB51eAm9fDnyci8i8HD16hXbtAjh4NsY+9+moNPv64CR4eriYmExGR1CZRJev06dPkypXLflskTbDGwurOcHm7bdvXH9qsAe/sD3+eiMi/nDt3i5o1p3PvXgwAmTN7MmPGc7RtW9bkZCIikholqmQVKlQowdsiqZZhwKbX4MRy27Z7JnhxDWTW/isiSVewYBZ6967CV1/tonJlfwID21G8uH5hIyIiCXukixH/9NNP9u23336brFmzUrt2bc6ePevUcCKPbNfHsP8r220XN3huGeSubGokEUnbPv20Cf/3fw3Zvr2PCpaIiDxUkkvW+PHj8fb2BmD79u1MnjyZiRMnkjNnTl5//XWnBxRJsiPzYcs7cdtNZkDhxublEZE0Z/78gyxY4LjIk6enG++99zReXolemFdERDKoJP9Pcf78eYoXLw7A8uXLadu2Lf3796dOnTrUr1/f2flEkubsz7C2Z9z2U/8H5bqbFkdE0paIiBiGDFnLN9/swcfHnUqV/ClbNpfZsUREJI1J8kxWpkyZCAmxray0fv16GjVqBICXlxf37t1zbjqRpLh2AFa+CNZo23bFAVBjmLmZRCTNOHHiBk8+OYNvvtkDQHh4NIsW/fkfzxIREYkvyTNZjRs3pm/fvlSpUoVjx47x7LPPAnDo0CEKFy7s7HwiiXP7PCxrAVG3bdtFW8Ezk3WxYRFJlKVLD9O790pu344EwMvLja+/bkGvXlVMTiYiImlRkmeyvvrqK5588kmuXbvG0qVLyZEjBwB79uyhU6dOTg8o8p8iQmFZc7hz0badtya0XGhb8EJE5CGiomIZMmQtbdsG2gtWyZI5+P33vipYIiLyyJL8LjRr1qxMnjw53vjo0aOdEkgkSWIiYUVrCDlk285aHFqvAncfU2OJSOp39mwo7dsvYefOi/axjh3L8+23LfHz8zQxmYiIpHWP9Kv+0NBQZsyYwZEjR7BYLJQpU4Y+ffqQJUsWZ+cTeTDDCmt7wIVfbdveuaDNWvDRSeoi8nBWq8Gzz87n0KFrAHh4uPLFF80YMKAaFh1mLCIijynJhwvu3r2bYsWK8fnnn3Pjxg2uX7/O559/TrFixdi7d29yZBRJ2K9vw9FFtttuPvDiT5C1mLmZRCRNcHGx8NVXLXBxsVC0aDa2b+/DwIHVVbBERMQpkjyT9frrr/Pcc88xbdo03NxsT4+JiaFv374MGTKEzZs3Oz2kSDx7v4A9n9puW1yh1WLwf8LcTCKSptSrV5glS9rRoEERsmb1MjuOiIikI480k/XOO+/YCxaAm5sbb7/9Nrt373ZqOJEEHVsCm/5x4etGU6Dos+blEZFUb926E/TosRyr1XAYf+GFMipYIiLidEkuWZkzZ+bcuXPxxs+fP4+fn59TQok80IUtsLor8PcbpVrvQ8V+pkYSkdQrNtbK++9vpHnzecyZ8wcTJ241O5KIiGQAST5csEOHDvTp04dPPvmE2rVrY7FY+O2333jrrbe0hLskr5AjsOJ5iLUts0y5XlBbq1qKSMKCg+/QufNSNm06Yx/bseMCVquBi4vOvRIRkeST5JL1ySefYLFY6N69OzExMQC4u7vz0ksv8eGHHzo9oAgAdy7B0mYQcdO2XbgZNP5GFxsWkQRt2nSaTp2WcuXKXQBcXS2MH/8Mb75ZWwVLRESSXZJLloeHB1988QUTJkzg5MmTGIZB8eLF8fHRdYkkmUTehmXPQtjfh6nmrgqtAsHV3dxcIpLqWK0GEyZsYeTIX+znXwUE+LFoUVueeqqgyelERCSjSPQ5WeHh4QwePJh8+fKRO3du+vbtS968ealYsaIKliSf2ChY1Rau7bdtZy5sW6rdI5OZqUQkFbp+PZwWLeYxYsQme8Fq3Lgo+/YNUMESEZEUleiSNWrUKGbNmsWzzz5Lx44dCQoK4qWXXkrObJLRGVZY3xfOBtm2vbLbLjbs629uLhFJlcaN28y6dScB25HEY8bUZ82aLuTO7WtyMhERyWgSfbjgsmXLmDFjBh07dgSga9eu1KlTh9jYWFxdXZMtoGRgm9+Fw3Ntt928oPUqyF7K3EwikmqNG9eQ9etPEhJyj/nzX+SZZ4qaHUlERDKoRJes8+fP8/TTT9u3a9SogZubG5cuXaJAgQLJEk4ysN2fwu6PbbctLtBiAeSrbW4mEUlVDMPA8o/FbzJl8mD58o74+XmQN68uKSIiIuZJ9OGCsbGxeHh4OIy5ubnZVxgUcZrD38Ovb8ZtN5oKJVqbFkdEUp+dOy9Steq3nD5902G8ZMkcKlgiImK6RM9kGYZBz5498fT0tI9FREQwcOBAfH3jjndftmyZcxNKxnJ6LazrFbddZ6wuNiwidoZhMHnyTt54Yz3R0Vbat1/Cb7/1wtMzyYvlioiIJJtE/6/Uo0ePeGNdu3Z1ahjJ4C7/DivbgPXv2dHKg6HmcHMziUiqcetWBH37rmLJksP2MXd3F27diiR3bpUsERFJPRL9v9LMmTOTM4dkdDeO2q6FFRNu2y7ZDhp8oYsNiwgA+/Zdpl27QE6ejDs88I03nmTChGdwd9fiSyIikrroV39ivrCLsKQJRITYtgs2hOZzwUVvnEQyOsMwmDZtL6++uobIyFgAsmb1Ytas53n++dImpxMREUmYSpaYKyIUljWDsHO27VyV4bkfwM3zYc8SkQzgzp0oBg78kXnzDtrHqlcPYPHithQpks3EZCIiIg+X6NUFRZwu+h4sfw6u/2nbzlIU2qwBz8zm5hKRVGH79vMOBeuVV2rw22+9VLBERCTVU8kSc1hjYHVnuLjFtu2TG9qsA19/c3OJSKrRuHEx3nmnDn5+Hixe3JYvv2yuVQRFRCRN0P9WkvIMAzYMghPLbdvumeDFNZCtuKmxRMRckZExeHi4OlxgeNy4hgwYUE2zVyIikqY80kzW3LlzqVOnDgEBAZw9exaASZMmsWLFCqeGk3Rq2yg4OM1228Udnl8OeaqaGklEzHX06HWeeGIa33yzx2Hczc1FBUtERNKcJJesKVOmMHToUFq0aEFoaCixsfdXe8rKpEmTnJ1P0pt9X8GOsX9vWKDF91DoGVMjiYi5Fi78k+rVp3Hw4FVee20te/deNjuSiIjIY0lyyfrf//7HtGnTGD58OK6ucUtsV69enYMHDz7kmZLhHQ2Eja/EbTf4Akq1Ny+PiJgqIiKGQYN+olOnpdy5EwVAsWLZ8PLSkewiIpK2Jfl/stOnT1OlSpV4456enty9e9cpoSQdOrcR1nQFDNt2zfeg6isPfYqIpF+nTt2kXbtAh1mrbt0qMmXKs/j6epiYTERE5PEleSarSJEi7N+/P974mjVrKFu2rDMySXpzZR+saA2xtt9UU7431BlnaiQRMc8PPxyhatVv7AXLy8uNadNaMXt2axUsERFJF5I8k/XWW28xePBgIiIiMAyDnTt3smDBAiZMmMD06dOTI6OkZaEnYVlziAqzbRdtBY2/gX+sHiYiGUNUVCzvvruBzz/fYR8rUSI7gYHtqFRJl28QEZH0I8klq1evXsTExPD2228THh5O586dyZcvH1988QUdO3ZMjoySVt29AkubQvgV23ZAHWi5EFx0voVIRnT3bhTLlh2xb7dvX45p01qRObOnialERESc75GWcO/Xrx9nz57l6tWrBAcHc/78efr06ePsbJKWRd62zWCFnrRt5ygHrVeCu4+5uUTENNmyebN4cTsyZfLgq69asHBhGxUsERFJlx5rSiFnzpzOyiHpSUwkrHwRru6zbfsVgDZrwTu7ublEJEXFxFgJC4skWzZv+1iNGvk4e3YI2bN7P+SZIiIiaVuSS1aRIkWwPOR8mlOnTj1WIEnjDCus6Q7nfrZte2WHNuvAL7+5uUQkRV26FEbHjktwdXVhw4ZuuLrGHTihgiUiIuldkkvWkCFDHLajo6PZt28fa9eu5a233nJWLkmLDAM2vgbHFtu23bzhhZ8gRxlzc4lIigoKOkmXLsu4di0cgA8++IWxYxuanEpERCTlJLlkvfbaawmOf/XVV+zevfuxA0katnMC7J9su21xhVZLIKCWuZlEJMXExloZO3YzY8b8ivH3JfHy589MixYlzA0mIiKSwh5p4YuENG/enKVLlzrr5SStOTAdfhset930Oyjawrw8IpKirly5Q9Om3zN6dFzBat68OPv2DeDJJwuYG05ERCSFOW0t7SVLlpA9uxY2yJBOrIQNA+K2606Ect3NyyMiKWrz5rN07LiEy5fvAODiYmHcuAa8885TuLjomngiIpLxJLlkValSxWHhC8MwCA4O5tq1a3z99ddODSdpwIXf4KcOtgUvAKoNhepvmptJRFKEYRh89NFWhg/fiNVqm77y98/EwoVtqFevsLnhRERETJTkktW6dWuHbRcXF3LlykX9+vUpXbq0s3JJWnD9T1jeCmIibNtlukC9j+Ehq0+KSPphsVg4cybUXrCeeaYI8+a9SJ48mUxOJiIiYq4klayYmBgKFy5M06ZN8ff3T65MkhbcPgtLm0JkqG27cFPbeVgWp53mJyJpwKRJzdi9+xItW5bk/ffrOizVLiIiklElqWS5ubnx0ksvceTIkeTKI2lB+HVY0hTuXLJt+z9hW0nQ1cPcXCKSrAzD4OjREEqXjrsQvZeXG9u29cHDw9XEZCIiIqlLkn/lWLNmTfbt25ccWSQtiL4Ly1vCzaO27WwlbdfC8tDhQSLp2c2b93jhhUVUr/4tf/113eE+FSwRERFHST4na9CgQbzxxhtcuHCBatWq4evr63B/xYoVnRZOUpnYaFjVDi7/btv2zQtt1oFPLnNziUiy2r37Eu3aBXLmTCgA7dsHsnfvANzcdGigiIhIQhJdsnr37s2kSZPo0KEDAK+++qr9PovFgmEYWCwWYmNjnZ9SzGdYYX0fOL3Gtu2ZBdqshSyFTY0lIsnHMAy+/noXQ4euJyrK9rM9e3ZvPvywkQqWiIjIQyS6ZM2ePZsPP/yQ06dPJ2ceSa02vwuH59puu3pC65WQS7OWIunV7duR9Ou3isWLD9nHatXKz6JFbSlYMIuJyURERFK/RJcsw7At0VuoUKFkCyOp1O5PYffHttsWF3h2IeSva24mEUk2f/wRTLt2gRw/fsM+9vrrtfjww0Y6/0pERCQRknROlkXXP8p4Dn8Pv/7j4sKNpkCJ1qbFEZHkNX/+Qfr0WUlERAwAWbJ4MnPm87zwQhmTk4mIiKQdSSpZJUuW/M+idePGjYfeL2nI6bWwrlfcdu0xULG/eXlEJNnlzZvJfv5V1ap5CQxsR9Gi2UxOJSIikrYkqWSNHj2aLFl0LH6GcPl3WNkGrLbfZlNpENQaYW4mEUl2DRoU4YMP6hEcfIdPP22Kl1eSF6EVERHJ8JL0v2fHjh3JnTt3cmWR1OLGUVj2LMSE27ZLtoWGX4IOFxVJd37++RQNGxZxOEphxIi6OjxcRETkMSR6DV79h5tBhF2EJU0gIsS2XaABNP8eXHSyu0h6cu9eNP36raRRo7l89tl2h/v0815EROTxJLpk3V9dUNKxiFBY1gzCztm2c1WG538AN08zU4mIkx07FkKtWjOYPn0fAO+8s4Hjx0NMTiUiIpJ+JPpwQavVmpw5xGzR92D5c3D9T9t2liLQZo3tosMikm4sXnyIvn1XEhYWBYC3txtTpjxLiRI5TE4mIiKSfuiMZrEtbrG6M1zcYtv2zgVt1oOvv7m5RMRpIiNjePPN9UyevMs+Vrp0TgID21G+vM61FRERcSaVrIzOMGDDIDix3Lbtnsk2g5WtuKmxRMR5Tp++Sfv2S9i9+5J9rEuXCkyd2pJMmTxMTCYiIpI+qWRldNtGwcFpttsu7rZzsPJUMzeTiDjN779foFmzeYSGRgDg6enK//7XnL59q2qBCxERkWSikpWR7fsKdoz9e8MCzedCoUamRhIR5ypbNhe5c/sSGhpB8eLZCQxsR+XKOhRYREQkOSV6dUFJZ44GwsZX4rYbfAGlO5iXR0SShZ+fJ4GB7ejatSK7d/dTwRIREUkBppesr7/+miJFiuDl5UW1atXYsmVLop63detW3NzcqFy5cvIGTI/ObYQ1XYG/l+Wv+R5UfeWhTxGRtGHt2hOcO3fLYaxixTzMnfsCWbJ4mZRKREQkYzG1ZC1atIghQ4YwfPhw9u3bx9NPP03z5s05d+7cQ59369YtunfvzjPPPJNCSdORK/tgRWuItS3fTPneUGecqZFE5PHFxhq8//4vNG8+jw4dlhAVFWt2JBERkQzL1JL12Wef0adPH/r27UuZMmWYNGkSBQoUYMqUKQ993oABA+jcuTNPPvlkCiVNJ0JPwrLmEBVm2y7aChp/Azr5XSRNu3z5DiNHnuCjj7YBsGPHBebM+cPkVCIiIhmXaQtfREVFsWfPHt59912H8SZNmrBt27YHPm/mzJmcPHmS77//nnHj/nsGJjIyksjISPv27du3AYiOjiY6OvoR06dB967jtqQplvArAFjz1ia26VyINSA2A30dUtD9/StD7WeS4jZtOkPXrsu5di0cAFdXC+PHN6R79/La98Tp9HNNUor2NUkpybWPmVayrl+/TmxsLHny5HEYz5MnD8HBwQk+5/jx47z77rts2bIFN7fERZ8wYQKjR4+ON75p0yZ8fHySHjyNqnLlSwqGnQTgtkdBfvN6ieigX8wNlUEEBQWZHUHSodhYgyVLrrBwYTDG36dX5sjhzptvFqJUqRDWrFljbkBJ1/RzTVKK9jVJbuHh4cnyuqYv4f7v67QYhpHgtVtiY2Pp3Lkzo0ePpmTJkol+/WHDhjF06FD79u3btylQoAANGjQgR44cjx48DbEE78Rt8UYADM+seHf+hcZ++U1Olf5FR0cTFBRE48aNcXd3NzuOpCNXr96lZ8+VbNgQ9wupKlX8+OGH7gQEZDExmaR3+rkmKUX7mqSUkJCQZHld00pWzpw5cXV1jTdrdfXq1XizWwBhYWHs3r2bffv28fLLLwNgtVoxDAM3NzfWr19Pw4YN4z3P09MTT0/PeOPu7u4Z45vWsMLm1+2bltpjcM9exMRAGU+G2dckRYSGRlCz5ndcvGg7t9LFxcKoUXWpUCGUgIAs2tckRejnmqQU7WuS3JJr/zJt4QsPDw+qVasWbxo4KCiI2rVrx3t85syZOXjwIPv377f/GThwIKVKlWL//v3UrFkzpaKnLYdmQ/Au2+0c5aDyS+bmEZHHkjWrFx06lAPA3z8TGzZ0Y9iwOri4aAEbERGR1MLUwwWHDh1Kt27dqF69Ok8++STffvst586dY+DAgYDtUL+LFy8yZ84cXFxcKF++vMPzc+fOjZeXV7xx+VvkLdjyj4VFGn4JLqYfISoij+nDDxthGPD223Xw98+kE8NFRERSGVPfcXfo0IGQkBDGjBnD5cuXKV++PKtXr6ZQoUIAXL58+T+vmSUPsX0MhF+13S7ZFgrGP5xSRFK333+/wOnToXTsGPfLJHd3Vz77rKmJqURERORhTJ/WGDRoEIMGDUrwvlmzZj30uR988AEffPCB80OlByFHYN+XtttuXlDvE3PziEiSGIbBl1/+zltvBeHq6kKZMjmpVMnf7FgiIiKSCKZejFiSiWHApiFgjbFtP/EuZC5kaiQRSbzQ0Ajatg1kyJB1REdbiYiI4bPPdpgdS0RERBLJ9JksSQYnV8LZ9bbbmQvBE2+bm0dEEm3v3su0axfIqVM37WNvv12bceN0uK+IiEhaoZKV3sREwC9xS7ZT71Nw9zYvj4gkimEYfPPNHl57bS1RUbEAZMvmxezZrWnVqpTJ6URERCQpVLLSm92fwq3TttsFG0KJF83NIyL/KSwskgEDfmTBgj/tYzVq5GPRorYULpzVvGAiIiLySFSy0pPb5+H38bbbFldo8AVYdO0ckdSuTZvFBAWdsm+/9lpNJk5sjIeHq4mpRERE5FFp4Yv0ZPPbEBNuu115MOTU9cNE0oLRo+vj5uZC5syeLFnSjkmTmqlgiYiIpGGayUovLmyGowttt71zQu0PTI0jIon35JMFmDXreWrWzE/x4tnNjiMiIiKPSTNZ6YE1Bja+Erf91HjwymZeHhF5oCNHrjF48E/Exlodxrt0qaiCJSIikk5oJis9ODANrh2w3c5dFcr3NjePiCRo3rwDDBjwI3fvRpM7ty+jRtU3O5KIiIgkA81kpXX3QmDriLjthl+Ci87lEElNIiJiGDBgFV27/sDdu9EALFv2F5GRMSYnExERkeSgmay0butIiLhhu12mK+SrY24eEXFw4sQN2rULZP/+YPtYr16VmTy5BZ6e+hEsIiKSHul/+LTs6h9wYKrttrsv1P3I3Dwi4mDJksP07r2CsLAoALy93fjqqxb06lXF5GQiIiKSnFSy0irDgE2vgvH3yfO13odMAeZmEhEAoqJieeut9Xz55U77WKlSOQgMbEeFCnlMTCYiIiIpQSUrrTq62LZsO0DW4lB1iKlxRCTOJ59scyhYnTqV55tvWuLn52liKhEREUkpWvgiLYq+C7++GbfdYBK46c2bSGrx+uu1qFgxD56erkyd+izz5r2ogiUiIpKBaCYrLdr5Idy5YLtdpAUUfdbcPCLiwNvbncDAdty5E0XVqnnNjiMiIiIpTDNZaU3oKdj1se22izvU/9zcPCIZ3IULt2nRYh7Hj4c4jJcsmUMFS0REJINSyUprfn0DYiNtt6u9DtlLmptHJANbt+4EVap8w5o1J2jXLpB796LNjiQiIiKpgEpWWnJmPZxYbrvt6w+1Rjz04SKSPGJjrbz//kaaN5/H9evhANy8GcH587dNTiYiIiKpgc7JSitio2Djq3HbdSeCh595eUQyqODgO3TuvJRNm87Yx1q2LMns2a3Jnt3bvGAiIiKSaqhkpRX7JsPNo7bbeZ+EMl3MzSOSAW3adJpOnZZy5cpdAFxdLYwf/wxvvlkbFxeLyelEREQktVDJSgvuBsP2D/7esEDDL8GiIz1FUorVajB+/BZGjfoFq9UAICDAj0WL2vLUUwVNTiciIiKpjUpWWrDlPYgKs92u0Af8q5ubRySD2b37EiNHbsKw9SuaNCnG3LkvkDu3r7nBREREJFXSdEhqd3knHJppu+2ZBZ76P3PziGRANWrk4/336+LiYmHMmPqsXt1ZBUtEREQeSDNZqZlhhY2vxG3XHg0+uc3LI5JBGH9PWVkscedZjRxZj5YtS/LEE/nMiiUiIiJphGayUrNDcyB4p+12jnJQaZC5eUQygBs37vH88wv58svfHcZdXV1UsP6/vTuPi6rq/wD+GQaGTUEQUBTEJRFNU8Tcl8w1DNMSTS337TFzQVN7/D2PS4+ZpmblmguauSBuaVFK5o6VIu4bKogLpKCArLNwfn9MjIyAsgxzmeHzfr14de+Ze2e+A0eaD+fcc4mIiKhIOJJVXmWnAMdnPtvv/DUgt5KuHqIK4K+/7qN//1DcuZOCX365iVatPNC6tYfUZREREZGJ4UhWeXXqMyDjb+12/fcAry7S1kNkxoQQ+OabP9G+/QbcuZMCAHB0tEZamlLiyoiIiMgUcSSrPEq6BkR9rd22tAE6LZa2HiIzlpKShZEj92HXrqu6tjZtPBAS0g+eno4SVkZERESmiiGrvBECODwJyFFr91+fATjWlrQkInMVFRWPwMBQ3Lr1RNc2dWobLFjQBVZWcgkrIyIiIlPGkFXe3NoP3Dmo3a5cC3h9urT1EJkhIQTWrj2LiRN/QXa2BgBQpYoNNm58B++84yNxdURERGTqGLLKE3UWcGTKs/03lgBWdtLVQ2SmMjPVWLTopC5gtWhRAzt29EOdOk4SV0ZERETmgAtflCeRS4GU29ptz87aBS+IyODs7KwQGhoIa2s5Pv64JU6cGM6ARURERAbDkazy4uk94I/52m2ZXLtke54boRJR6WRkqGBn9+w2CL6+7rh2bQJq164iXVFERERkljiSVV4cmw6oM7TbzcYDrk2krYfITGRkqDBy5I/o3n0zVCqN3mMMWERERFQWGLLKg3vHgWvbtNs2VYG2c6Wth8hMXL+eiNat12HDhnM4efIuZs36XeqSiIiIqALgdEGp5WiA3z9+tt/hc8CG14YQldb27ZcwevR+3Q2F7eys0KSJm8RVERERUUXAkCW1i2uBR+e1226+QOOR0tZDZOKystQICjqAVavO6NoaNXJFaGggGjVylbAyIiIiqigYsqSU+Rg4MevZ/pvfAha8ASpRSd2+/QSBgaE4ezZe1/bhh69h1apesLdXSFgZERERVSQMWVKK+C+Q9Vi73XAwULOdtPUQmbA9e65i+PAfkZKSDQCwsbHE8uVvYcQIX8i4UicREREZEUOWVB5dAM6v0m5b2QMdFkpbD5GJO3IkVhew6td3RmhoIJo2rS5xVURERFQRMWRJQQjg94mAyNHut/o/oHJNaWsiMnFfftkdf/xxH3XqVMF33wXAwcFa6pKIiIiogmLIksKNUODeUe12lXqA3xRp6yEyQfHxT+HuXlm3r1DIER7+ISpXVnB6IBEREUmK98kyNlU6cHTas/03lgGW/Is7UVGpVBrMmBGOV175Fhcv/q33mIODNQMWERERSY4hy9j+Wgg8vavdrvMWULeXtPUQmZD791Px5pvfY9GiCGRkqBAYGIr0dKXUZRERERHp4XRBY0qJAU4v0m5bWAFvfAXwr+5ERXLw4C0MHrwbiYkZAABLSwuMG9cCdnZWEldGREREpI8hy5iOTAU02tXP0Hwy4NxA0nKITIFGk4N5847is8+OQQhtm6enA3bsCETr1h7SFkdERERUAIYsY4kNB27u0W7bVwda/5+09RCZgL//TsOgQbvx++8xujZ///r4/vs+qFrVTsLKiIiIiArHkGUMGhVweNKz/Q4LAWsH6eohMgEnTsQhMDAUCQlpAAC5XIb589/EJ5+0g4UFp9kSERFR+cWQZQznVgCPr2q33VsDjT6Qth4iE/HoUToAwN29ErZv74eOHb0kroiIiIjo5RiyylrGQyBi9j87MuDNbwAZF3Ukepn27WthwYIuOHjwNrZseRdubvZSl0RERERUJPy0X9aOfwooU7XbjUcA1V+Xth6icurixb+RkyP02qZObYtffx3MgEVEREQmhSGrLMX/BVzaoN1WOAAdPpe2HqJySAiBJUsi0Lz5d1iw4LjeYxYWMsjl/DVFREREpoWfXsqKyAEOT3y233YuYOcmXT1E5dCTJ5no2zcE06aFQ63OwX//ewR//XVf6rKIiIiISoXXZJWVK5uB+D+1284NgWYfSVsPUTlz5swDBAaGIjY2Wdc2Y0Y7NG/uLl1RRERERAbAkFUWslOBYzOe7Xf+GpBbSVcPUTkihMDKlacRFHQQSqUGAODsbIvNm/vC37++xNURERERlR5DVln44zMg42/t9it9gdrdpK2HqJxITc3G6NH7sWPHZV1b69YeCAnph1q1HCWsjIiIiMhwGLIMLekacHaZdltuDbyxRNJyiMqL27efoGfPHxAd/VjXFhTUGgsWdIVCIZewMiIiIiLDYsgyJCGAI5OBHLV2//XpgGMdSUsiKi+qVbPXhSlHR2sEB7+Dvn0bSlwVERERkeFxdUFDuhMOxB7Qblf2BFrOlLYeonLE3l6B0NBAdOrkhbNnxzJgERERkdniSJYhxf76bLv954CVnXS1EEnsypVHsLGxRN26Trq2hg1dceTIMOmKIiIiIjICjmQZ0pObz7Y9OklXB5HENm8+j9dfX4vAwFBkZamlLoeIiIjIqBiyDCnllva/cmugck1payGSQGamCqNH78OQIXuRkaHC2bPxWLw4QuqyiIiIiIyK0wUNReQAyf+ErCr1ABnzK1UsN24kITAwFBcu/K1rGznSF1OntpGwKiIiIiLjY8gylKf3AU22drvKK9LWQmRkO3ZcxqhR+/D0qRIAYGdnhVWremHIkKYSV0ZERERkfAxZhpKc53oshiyqILKz1Zg27SCWLz+ta2vY0AWhoYF49VU3CSsjIiIikg5DlqHohax60tVBZCRKpQYdOgTj9OkHurbBg5tg9eq3UamSQsLKiIiIiKTFC4cMhSNZVMEoFHJ061YXAGBtLcd3372NzZv7MmARERFRhceRLENhyKIKaO7czoiPT8PEia3QrFl1qcshIiIiKhc4kmUouSHLwhJwqCVtLURl4O7dFOzadUWvzdLSAhs2vMOARURERJQHR7IMQYhnNyJ2rKMNWkRm5JdfovHhh3uQmpqNkycd8frrvA8cERERUWE4kmUI6QmAOkO7zamCZEbU6hzMmnUI/v5bkZSUCZUqBzNm/CZ1WURERETlGodcDCH3JsQAQxaZjQcPnmLQoF04evSOri0gwBubNvWRrigiIiIiE8CQZQhc9ILMzKFDtzFo0G48fJgOAJDLZVi4sCuCgtpAJpNJXB0RERFR+caQZQgMWWQmNJoczJ9/HHPmHIEQ2raaNSsjJKQf2rXjgi5ERERERcGQZQgMWWQmxo37CevWRen2e/Soh82b+8LV1V7CqoiIiIhMCxe+MITckCWzABxrS1oKUWmMHdsCCoUcFhYyzJ//JsLCBjNgERERERUTR7JKS4hnIatyLUCukLYeolJo0aIGvvvubXh5VcEbb9SWuhwiIiIik8SRrNLKTAKyU7TbnCpIJiQpKQOzZh2CWp2j1z50aDMGLCIiIqJS4EhWaeW9HsuJIYtMwx9/3EP//qG4ezcVOTkCCxZ0lbokIiIiIrPBkazS4qIXZEKEEPjqq1Po0CEYd++mAgA2bDiHJ08yJa6MiIiIyHxwJKu0eCNiMhHJyVkYMeJH7NlzTdfWvn0tbN/+HpycbCWsjIiIiMi8MGSVFkeyyARERj5AYGAoYmKSdW0zZrTD//73JiwtOaBNREREZEgMWaWVN2Q51pWuDqICCCGwevUZTJ58AEqlBgDg5GSD77/vi7ff9pa4OiIiIiLzxJBVWrkhq5IHYMUpV1S+bNp0HuPHh+n2W7asiR07+sHLq4p0RRERERGZOc4TKo2sZCAzUbvNlQWpHBo4sDH8/NwBAJMnt8Lx48MZsIiIiIjKGEeySiMlz6IXjvWkq4OoENbWltixIxDnziXg3XcbSl0OERERUYXAkazSeMJFL6j8SE9XYuzY/bhy5ZFee926TgxYREREREbEkFUavBExlRNXrz5Cq1br8N13ZxEYGIr0dKXUJRERERFVWAxZpcHl26kc2LLlAl5/fS0uX9aOYN25k4xz5xIkroqIiIio4uI1WaWhF7J4TRYZV1aWGpMm/YLvvjura2vc2A2hoYHw8XGRsDIiIiKiio0hqzSS/1n4wq4aoKgsbS1Uody8+RiBgaF6I1bDhzfD8uX+sLOzkrAyIiIiImLIKilVOpAer93mVEEyotDQyxg5ch+ePtVed2Vra4kVK/wxfLivxJUREREREcCQVXLJeZZv56IXZCS3bj3GwIG7oNEIAECDBlWxc2d/NG7sJnFlRERERJSLC1+UFBe9IAnUq+eMefM6AwAGDWqCM2fGMGARERERlTMcySop3iOLJDJzZns0aeKGt9/2hkwmk7ocIiIiInoOR7JKiisLUhlTqTT45JODWLTopF67hYUMAQENGLCIiIiIyimOZJUUpwtSGbp3LxUDBuxERMRdyOUytG7tgY4dvaQui4iIiIiKgCNZJZUbsmycARsnaWshs/LrrzfRrNlqRETcBaAdubp587HEVRERERFRUXEkqyRUmcBT7QdgjmKRoajVOZgz5wg+//w4hHbxQNSq5YgdO/qhVSsPaYsjIiIioiJjyCqJ1Jhn2wxZZADx8U8xaNBuHDkSq2t7+21vbNrUB87OttIVRkRERETFxpBVEnnvkcWQRaV0+HAMBg7chb//TgcAyOUyLFjQBVOntoWFBRe3ICIiIjI1DFklkXfRC96ImEpBo8nBxx//ogtYNWpURkhIP7RvX0viyoiIiIiopLjwRUnwHllkIHK5BbZv7wdbW0t0714P586NZcAiIiIiMnEcySoJLt9OpaDR5EAuf/b3jcaN3RARMRJNmrjptRMRERGRaeInupLIDVkKB8DWRdpayGTk5Ah8+eVJdOy4EUqlRu+xZs2qM2ARERERmQnJP9WtXLkSderUgY2NDfz8/HD8+PFCj929eze6desGV1dXODg4oE2bNjhw4IARqwWgUQKpsdrtKvUAGRcmoJd7/DgTffpsx/TpvyEi4i6mTTsodUlEREREVEYkDVkhISGYPHkyZs2ahaioKHTo0AFvvfUW4uLiCjz+2LFj6NatG8LCwhAZGYnOnTsjICAAUVFRxis69Q4gcrTbnCpIRXD69AM0b74G+/ff0LU5OFhD5N4Mi4iIiIjMiqTXZC1duhQjR47EqFGjAADLli3DgQMHsGrVKixYsCDf8cuWLdPb//zzz/Hjjz9i//798PX1NUbJvB6LikwIgZ9+eoRNm76HSqUN5lWr2uKHH95Fz57sO0RERETmSrKQpVQqERkZiZkzZ+q1d+/eHREREUV6jpycHDx9+hTOzs6FHpOdnY3s7GzdfmpqKgBApVJBpVIVu26LpOuQ/7OtrlwHogTPQeYvJSULo0f/hL177+va2rTxwA8/9IGnp0OJ+h5RYXL7E/sVlTX2NTIW9jUylrLqY5KFrMTERGg0GlSrVk2vvVq1akhISCjScyxZsgTp6eno379/occsWLAAc+fOzdd++PBh2NnZFa9oAI0fHUK9f7b/uPYISXfCiv0cZN5u387AokWxSEhQ6tr69HHFBx9UxcWLJ3DxooTFkVkLDw+XugSqINjXyFjY16isZWRklMnzSr6Eu+y5hSOEEPnaCrJt2zbMmTMHP/74I9zc3Ao97tNPP0VQUJBuPzU1FZ6enujcuTOqVq1a7Hrl+74DUrTbrXoMBirVKPZzkHmbNeswEhK011/Z28uxYUMA+vZtJHFVZM5UKhXCw8PRrVs3WFlZSV0OmTH2NTIW9jUylqSkpDJ5XslClouLC+Ryeb5Rq4cPH+Yb3XpeSEgIRo4cidDQUHTt2vWFx1pbW8Pa2jpfu5WVVcn+0abc0v7X0hZWVWpxdUHKZ/78LoiIuIesLDXGjKmCvn0b8X8QZBQl/r1GVEzsa2Qs7GtU1sqqf0m2uqBCoYCfn1++YeDw8HC0bdu20PO2bduGYcOGYevWrejVq1dZl6kvRwOk3NZuV3mFAYsAAE+fZuvtW1nJsWfPABw58iGqVcsf8ImIiIjIvEm6hHtQUBDWrVuHDRs24OrVq5gyZQri4uIwbtw4ANqpfkOGDNEdv23bNgwZMgRLlixB69atkZCQgISEBKSkpBin4Kd3gZx/Lo7jyoIEIDg4Cl5ey3D2bLxeu6urPaytJZ+NS0REREQSkDRkDRgwAMuWLcO8efPQrFkzHDt2DGFhYfDy8gIAxMfH690za82aNVCr1fjoo4/g7u6u+5o0aZJxCuby7fSPjAwVhg//ESNG7MOTJ1kIDAxFcnKW1GURERERUTkg+Z/ax48fj/Hjxxf42MaNG/X2jxw5UvYFvYheyKpX+HFk1q5dS0RgYCguXXqoa+vatQ6sreUvOIuIiIiIKgrJQ5ZJecKRrIpu69aLGDNmP9LTtdNG7e2tsGbN2xg8+DWJKyMiIiKi8oIhqzjyjmQ5MWRVJFlZakyZ8itWr47Utb36qitCQwPRsKGrhJURERERUXnDkFUcuSFLrgAqeUhbCxnNrVuPERgYiqioZ7cbGDq0KVas8Ie9vULCyoiIiIioPGLIKiqR82z5dse6gAWvv6kokpIydddf2dhYYuVKfwwf7itxVURERERUXkm6uqBJSYsH1JnabV6PVaG0bFkTixd3h7d3Vfz55ygGLCIiIiJ6IYasouLy7RXG/fup0Ghy9No+/rglzp4dg9deqyZRVURERERkKhiyioohq0L46acbaNJkFebOParXLpPJeP0VERERERUJQ1ZRcWVBs6ZSaTBjRjgCArbhyZMs/O9/x3Do0G2pyyIiIiIiE8SFL4qKI1lm6/79VLz//i6cOBGna3vnHR/4+dWQsCoiIiIiMlUMWUWVeyNimRyoXEvaWshgDh68hcGDdyMxMQMAYGlpgS+/7IZJk1pBJpNJXB0RERERmSKGrKIQ4tlIlmNtQG4laTlUehpNDubNO4rPPjsGIbRtnp4O2LEjEK1b8x5oRERERFRyDFlFkfEQUKVptzlV0OQlJWWgf/+d+P33GF2bv399fP99H1StaidhZURERERkDrjwRVHweiyzYm+vwOPH2nueyeUyfPFFF+zfP5ABi4iIiIgMgiGrKJJvPdtmyDJ5NjaWCA0NhI+PC37/fShmzGgPCwtef0VEREREhsHpgkXBkSyTlpiYgdTUbNSt66Rre+UVZ1y+PJ7hioiIiIgMjiNZRcGQZbIiIu7C13cN+vTZjowMld5jDFhEREREVBYYsopCF7JkgGMdSUuhohFCYMmSCHTqtBH37qXi4sWH+Pe/D0ldFhERERFVAJwuWBS5IcuhFmBpLW0t9FJPnmRi+PAf8eOP13VtHTt6Yfr0dhJWRUREREQVBUPWy2Q+BrKeaLc5VbDcO3PmAQIDQxEbm6xr+/TT9pg3rzMsLTlwS0RERERljyHrZfSux6onXR30QkIIrFx5GkFBB6FUagAAzs622Ly5L/z960tcHRERERFVJAxZL8NFL8o9IQQGD96Nbdsu6dpat/ZASEg/1KrlKGFlRERERFQRcf7UyzBklXsymQw+Pi66/aCg1jh6dBgDFhERERFJgiNZL8OQZRJmzeqAS5ceYtCgJujTx0fqcoiIiIioAmPIepnkW8+2q9SVrg7SSUtT4vjxO3jrrWfXWsnlFtixI1DCqoiIiIiItDhd8GVyR7Iq1QCs7KWthXDlyiO0bLkWvXtvR0TEXanLISIiIiLKhyHrRbJTgYyH2m1OFZTc5s3n8frra3H1aiLU6hyMGrUPOTlC6rKIiIiIiPRwuuCL6E0VZMiSSmamCh9//AvWr4/StTVp4oadO/vDwkImYWVERERERPkxZL0IF72Q3I0bSQgMDMWFC3/r2kaO9MW3374FW1srCSsjIiIiIioYQ9aLMGRJKiTkEkaN2o+0NCUAwM7OCqtW9cKQIU0lroyIiIiIqHAMWS/CkCWZBQuO49///l2337ChC0JDA/Hqq24SVkVERERE9HJc+OJF9EJWPenqqID8/evDxkb7N4APPngNf/01mgGLiIiIiEwCR7JeJDdk2boC1g7S1lLBNG1aHatW9YJKpcGoUc0hk3GBCyIiIiIyDQxZhVFlAGkPtNucKlimlEoNVq48jfHjX4dCIde1DxvWTLqiiIiIiIhKiCGrMCm3n207MWSVlbi4FAwYsBN//HEPcXEpWLq0h9QlERERERGVCq/JKswTLnpR1sLCouHruwZ//HEPALBixWnExiZLWxQRERERUSkxZBWGKwuWGbU6B59++ht69dqKx48zAQC1a1fByZMjULt2FWmLIyIiIiIqJU4XLAxDVpl48OApBg7chWPH7uja3nmnAYKD34GTk62ElRERERERGQZDVmEYsgzut99uY9CgXXj0KAMAYGlpgYULu2LKlNZcPZCIiIiIzAZDVmFyQ5aNE2DrLG0tZuDgwVvo2fMHCKHd9/BwQEhIP7Rt6yltYUREREREBsZrsgqizgZS47TbHMUyiM6da6N1aw8AQM+eryAqaiwDFhERERGZJY5kFSQlBsA/Qy6O9SQtxVxYWckREtIPISGXERTUBhYWnB5IREREROaJI1kFyXs9Fu+RVWw5OQILF57A+fMJeu2eno6YNq0tAxYRERERmTWGrIKk3Hq2zemCxZKUlIGAgG2YOfMQAgNDkZqaLXVJRERERERGxZBVEN6IuET++OMefH3XICwsGgBw8+ZjHDx46yVnERERERGZF4asgnD59mIRQuCrr06hQ4dg3L2bCgBwdbXDgQMfoF+/RhJXR0RERERkXFz4oiC5IcuqEmDnJm0t5VxychZGjPgRe/Zc07W1b18L27e/h5o1HSSsjIiIiIhIGgxZz9OogNRY7XaVVwDeJLdQkZEPEBgYipiYZF3bjBnt8L//vQlLSw6SEhEREVHFxJD1vKdxQI5au82VBQuVlJSBTp02Ij1dBQBwcrLB99/3xdtve0tcGRERERGRtDjc8Dxej1UkVavaYd68zgCAVq1qIipqLAMWERERERE4kpUfVxYssilTWsPR0RofftgUCoVc6nKIiIiIiMoFjmQ9T28kq550dZQjQgisX38WCxee0GuXyWQYObI5AxYRERERUR4cyXoepwvqSU9XYvz4MHz//XnIZECLFjXQpUtdqcsiIiIiIiq3OJL1vOR/bp5raQNUqiFtLRK7evURWrZch++/Pw8AEAL4/fcYiasiIiIiIirfOJKVV44GSPknZDnWA2QVN4P+8MMFjB37EzIytKsH2ttbYe3aAAwc2ETiyoiIiIiIyjeGrLzS7gMapXa7gk4VzMxUYdKkX7F27VldW+PGbggNDYSPj4uElRERERERmQaGrLwq+PVY0dFJCAwMxfnzf+vahg9vhuXL/WFnZyVhZUREREREpoMhK6+8IauC3YhYCIERI/bpApatrSVWruyFYcOaSVsYEREREZGJqbgXHRWkAt8jSyaTYd26AFSqpECDBlXx11+jGbCIiIiIiEqAI1l5VbDpgkIIyGQy3X6DBi749dfBaNq0OipVUkhYGRERERGR6eJIVl65IcvCCqjsKW0tZWzfvuvo0uV7ZGaq9NrbtavFgEVEREREVAoMWbmEeBayHOsAFnJp6ykjKpUGn3xyEO+8sx2HD8di8uRfpS6JiIiIiMiscLpgrvR4QJ2p3TbTqYL37qViwICdiIi4q2tLSsqESqWBlZV5hkoiIiIiImNjyMqVfOvZthmGrF9/vYkPPtiNpCRtkLSyssDixd3x8cct9a7LIiIiIiKi0mHIymWmi16o1TmYM+cIPv/8OITQtnl5OWLHjkC0bFlT2uKIiIiIiMwQQ1YuM7xHVnz8UwwatBtHjsTq2t5+2xubNvWBs7OtdIUREREREZkxLnyRywzvkbVx4zldwJLLZVi0qCt+/PF9BiwiIiIiojLEkaxcuSNZMjng4CVtLQYyfXo7hIffxvXrSQgJ6Yf27WtJXRIRERERkdljyAL0l2938ALkpnmfqOdXCZTLLbBt23uwsJDB1dVewsqIiIiIiCoOThcEgMxEQJmq3TbRqYLHj9+Bt/dy/PnnPb32atUqMWARERERERkRQxZg0isL5uQILFx4Ap07b0JsbDL699+JpKQMqcsiIiIiIqqwOF0QeC5k1ZOujmJKSsrA0KF78fPP0bq2unWdoFbnSFgVEREREVHFxpAFmOSNiP/88x7699+JuLgUAIBMBsya1QFz5rwBuZwDlEREREREUmHIAkzqHllCCHzzzZ/45JNwqFTaESsXFzv88ENf9OhRvmsnIiIiIqoIGLKAPCFLBjjWlbSUF0lJycKIEfuwe/dVXVu7dp7Yvr0fPDwcJKyMiIiIiIhycV4Z8OxGxJU9AEsbaWt5gdjYZPz88w3d/vTpbXH48FAGLCIiIiKicoQhK+sJkJWk3S7n12M1bVodX3/dE05ONti3730sXNhN775YREREREQkPU4XLMeLXqSlKWFtLdcLUmPG+KFv34Zwc+O9r4iIiIiIyiOOZJXTe2RdvPg3/Py+w//93+967TKZjAGLiIiIiKgc40hWOVxZMDg4Ch99FIbMTDUWLYpA+/a1EBDQQOqyiIiIqJg0Gg1UKpXUZZgclUoFS0tLZGVlQaPRSF0OmTgrKyvI5ca9xIYhqxyNZGVkqPDRR2HYuPGcrq1Zs+po2NBVuqKIiIioRNLS0nDv3j0IIaQuxeQIIVC9enXcvXsXMplM6nLIxMlkMnh4eKBSpUpGe02GrCd5QpaEy7dfu5aIwMBQXLr0UNc2dqwfli3rCRsb/piIiIhMiUajwb1792BnZwdXV1cGhWLKyclBWloaKlWqBAsLXt1CJSeEwKNHj3Dv3j3Ur1/faCNa/PSe8s/CF/bVAYXx0m1eW7dexJgx+5Gerp1OYG9vhTVr3sbgwa9JUg8RERGVjkqlghACrq6usLW1lbock5OTkwOlUgkbGxuGLCo1V1dXxMbGQqVSMWQZhTINSE/QbkswVTA7W43Jk3/F6tWRurZXX3XFzp394ePjYvR6iIiIyLA4gkUkPSn+HVbsPw1IvHy7TCbD2bMJuv2hQ5vizz9HMWAREREREZmwCh6ypF30QqGQIySkH2rUqIwNG3pj48Y+sLdXGL0OIiIiIiIyHIasXEYIWUqlBnfvpui11a5dBbduTcTw4b5l/vpEREREVDaSkpLg5uaG2NhYqUupUH766Sf4+voiJydH6lL0MGTlKuN7ZN25k4wOHYLRvfsPSEtT6j3G1QOJiIioPBg2bBhkMhlkMhksLS1Rq1Yt/Otf/8KTJ0/yHRsREQF/f384OTnBxsYGTZo0wZIlSwq8r9Xhw4fh7++PqlWrws7ODo0aNcLUqVNx//59Y7wto1iwYAECAgJQu3btfI91794dcrkcf/zxR77H3njjDUyePDlf+969e/NdS6RUKrFo0SI0bdoUdnZ2cHFxQbt27RAcHFym92OLi4tDQEAA7O3t4eLigokTJ0KpVBZ6fGxsrK4fPf8VGhqqd+zPP/+MVq1awdbWFi4uLnj33Xf1Hp80aRL8/PxgbW2NZs2a5Xutt99+GzKZDFu3bjXIezUUhqxcjvXK7GV++ukGfH3X4K+/7uPatURMmBBWZq9FREREVBo9e/ZEfHw8YmNjsW7dOuzfvx/jx4/XO2bPnj3o1KkTPDw8cPjwYVy7dg2TJk3C/Pnz8f777+vdG2zNmjXo2rUrqlevjl27duHKlStYvXo1UlJSsGTJEqO9rxeFgtLKzMzE+vXrMWrUqHyPxcXF4dSpU5gwYQLWr19f4tdQKpXo0aMHvvjiC4wZMwYRERH466+/8NFHH+Hbb7/F5cuXS/MWCqXRaNCrVy+kp6fjxIkT2L59O3bt2oWpU6cWeo6npyfi4+P1vubOnQt7e3u89dZbuuN27dqFDz/8EMOHD8f58+dx8uRJDBo0SO+5hBAYMWIEBgwYUOjrDR8+HN9++23p36whiQomJSVFABCJiYlCrPYQYjGEWOFSJq+lVKrF9OkHBTBH91W37tfizJn7ZfJ6VL4olUqxd+9eoVQqpS6FzBz7GhkL+1rRZWZmiitXrojMzEypSymWoUOHinfeeUevLSgoSDg7O+v209LSRNWqVcW7776b7/x9+/YJAGL79u1CCCHu3r0rFAqFmDx5coGv9+TJkwLbNRqNiI2NFaNGjRJubm7C2tpavPrqq2L//v1CCCFmz54tmjZtqnfOV199Jby8vPK9l88//1y4u7sLLy8vMXPmTNGqVat8r9ekSRPx3//+V7e/YcMG4ePjI6ytrUWDBg3EihUrCqwz165du4SLS8GfJ+fMmSPef/99cfXqVVG5cmWRlpam93inTp3EpEmT8p23Z88ekfej+sKFC4WFhYU4e/ZsvmOVSmW+5zWUsLAwYWFhIe7ff/b5ddu2bcLa2lqkpKQU+XmaNWsmRowYodtXqVSiZs2aYt26dUU6v6Cfea7Y2FgBQNy6davAx1/07zExMVEAKNZ7KYqKO09NnQmk3dNul8H1WPfvp+L993fhxIk4XVvfvj7YsOEdVKliY/DXIyIionLuhxbPbh1jTPbVgQ/OlOjU27dv49dff4WVlZWu7eDBg0hKSsK0adPyHR8QEABvb29s27YNAwYMQGhoKJRKJaZPn17g81epUqXA9pycHAQGBiIjIwM//PAD6tWrhytXrhT7HkeHDh2Cg4MDwsPDdaNrX3zxBW7duoV69bSzmC5fvoyLFy9i586dAIC1a9di9uzZWL58OXx9fREVFYXRo0fD3t4eQ4cOLfB1jh07hhYtWuRrF0IgODgYK1asgI+PD7y9vbFjxw4MHz68WO8DALZs2YKuXbvC1zf/dfxWVlZ6P6O84uLi0KhRoxc+9wcffIDVq1cX+NipU6fQuHFj1KhRQ9fWo0cPZGdnIzIyEp07d35p7ZGRkTh37hxWrFihazt79izu378PCwsL+Pr6IiEhAc2aNcPixYvx6quvvvQ58/Ly8oKbmxuOHz+OunXrFuvcslJxQ1Zq7LPtKoadKnjw4C0MHrwbiYkZAABLSwt8+WU3TJrUivfLICIiqqjSE4C08n8N0k8//YRKlSpBo9EgKysLALB06VLd4zdu3AAANGzYsMDzfXx8dMdER0fDwcEB7u7uxarht99+Q2RkJC5fvgwfHx8AKNGHZ3t7e6xbtw4KxbPVm1977TVs3boV//nPfwBow8vrr78Ob29vAMBnn32GJUuW6K4NqlOnDq5cuYI1a9YUGrJiY2P1Qkje95GRkYEePXoA0IaZ9evXlyhkRUdH44033ij2eTVq1MC5c+deeIyDg0OhjyUkJKBatWp6bU5OTlAoFEhIKNofDdavX4+GDRuibdu2urbbt28DAObMmYOlS5eidu3aWLJkCTp16oQbN27A2dm5SM+dq2bNmuVq0ZEKG7JkeiHLcCNZn312FLNnH0HuVGRPTwfs2BGI1q09DPYaREREZILsq5vE63bu3BmrVq1CRkYG1q1bhxs3buDjjz/Od5zIc93V8+25f1TOu10c58+fR40aNXTBp6SaNGmiF7AAYPDgwdiwYQP+85//QAiBbdu26RaeePToEe7evYuRI0di9OjRunPUajUcHR0LfZ3MzEzY2OSfqbR+/XoMGDAAlpbaj9wDBw7EJ598guvXr6NBgwbFei8l/V5aWlrilVdK91m3oNctaj2ZmZl6oTZX7mqAs2bNwnvvvQcACA4OhoeHB0JDQzF27Nhi1Whra4uMjIxinVOWKm7ISol5tmPAkKVQyHUBy9+/Pr7/vg+qVrUz2PMTERGRiSrhlD1js7e3130o/+abb9C5c2fMnTsXn332GQDogs/Vq1f1RiZyXbt2TTc9zdvbGykpKYiPjy/WaJatre0LH7ewsMgX8gpaXc/e3j5f26BBgzBz5kycPXsWmZmZuHv3Lt5//30Azz74r127Fq1atdI770VTFV1cXPKtwPj48WPs3bsXKpUKq1at0rVrNBps2LABCxcuBKAdRUpJ0b/FDwAkJyfrjTB5e3vj6tWrhdZQmNJOF6xevTr+/PNPvbYnT55ApVLlG+EqyM6dO5GRkYEhQ4botef2h7y1WVtbo27duoiLi0NxPX78GK6ursU+r6xU3NUFyyhkffJJO7zzTgN88UUX7N8/kAGLiIiITNrs2bOxePFiPHjwAIB2OXJnZ+cCVwbct28foqOjMXDgQABAv379oFAosGjRogKfOzk5ucD2Jk2a4MGDB7pph89zdXVFQkKCXtB62ZS4XB4eHujYsSO2bNmiu84pNyxUq1YNNWvWxO3bt/HKK6/ofdWpU6fQ5/T19cWVK1f02rZs2QIPDw+cP38e586d030tW7YMmzZtglqtBqCdXnnmTP4Afvr0ab3RrkGDBuG3335DVFRUvmPVajXS09MLrC13uuCLvubNm1foe2vTpg0uXbqE+Ph4XdvBgwdhbW0NPz+/Qs/LtX79evTu3TtfAMpdlv369eu6NpVKhdjYWHh5eb30efPKysrCrVu3CrxeTTIGXUbDBOSuLvgk+A3tyoKLIUT6oxI9l0aTI06ejMvXnpOTU9oyyQxwFS4yFvY1Mhb2taIzp9UFhRDCz89PfPTRR7r90NBQIZfLxejRo8X58+dFTEyMWLdunXBychL9+vXT+yy0YsUKIZPJxIgRI8SRI0dEbGysOHHihBgzZowICgoqsA6NRiPat28vGjduLA4ePChu374twsLCxC+//CKEEOLKlStCJpOJL774Qty8eVMsX75cODk5Fbi6YEG+++47UaNGDeHi4iI2b96s99jatWuFra2tWLZsmbh+/bq4cOGC2LBhg1iyZEmh37cLFy4IS0tL8fjxY11b06ZNxYwZM/Idm5qaKqytrcXevXuFEELExMQIW1tbMX78eHHu3Dlx/fp1sXz5cmFtbS127NihOy8rK0t06NBBODk5ieXLl4tz586JW7duiZCQENG8eXMRFRVVaH2loVarRePGjUWXLl3E2bNnxW+//SY8PDzEhAkTdMfcu3dPNGjQQPz5559650ZHRwuZTKb7uT1v0qRJombNmuLAgQPi2rVrYuTIkcLNzU3v+xgdHS2ioqLE2LFjhbe3t4iKihJRUVEiOztbd8zhw4dFpUqVRHp6eoGvI8XqghU2ZCV/7aUNWN86ClGCUPToUbro2fMHYWExVxw5EmPoMskM8MMIGQv7GhkL+1rRmVvI2rJli1AoFCIu7tkfl48dOyZ69uwpHB0dhUKhEI0aNRKLFy8WarU63/nh4eGiR48ewsnJSdjY2AgfHx8xbdo08eDBgwLr0Gg04vbt22LYsGGiatWqwsbGRjRu3Fj89NNPumNWrVolPD09hb29vRgyZIiYP39+kUPWkydPhLW1tbCzsxNPnz4t8P02a9ZMKBQK4eTkJDp27Ch2795dyHdNq3Xr1mL16tVCCCHOnDkjAIi//vqrwGMDAgJEQECAbv/MmTOiR48ews3NTTg4OIgWLVqIbdu25TsvKytLLFiwQDRp0kTY2NgIZ2dn0a5dO7Fx40ahUqleWF9p3LlzR/Tq1UvY2toKZ2dnMWHCBJGVlaV7PCYmRgAQhw8f1jvv008/FR4eHkKj0RT4vEqlUkydOlW4ubmJypUri65du4pLly7pHdOpUycBIN9XTEyM7pgxY8aIsWPHFlq/FCFLJkQhVy2aqdTUVDg6OiJ5vgyO1gKo5lfsOdIREXcxYMBO3LuXCgCoWbMyoqM/hq1twUtnUsWkUqkQFhYGf3//QpdVJTIE9jUyFva1osvKykJMTAzq1KlT4III9GI5OTlITU2Fg4MDLCxM4+qWsLAwTJs2DZcuXTKZms3Bo0ePdFMuC5vS+aJ/j0lJSXBxcUFKSsoLV1ksroq78EVutizG9VhCCCxdegozZx6CWq29MNLNzR6bNvVhwCIiIiKqwPz9/REdHY379+/D09NT6nIqjJiYGKxcufKF18xJocKGLJ0ihqwnTzIxbNiP2Lfv2cV5HTt6Ydu291CjRuWyqo6IiIiITMSkSZOkLqHCadmyJVq2bCl1GfkwZBUhZJ0+fR/9++9EbGyyru3TT9tj3rzOsLTkcDARERERET3DkFWl3gsf3rbtIoYO3QuVSjs90NnZFps394W/f31jVEdERERERCaGIeslI1lNm1aHlZUcKlUO2rTxQEhIP3h6Fn7HbyIiIqJcFWx9MaJySYp/hxU7ZFnaAfbVX3hIo0auWLPmbURFxeOLL7rCyqrwu30TERERAYBcrv28oFQqYWtrK3E1RBWbUqkE8OzfpTFU7JDl9Aogk+l2hRAIDb2C3r0bwMbm2bfmgw9ewwcfvCZFhURERGSCLC0tYWdnh0ePHsHKyopLehdTTk4OlEolsrKy+L2jUsnJycGjR49gZ2cHS0vjRZ+KHbLyTBVMS1Ni3LifsGXLRYwf3wIrVvSSsDAiIiIyZTKZDO7u7oiJicGdO3ekLsfkCCGQmZkJW1tbyPL8QZyoJCwsLFCrVi2j9iWGLACXLz9Ev36huHYtEQCwcuUZjBjhCz+/GlJWR0RERCZMoVCgfv36uqlKVHQqlQrHjh1Dx44deeNrKjWFQmH0EdEKH7I2bTqHf/3rZ2RmqgEAlSsrsG5dbwYsIiIiKjULCwvY2NhIXYbJkcvlUKvVsLGxYcgikyT5JNfcOzTb2NjAz88Px48ff+HxR48ehZ+fH2xsbFC3bl2sXr26RK+bobTEyPkaDBv2oy5gNW1aDZGRY9C//6slek4iIiIiIiJJQ1ZISAgmT56MWbNmISoqCh06dMBbb72FuLi4Ao+PiYmBv78/OnTogKioKPz73//GxIkTsWvXrmK/dpc1Q7Fhx9+6/dGjm+PUqZGoX79qid8PERERERGRpCFr6dKlGDlyJEaNGoWGDRti2bJl8PT0xKpVqwo8fvXq1ahVqxaWLVuGhg0bYtSoURgxYgQWL15c7Ne+8rcrAMDOzgrff98H330XAFtbDkcTEREREVHpSHZNllKpRGRkJGbOnKnX3r17d0RERBR4zqlTp9C9e3e9th49emD9+vVQqVQFztnNzs5Gdna2bj8lJSX3EdSv74zg4N7w8XFBUlJS6d4Q0XNUKhUyMjKQlJTE+eRUptjXyFjY18hY2NfIWB4/fgzA8DcslixkJSYmQqPRoFq1anrt1apVQ0JCQoHnJCQkFHi8Wq1GYmIi3N3d852zYMECzJ07t4Bn+wrR0UD79tNL/B6IiIiIiMj0JSUlwdHR0WDPJ/nqgs+vVy+EeOEa9gUdX1B7rk8//RRBQUG6/eTkZHh5eSEuLs6g30ii56WmpsLT0xN3796Fg4OD1OWQGWNfI2NhXyNjYV8jY0lJSUGtWrXg7Oxs0OeVLGS5uLhALpfnG7V6+PBhvtGqXNWrVy/weEtLS1StWvCCFdbW1rC2ts7X7ujoyH+0ZBQODg7sa2QU7GtkLOxrZCzsa2Qshr6PlmQLXygUCvj5+SE8PFyvPTw8HG3bti3wnDZt2uQ7/uDBg2jRogXn6xIRERERUbkg6eqCQUFBWLduHTZs2ICrV69iypQpiIuLw7hx4wBop/oNGTJEd/y4ceNw584dBAUF4erVq9iwYQPWr1+PadOmSfUWiIiIiIiI9Eh6TdaAAQOQlJSEefPmIT4+Ho0bN0ZYWBi8vLwAAPHx8Xr3zKpTpw7CwsIwZcoUrFixAjVq1MA333yD9957r8ivaW1tjdmzZxc4hZDIkNjXyFjY18hY2NfIWNjXyFjKqq/JhKHXKyQiIiIiIqrAJJ0uSEREREREZG4YsoiIiIiIiAyIIYuIiIiIiMiAGLKIiIiIiIgMyCxD1sqVK1GnTh3Y2NjAz88Px48ff+HxR48ehZ+fH2xsbFC3bl2sXr3aSJWSqStOX9u9eze6desGV1dXODg4oE2bNjhw4IARqyVTVtzfa7lOnjwJS0tLNGvWrGwLJLNR3L6WnZ2NWbNmwcvLC9bW1qhXrx42bNhgpGrJlBW3r23ZsgVNmzaFnZ0d3N3dMXz4cCQlJRmpWjJVx44dQ0BAAGrUqAGZTIa9e/e+9BxDZAOzC1khISGYPHkyZs2ahaioKHTo0AFvvfWW3lLwecXExMDf3x8dOnRAVFQU/v3vf2PixInYtWuXkSsnU1Pcvnbs2DF069YNYWFhiIyMROfOnREQEICoqCgjV06mprh9LVdKSgqGDBmCLl26GKlSMnUl6Wv9+/fHoUOHsH79ely/fh3btm2Dj4+PEasmU1TcvnbixAkMGTIEI0eOxOXLlxEaGorTp09j1KhRRq6cTE16ejqaNm2K5cuXF+l4g2UDYWZatmwpxo0bp9fm4+MjZs6cWeDx06dPFz4+PnptY8eOFa1bty6zGsk8FLevFaRRo0Zi7ty5hi6NzExJ+9qAAQPE//3f/4nZs2eLpk2blmGFZC6K29d++eUX4ejoKJKSkoxRHpmR4va1L7/8UtStW1ev7ZtvvhEeHh5lViOZHwBiz549LzzGUNnArEaylEolIiMj0b17d7327t27IyIiosBzTp06le/4Hj164MyZM1CpVGVWK5m2kvS15+Xk5ODp06dwdnYuixLJTJS0rwUHB+PWrVuYPXt2WZdIZqIkfW3fvn1o0aIFFi1ahJo1a8Lb2xvTpk1DZmamMUomE1WSvta2bVvcu3cPYWFhEELg77//xs6dO9GrVy9jlEwViKGygaWhC5NSYmIiNBoNqlWrptderVo1JCQkFHhOQkJCgcer1WokJibC3d29zOol01WSvva8JUuWID09Hf379y+LEslMlKSvRUdHY+bMmTh+/DgsLc3q1zyVoZL0tdu3b+PEiROwsbHBnj17kJiYiPHjx+Px48e8LosKVZK+1rZtW2zZsgUDBgxAVlYW1Go1evfujW+//dYYJVMFYqhsYFYjWblkMpnevhAiX9vLji+oneh5xe1rubZt24Y5c+YgJCQEbm5uZVUemZGi9jWNRoNBgwZh7ty58Pb2NlZ5ZEaK83stJycHMpkMW7ZsQcuWLeHv74+lS5di48aNHM2ilypOX7ty5QomTpyI//73v4iMjMSvv/6KmJgYjBs3zhilUgVjiGxgVn/idHFxgVwuz/dXkIcPH+ZLpLmqV69e4PGWlpaoWrVqmdVKpq0kfS1XSEgIRo4cidDQUHTt2rUsyyQzUNy+9vTpU5w5cwZRUVGYMGECAO0HYSEELC0tcfDgQbz55ptGqZ1MS0l+r7m7u6NmzZpwdHTUtTVs2BBCCNy7dw/169cv05rJNJWkry1YsADt2rXDJ598AgB47bXXYG9vjw4dOuB///sfZx6RwRgqG5jVSJZCoYCfnx/Cw8P12sPDw9G2bdsCz2nTpk2+4w8ePIgWLVrAysqqzGol01aSvgZoR7CGDRuGrVu3ch45FUlx+5qDgwMuXryIc+fO6b7GjRuHBg0a4Ny5c2jVqpWxSicTU5Lfa+3atcODBw+Qlpama7tx4wYsLCzg4eFRpvWS6SpJX8vIyICFhf7HVrlcDuDZKAORIRgsGxRrmQwTsH37dmFlZSXWr18vrly5IiZPnizs7e1FbGysEEKImTNnig8//FB3/O3bt4WdnZ2YMmWKuHLlili/fr2wsrISO3fulOotkIkobl/bunWrsLS0FCtWrBDx8fG6r+TkZKneApmI4va153F1QSqq4va1p0+fCg8PD9GvXz9x+fJlcfToUVG/fn0xatQoqd4CmYji9rXg4GBhaWkpVq5cKW7duiVOnDghWrRoIVq2bCnVWyAT8fTpUxEVFSWioqIEALF06VIRFRUl7ty5I4Qou2xgdiFLCCFWrFghvLy8hEKhEM2bNxdHjx7VPTZ06FDRqVMnveOPHDkifH19hUKhELVr1xarVq0ycsVkqorT1zp16iQA5PsaOnSo8Qsnk1Pc32t5MWRRcRS3r129elV07dpV2NraCg8PDxEUFCQyMjKMXDWZouL2tW+++UY0atRI2NraCnd3dzF48GBx7949I1dNpubw4cMv/PxVVtlAJgTHWImIiIiIiAzFrK7JIiIiIiIikhpDFhERERERkQExZBERERERERkQQxYREREREZEBMWQREREREREZEEMWERERERGRATFkERERERERGRBDFhERERERkQExZBERUYls3LgRVapUkbqMEqtduzaWLVv2wmPmzJmDZs2aGaUeIiIyHwxZREQV2LBhwyCTyfJ93bx5U+rSsHHjRr2a3N3d0b9/f8TExBjk+U+fPo0xY8bo9mUyGfbu3at3zLRp03Do0CGDvF5hnn+f1apVQ0BAAC5fvlzs5zHl0EtEZE4YsoiIKriePXsiPj5e76tOnTpSlwUAcHBwQHx8PB48eICtW7fi3Llz6N27NzQaTamf29XVFXZ2di88plKlSqhatWqpX+tl8r7Pn3/+Genp6ejVqxeUSmWZvzYRERkeQxYRUQVnbW2N6tWr633J5XIsXboUTZo0gb29PTw9PTF+/HikpaUV+jznz59H586dUblyZTg4OMDPzw9nzpzRPR4REYGOHTvC1tYWnp6emDhxItLT019Ym0wmQ/Xq1eHu7o7OnTtj9uzZuHTpkm6kbdWqVahXrx4UCgUaNGiAzZs3650/Z84c1KpVC9bW1qhRowYmTpyoeyzvdMHatWsDAPr27QuZTKbbzztd8MCBA7CxsUFycrLea0ycOBGdOnUy2Pts0aIFpkyZgjt37uD69eu6Y1708zhy5AiGDx+OlJQU3YjYnDlzAABKpRLTp09HzZo1YW9vj1atWuHIkSMvrIeIiEqHIYuIiApkYWGBb775BpcuXcKmTZvw+++/Y/r06YUeP3jwYHh4eOD06dOIjIzEzJkzYWVlBQC4ePEievTogXfffRcXLlxASEgITpw4gQkTJhSrJltbWwCASqXCnj17MGnSJEydOhWXLl3C2LFjMXz4cBw+fBgAsHPnTnz11VdYs2YNoqOjsXfvXjRp0qTA5z19+jQAIDg4GPHx8br9vLp27YoqVapg165dujaNRoMdO3Zg8ODBBnufycnJ2Lp1KwDovn/Ai38ebdu2xbJly3QjYvHx8Zg2bRoAYPjw4Th58iS2b9+OCxcuIDAwED179kR0dHSRayIiomISRERUYQ0dOlTI5XJhb2+v++rXr1+Bx+7YsUNUrVpVtx8cHCwcHR11+5UrVxYbN24s8NwPP/xQjBkzRq/t+PHjwsLCQmRmZhZ4zvPPf/fuXdG6dWvh4eEhsrOzRdu2bcXo0aP1zgkMDBT+/v5CCCGWLFkivL29hVKpLPD5vby8xFdffaXbByD27Nmjd8zs2bNF06ZNdfsTJ04Ub775pm7/wIEDQqFQiMePH5fqfQIQ9vb2ws7OTgAQAETv3r0LPD7Xy34eQghx8+ZNIZPJxP379/Xau3TpIj799NMXPj8REZWcpbQRj4iIpNa5c2esWrVKt29vbw8AOHz4MD7//HNcuXIFqampUKvVyMrKQnp6uu6YvIKCgjBq1Chs3rwZXbt2RWBgIOrVqwcAiIyMxM2bN7Flyxbd8UII5OTkICYmBg0bNiywtpSUFFSqVAlCCGRkZKB58+bYvXs3FAoFrl69qrdwBQC0a9cOX3/9NQAgMDAQy5YtQ926ddGzZ0/4+/sjICAAlpYl/1/f4MGD0aZNGzx48AA1atTAli1b4O/vDycnp1K9z8qVK+Ps2bNQq9U4evQovvzyS6xevVrvmOL+PADg7NmzEELA29tbrz07O9so15oREVVUDFlERBWcvb09XnnlFb22O3fuwN/fH+PGjcNnn30GZ2dnnDhxAiNHjoRKpSrweebMmYNBgwbh559/xi+//ILZs2dj+/bt6Nu3L3JycjB27Fi9a6Jy1apVq9DacsOHhYUFqlWrli9MyGQyvX0hhK7N09MT169fR3h4OH777TeMHz8eX375JY4ePao3Da84WrZsiXr16mH79u3417/+hT179iA4OFj3eEnfp4WFhe5n4OPjg4SEBAwYMADHjh0DULKfR249crkckZGRkMvleo9VqlSpWO+diIiKjiGLiIjyOXPmDNRqNZYsWQILC+3luzt27Hjped7e3vD29saUKVMwcOBABAcHo2/fvmjevDkuX76cL8y9TN7w8byGDRvixIkTGDJkiK4tIiJCb7TI1tYWvXv3Ru/evfHRRx/Bx8cHFy9eRPPmzfM9n5WVVZFWLRw0aBC2bNkCDw8PWFhYoFevXrrHSvo+nzdlyhQsXboUe/bsQd++fYv081AoFPnq9/X1hUajwcOHD9GhQ4dS1UREREXHhS+IiCifevXqQa1W49tvv8Xt27exefPmfNPX8srMzMSECRNw5MgR3LlzBydPnsTp06d1gWfGjBk4deoUPvroI5w7dw7R0dHYt28fPv744xLX+Mknn2Djxo1YvXo1oqOjsXTpUuzevVu34MPGjRuxfv16XLp0SfcebG1t4eXlVeDz1a5dG4cOHUJCQgKePHlS6OsOHjwYZ8+exfz589GvXz/Y2NjoHjPU+3RwcMCoUaMwe/ZsCCGK9POoXbs20tLScOjQISQmJiIjIwPe3t4YPHgwhgwZgt27dyMmJganT5/GwoULERYWVqyaiIio6BiyiIgon2bNmmHp0qVYuHAhGjdujC1btmDBggWFHi+Xy5GUlIQhQ4bA29sb/fv3x1tvvYW5c+cCAF577TUcPXoU0dHR6NChA3x9ffGf//wH7u7uJa6xT58++Prrr/Hll1/i1VdfxZo1axAcHIw33ngDAFClShWsXbsW7dq1w2uvvYZDhw5h//79hV6LtGTJEoSHh8PT0xO+vr6Fvm79+vXx+uuv48KFC7pVBXMZ8n1OmjQJV69eRWhoaJF+Hm3btsW4ceMwYMAAuLq6YtGiRQC0KyYOGTIEU6dORYMGDdC7d2/8+eef8PT0LHZNRERUNDIhhJC6CCIiIiIiInPBkSwiIiIiIiIDYsgiIiIiIiIyIIYsIiIiIiIiA2LIIiIiIiIiMiCGLCIiIiIiIgNiyCIiIiIiIjIghiwiIiIiIiIDYsgiIiIiIiIyIIYsIiIiIiIiA2LIIiIiIiIiMiCGLCIiIiIiIgP6fzRraIQX+dAyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.7609\n",
      "\n",
      "Sample recommendations for user: MHxPC130442623\n",
      "Course: HarvardX/CS50x/2012, Score: 0.9654\n",
      "Course: MITx/6.00x/2012_Fall, Score: 0.9020\n",
      "Course: MITx/6.00x/2013_Spring, Score: 0.8761\n",
      "Course: HarvardX/ER22x/2013_Spring, Score: 0.8714\n",
      "Course: HarvardX/PH207x/2012_Fall, Score: 0.8248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18876\\4019370041.py:223: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  (self.item_encoder.inverse_transform([idx])[0], float(predictions[idx]))\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18876\\4019370041.py:223: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  (self.item_encoder.inverse_transform([idx])[0], float(predictions[idx]))\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18876\\4019370041.py:223: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  (self.item_encoder.inverse_transform([idx])[0], float(predictions[idx]))\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18876\\4019370041.py:223: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  (self.item_encoder.inverse_transform([idx])[0], float(predictions[idx]))\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18876\\4019370041.py:223: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  (self.item_encoder.inverse_transform([idx])[0], float(predictions[idx]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ImprovedNCF:\n",
    "    def __init__(self):\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.item_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        \n",
    "    def preprocess_data(self, df, user_col='userid_DI', item_col='course_id'):\n",
    "        \"\"\"\n",
    "        Preprocess the data efficiently\n",
    "        \"\"\"\n",
    "        print(\"Starting preprocessing...\")\n",
    "        \n",
    "        # Encode users and items\n",
    "        df['user_idx'] = self.user_encoder.fit_transform(df[user_col])\n",
    "        df['item_idx'] = self.item_encoder.fit_transform(df[item_col])\n",
    "        \n",
    "        # Get dimensions\n",
    "        self.n_users = len(self.user_encoder.classes_)\n",
    "        self.n_items = len(self.item_encoder.classes_)\n",
    "        \n",
    "        print(f\"Number of users: {self.n_users}, Number of items: {self.n_items}\")\n",
    "        return df\n",
    "\n",
    "    def generate_negative_samples(self, df, neg_ratio=0.5):  # Reduced negative ratio\n",
    "        \"\"\"\n",
    "        Generate negative samples more efficiently\n",
    "        \"\"\"\n",
    "        print(\"Generating negative samples...\")\n",
    "        \n",
    "        # Create positive samples dataframe\n",
    "        pos_samples = df[['user_idx', 'item_idx']].copy()\n",
    "        pos_samples['label'] = 1\n",
    "        \n",
    "        # Create set of positive interactions for faster lookup\n",
    "        pos_interactions = set(zip(df['user_idx'], df['item_idx']))\n",
    "        \n",
    "        # Generate negative samples\n",
    "        neg_samples = []\n",
    "        for user in df['user_idx'].unique():\n",
    "            n_neg = min(int(df[df['user_idx'] == user].shape[0] * neg_ratio), 5)  # Reduced max negatives\n",
    "            neg_items = np.random.randint(0, self.n_items, size=n_neg * 2)\n",
    "            valid_negs = [(user, item) for item in neg_items \n",
    "                         if (user, item) not in pos_interactions][:n_neg]\n",
    "            neg_samples.extend(valid_negs)\n",
    "        \n",
    "        # Convert negative samples to dataframe\n",
    "        neg_df = pd.DataFrame(neg_samples, columns=['user_idx', 'item_idx'])\n",
    "        neg_df['label'] = 0\n",
    "        \n",
    "        # Combine positive and negative samples\n",
    "        final_df = pd.concat([pos_samples, neg_df], ignore_index=True)\n",
    "        print(f\"Final dataset shape: {final_df.shape}\")\n",
    "        \n",
    "        return final_df\n",
    "\n",
    "    def build_model(self, embedding_dim=32):\n",
    "        \"\"\"\n",
    "        Build improved NCF model with regularization\n",
    "        \"\"\"\n",
    "        # Input layers\n",
    "        user_input = Input(shape=(1,), name='user_input')\n",
    "        item_input = Input(shape=(1,), name='item_input')\n",
    "        \n",
    "        # Embedding layers with regularization\n",
    "        user_embedding = Embedding(\n",
    "            self.n_users, \n",
    "            embedding_dim,\n",
    "            embeddings_regularizer=l2(0.01),\n",
    "            name='user_embedding'\n",
    "        )(user_input)\n",
    "        \n",
    "        item_embedding = Embedding(\n",
    "            self.n_items, \n",
    "            embedding_dim,\n",
    "            embeddings_regularizer=l2(0.01),\n",
    "            name='item_embedding'\n",
    "        )(item_input)\n",
    "        \n",
    "        # Flatten embeddings\n",
    "        user_flat = Flatten()(user_embedding)\n",
    "        item_flat = Flatten()(item_embedding)\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        concat = Concatenate()([user_flat, item_flat])\n",
    "        \n",
    "        # Dense layers with dropout and regularization\n",
    "        dense1 = Dense(64, \n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=l2(0.01))(concat)\n",
    "        dropout1 = Dropout(0.2)(dense1)\n",
    "        \n",
    "        dense2 = Dense(32, \n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=l2(0.01))(dropout1)\n",
    "        dropout2 = Dropout(0.2)(dense2)\n",
    "        \n",
    "        output = Dense(1, activation='sigmoid')(dropout2)\n",
    "        \n",
    "        model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "        \n",
    "        # Use a lower learning rate\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def train(self, train_data, validation_split=0.2, batch_size=2048, epochs=10):\n",
    "        \"\"\"\n",
    "        Train the model with improved callbacks\n",
    "        \"\"\"\n",
    "        # Prepare training data\n",
    "        X = train_data[['user_idx', 'item_idx']].values\n",
    "        y = train_data['label'].values\n",
    "        \n",
    "        # Split training data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=validation_split, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Modified early stopping\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=0.001\n",
    "        )\n",
    "        \n",
    "        # Add learning rate reduction\n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=0.0001\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            [X_train[:, 0], X_train[:, 1]], \n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=([X_val[:, 0], X_val[:, 1]], y_val),\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "\n",
    "    def plot_roc_curve(self, test_data):\n",
    "        \"\"\"\n",
    "        Calculate and plot ROC curve for the model\n",
    "        \"\"\"\n",
    "        print(\"\\nCalculating ROC curve...\")\n",
    "        \n",
    "        # Get predictions for test data\n",
    "        X_test = test_data[['user_idx', 'item_idx']].values\n",
    "        y_true = test_data['label'].values\n",
    "        \n",
    "        # Get prediction scores\n",
    "        y_scores = self.model.predict(\n",
    "            [X_test[:, 0], X_test[:, 1]], \n",
    "            batch_size=2048,\n",
    "            verbose=0\n",
    "        ).flatten()\n",
    "        \n",
    "        # Calculate ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                 label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        return roc_auc\n",
    "\n",
    "    def get_recommendations(self, user_id, top_n=5):\n",
    "        \"\"\"\n",
    "        Get recommendations for a specific user\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_encoder.classes_:\n",
    "            return \"User not found in training data\"\n",
    "        \n",
    "        user_idx = self.user_encoder.transform([user_id])[0]\n",
    "        \n",
    "        # Create test data for all items\n",
    "        test_user = np.array([user_idx] * self.n_items)\n",
    "        test_items = np.array(range(self.n_items))\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = self.model.predict([test_user, test_items], verbose=0)\n",
    "        \n",
    "        # Get top N recommendations\n",
    "        top_indices = predictions.flatten().argsort()[-top_n:][::-1]\n",
    "        \n",
    "        # Convert back to original item IDs and include scores\n",
    "        recommendations = [\n",
    "            (self.item_encoder.inverse_transform([idx])[0], float(predictions[idx]))\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "def test_improved_model(df):\n",
    "    # Initialize the improved model\n",
    "    improved_recommender = ImprovedNCF()\n",
    "    \n",
    "    # Preprocess data\n",
    "    processed_df = improved_recommender.preprocess_data(df)\n",
    "    \n",
    "    # Generate training data with reduced negative samples\n",
    "    full_data = improved_recommender.generate_negative_samples(processed_df)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    train_data, test_data = train_test_split(full_data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Build and train the model\n",
    "    improved_recommender.build_model()\n",
    "    history = improved_recommender.train(train_data)\n",
    "    \n",
    "    # Calculate and plot ROC curve\n",
    "    print(\"\\nGenerating ROC curve...\")\n",
    "    auc_score = improved_recommender.plot_roc_curve(test_data)\n",
    "    print(f\"AUC Score: {auc_score:.4f}\")\n",
    "    \n",
    "    # Get recommendations for a sample user\n",
    "    sample_user = df['userid_DI'].iloc[0]\n",
    "    recommendations = improved_recommender.get_recommendations(sample_user)\n",
    "    \n",
    "    print(\"\\nSample recommendations for user:\", sample_user)\n",
    "    for course, score in recommendations:\n",
    "        print(f\"Course: {course}, Score: {score:.4f}\")\n",
    "    \n",
    "    return improved_recommender, history, auc_score\n",
    "\n",
    "# Run the test\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    df = pd.read_csv('cleaned_dataset.csv')  # Replace with your data path\n",
    "    \n",
    "    # Test the improved model\n",
    "    improved_model, history, auc_score = test_improved_model(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47cfff1-361e-4e23-9d29-798d4b080111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting encoders...\n",
      "Number of users: 375220, Number of items: 16\n",
      "Generating negative samples...\n",
      "Final dataset shape: (550578, 3)\n",
      "Epoch 1/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 155ms/step - accuracy: 0.8218 - loss: 36.4841 - val_accuracy: 0.8612 - val_loss: 0.6465 - learning_rate: 5.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 168ms/step - accuracy: 0.8613 - loss: 0.5742 - val_accuracy: 0.8612 - val_loss: 0.4374 - learning_rate: 5.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 158ms/step - accuracy: 0.8617 - loss: 0.4248 - val_accuracy: 0.8612 - val_loss: 0.3979 - learning_rate: 5.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 149ms/step - accuracy: 0.8611 - loss: 0.3980 - val_accuracy: 0.8612 - val_loss: 0.3903 - learning_rate: 5.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 142ms/step - accuracy: 0.8607 - loss: 0.3920 - val_accuracy: 0.8612 - val_loss: 0.3876 - learning_rate: 5.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 143ms/step - accuracy: 0.8608 - loss: 0.3896 - val_accuracy: 0.8612 - val_loss: 0.3858 - learning_rate: 5.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 142ms/step - accuracy: 0.8618 - loss: 0.3860 - val_accuracy: 0.8612 - val_loss: 0.3842 - learning_rate: 5.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 148ms/step - accuracy: 0.8615 - loss: 0.3855 - val_accuracy: 0.8612 - val_loss: 0.3828 - learning_rate: 5.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 142ms/step - accuracy: 0.8616 - loss: 0.3836 - val_accuracy: 0.8612 - val_loss: 0.3815 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 143ms/step - accuracy: 0.8621 - loss: 0.3816 - val_accuracy: 0.8612 - val_loss: 0.3804 - learning_rate: 5.0000e-04\n",
      "Evaluating model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Computing NDCG is only meaningful when there is more than 1 document. Got 1 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 308\u001b[0m\n\u001b[0;32m    305\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Replace with your data path\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Test the improved model\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m improved_model, history \u001b[38;5;241m=\u001b[39m test_improved_model(df)\n",
      "Cell \u001b[1;32mIn[2], line 290\u001b[0m, in \u001b[0;36mtest_improved_model\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    287\u001b[0m history \u001b[38;5;241m=\u001b[39m improved_recommender\u001b[38;5;241m.\u001b[39mtrain(training_data)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m precision, recall, ndcg \u001b[38;5;241m=\u001b[39m improved_recommender\u001b[38;5;241m.\u001b[39mevaluate_model(test_df, train_df, K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m    292\u001b[0m \u001b[38;5;66;03m# Get recommendations for a sample user\u001b[39;00m\n\u001b[0;32m    293\u001b[0m sample_user \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muserid_DI\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[2], line 220\u001b[0m, in \u001b[0;36mImprovedNCF.evaluate_model\u001b[1;34m(self, test_df, train_df, K)\u001b[0m\n\u001b[0;32m    218\u001b[0m relevance \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m true_items \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m top_K_items]\n\u001b[0;32m    219\u001b[0m ideal_relevance \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(true_items), K)\n\u001b[1;32m--> 220\u001b[0m ndcg_at_K \u001b[38;5;241m=\u001b[39m ndcg_score([ideal_relevance], [relevance], k\u001b[38;5;241m=\u001b[39mK)\n\u001b[0;32m    221\u001b[0m ndcgs\u001b[38;5;241m.\u001b[39mappend(ndcg_at_K)\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1834\u001b[0m, in \u001b[0;36mndcg_score\u001b[1;34m(y_true, y_score, k, sample_weight, ignore_ties)\u001b[0m\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndcg_score should not be used on negative y_true values.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1834\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1835\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing NDCG is only meaningful when there is more than 1 document. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1836\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1837\u001b[0m     )\n\u001b[0;32m   1838\u001b[0m _check_dcg_target_type(y_true)\n\u001b[0;32m   1839\u001b[0m gain \u001b[38;5;241m=\u001b[39m _ndcg_sample_scores(y_true, y_score, k\u001b[38;5;241m=\u001b[39mk, ignore_ties\u001b[38;5;241m=\u001b[39mignore_ties)\n",
      "\u001b[1;31mValueError\u001b[0m: Computing NDCG is only meaningful when there is more than 1 document. Got 1 instead."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ImprovedNCF:\n",
    "    def __init__(self):\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.item_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        self.n_users = None\n",
    "        self.n_items = None\n",
    "        \n",
    "    def fit_encoders(self, df, user_col='userid_DI', item_col='course_id'):\n",
    "        \"\"\"\n",
    "        Fit the label encoders on training data\n",
    "        \"\"\"\n",
    "        print(\"Fitting encoders...\")\n",
    "        self.user_encoder.fit(df[user_col])\n",
    "        self.item_encoder.fit(df[item_col])\n",
    "        self.n_users = len(self.user_encoder.classes_)\n",
    "        self.n_items = len(self.item_encoder.classes_)\n",
    "        print(f\"Number of users: {self.n_users}, Number of items: {self.n_items}\")\n",
    "\n",
    "    def preprocess_data(self, df, user_col='userid_DI', item_col='course_id'):\n",
    "        \"\"\"\n",
    "        Transform the data using fitted encoders\n",
    "        \"\"\"\n",
    "        df['user_idx'] = self.user_encoder.transform(df[user_col])\n",
    "        df['item_idx'] = self.item_encoder.transform(df[item_col])\n",
    "        return df\n",
    "\n",
    "    def generate_negative_samples(self, df, neg_ratio=0.5):  # Reduced negative ratio\n",
    "        \"\"\"\n",
    "        Generate negative samples more efficiently\n",
    "        \"\"\"\n",
    "        print(\"Generating negative samples...\")\n",
    "        \n",
    "        # Create positive samples dataframe\n",
    "        pos_samples = df[['user_idx', 'item_idx']].copy()\n",
    "        pos_samples['label'] = 1\n",
    "        \n",
    "        # Create set of positive interactions for faster lookup\n",
    "        pos_interactions = set(zip(df['user_idx'], df['item_idx']))\n",
    "        \n",
    "        # Generate negative samples\n",
    "        neg_samples = []\n",
    "        for user in df['user_idx'].unique():\n",
    "            n_neg = min(int(df[df['user_idx'] == user].shape[0] * neg_ratio), 5)  # Reduced max negatives\n",
    "            neg_items = np.random.randint(0, self.n_items, size=n_neg * 2)\n",
    "            valid_negs = [(user, item) for item in neg_items \n",
    "                         if (user, item) not in pos_interactions][:n_neg]\n",
    "            neg_samples.extend(valid_negs)\n",
    "        \n",
    "        # Convert negative samples to dataframe\n",
    "        neg_df = pd.DataFrame(neg_samples, columns=['user_idx', 'item_idx'])\n",
    "        neg_df['label'] = 0\n",
    "        \n",
    "        # Combine positive and negative samples\n",
    "        final_df = pd.concat([pos_samples, neg_df], ignore_index=True)\n",
    "        print(f\"Final dataset shape: {final_df.shape}\")\n",
    "        \n",
    "        return final_df\n",
    "\n",
    "    def build_model(self, embedding_dim=32):\n",
    "        \"\"\"\n",
    "        Build improved NCF model with regularization\n",
    "        \"\"\"\n",
    "        # Input layers\n",
    "        user_input = Input(shape=(1,), name='user_input')\n",
    "        item_input = Input(shape=(1,), name='item_input')\n",
    "        \n",
    "        # Embedding layers with regularization\n",
    "        user_embedding = Embedding(\n",
    "            self.n_users, \n",
    "            embedding_dim,\n",
    "            embeddings_regularizer=l2(0.01),\n",
    "            name='user_embedding'\n",
    "        )(user_input)\n",
    "        \n",
    "        item_embedding = Embedding(\n",
    "            self.n_items, \n",
    "            embedding_dim,\n",
    "            embeddings_regularizer=l2(0.01),\n",
    "            name='item_embedding'\n",
    "        )(item_input)\n",
    "        \n",
    "        # Flatten embeddings\n",
    "        user_flat = Flatten()(user_embedding)\n",
    "        item_flat = Flatten()(item_embedding)\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        concat = Concatenate()([user_flat, item_flat])\n",
    "        \n",
    "        # Dense layers with dropout and regularization\n",
    "        dense1 = Dense(64, \n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=l2(0.01))(concat)\n",
    "        dropout1 = Dropout(0.2)(dense1)\n",
    "        \n",
    "        dense2 = Dense(32, \n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=l2(0.01))(dropout1)\n",
    "        dropout2 = Dropout(0.2)(dense2)\n",
    "        \n",
    "        output = Dense(1, activation='sigmoid')(dropout2)\n",
    "        \n",
    "        model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "        \n",
    "        # Use a lower learning rate\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def train(self, train_data, validation_split=0.2, batch_size=2048, epochs=10):\n",
    "        \"\"\"\n",
    "        Train the model with improved callbacks\n",
    "        \"\"\"\n",
    "        # Prepare training data\n",
    "        X = train_data[['user_idx', 'item_idx']].values\n",
    "        y = train_data['label'].values\n",
    "        \n",
    "        # Split training data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=validation_split, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Modified early stopping\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=0.001\n",
    "        )\n",
    "        \n",
    "        # Add learning rate reduction\n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=0.0001\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            [X_train[:, 0], X_train[:, 1]], \n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=([X_val[:, 0], X_val[:, 1]], y_val),\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self, test_df, train_df, K=10):\n",
    "        \"\"\"\n",
    "        Evaluate the model using Precision@K, Recall@K, and NDCG@K\n",
    "        \"\"\"\n",
    "        print(\"Evaluating model...\")\n",
    "        from sklearn.metrics import ndcg_score\n",
    "\n",
    "        # Create user-item interactions dictionaries\n",
    "        test_user_items = test_df.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
    "        train_user_items = train_df.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
    "        \n",
    "        # All users in test set\n",
    "        all_users = test_df['user_idx'].unique()\n",
    "        \n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        ndcgs = []\n",
    "        \n",
    "        num_users = len(all_users)\n",
    "        for i, user in enumerate(all_users):\n",
    "            # Items the user interacted with in the test set\n",
    "            true_items = test_user_items.get(user, set())\n",
    "\n",
    "            # Skip if no true items\n",
    "            if len(true_items) == 0:\n",
    "                continue\n",
    "\n",
    "            # Items the user interacted with in the training set\n",
    "            train_items = train_user_items.get(user, set())\n",
    "\n",
    "            # Candidate items are all items not seen in training\n",
    "            candidate_items = list(set(range(self.n_items)) - train_items)\n",
    "\n",
    "            # Predict scores for candidate items\n",
    "            user_array = np.full(len(candidate_items), user)\n",
    "            predictions = self.model.predict([user_array, np.array(candidate_items)], verbose=0)\n",
    "            \n",
    "            # Get top K items\n",
    "            top_K_indices = np.argsort(predictions.flatten())[-K:][::-1]\n",
    "            top_K_items = [candidate_items[idx] for idx in top_K_indices]\n",
    "            \n",
    "            # Calculate Precision@K\n",
    "            num_relevant_items = len(set(top_K_items) & true_items)\n",
    "            precision_at_K = num_relevant_items / K\n",
    "            precisions.append(precision_at_K)\n",
    "            \n",
    "            # Calculate Recall@K\n",
    "            recall_at_K = num_relevant_items / len(true_items)\n",
    "            recalls.append(recall_at_K)\n",
    "            \n",
    "            # Calculate NDCG@K\n",
    "            relevance = [1 if item in true_items else 0 for item in top_K_items]\n",
    "            ideal_relevance = [1] * min(len(true_items), K)\n",
    "            ndcg_at_K = ndcg_score([ideal_relevance], [relevance], k=K)\n",
    "            ndcgs.append(ndcg_at_K)\n",
    "\n",
    "            if i % 100 == 0 and i > 0:\n",
    "                print(f\"Evaluated {i}/{num_users} users\")\n",
    "\n",
    "        avg_precision = np.mean(precisions)\n",
    "        avg_recall = np.mean(recalls)\n",
    "        avg_ndcg = np.mean(ndcgs)\n",
    "\n",
    "        print(f\"\\nPrecision@{K}: {avg_precision:.4f}\")\n",
    "        print(f\"Recall@{K}: {avg_recall:.4f}\")\n",
    "        print(f\"NDCG@{K}: {avg_ndcg:.4f}\")\n",
    "        \n",
    "        return avg_precision, avg_recall, avg_ndcg\n",
    "\n",
    "    def get_recommendations(self, user_id, top_n=5):\n",
    "        \"\"\"\n",
    "        Get recommendations for a specific user\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_encoder.classes_:\n",
    "            return \"User not found in training data\"\n",
    "        \n",
    "        user_idx = self.user_encoder.transform([user_id])[0]\n",
    "        \n",
    "        # Create test data for all items\n",
    "        test_user = np.array([user_idx] * self.n_items)\n",
    "        test_items = np.array(range(self.n_items))\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = self.model.predict([test_user, test_items], verbose=0)\n",
    "        \n",
    "        # Get top N recommendations\n",
    "        top_indices = predictions.flatten().argsort()[-top_n:][::-1]\n",
    "        \n",
    "        # Convert back to original item IDs and include scores\n",
    "        recommendations = [\n",
    "            (self.item_encoder.inverse_transform([idx])[0], float(predictions[idx]))\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Test the improved version\n",
    "def test_improved_model(df):\n",
    "    # Initialize the improved model\n",
    "    improved_recommender = ImprovedNCF()\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Filter test set to only include users and items present in training data\n",
    "    test_df = test_df[test_df['userid_DI'].isin(train_df['userid_DI'].unique())]\n",
    "    test_df = test_df[test_df['course_id'].isin(train_df['course_id'].unique())]\n",
    "    \n",
    "    # Fit encoders on training data\n",
    "    improved_recommender.fit_encoders(train_df)\n",
    "    \n",
    "    # Preprocess data\n",
    "    train_df = improved_recommender.preprocess_data(train_df)\n",
    "    test_df = improved_recommender.preprocess_data(test_df)\n",
    "    \n",
    "    # Generate training data with reduced negative samples\n",
    "    training_data = improved_recommender.generate_negative_samples(train_df)\n",
    "    \n",
    "    # Build and train the model\n",
    "    improved_recommender.build_model()\n",
    "    history = improved_recommender.train(training_data)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    precision, recall, ndcg = improved_recommender.evaluate_model(test_df, train_df, K=10)\n",
    "    \n",
    "    # Get recommendations for a sample user\n",
    "    sample_user = df['userid_DI'].iloc[0]\n",
    "    recommendations = improved_recommender.get_recommendations(sample_user)\n",
    "    \n",
    "    print(\"\\nSample recommendations for user:\", sample_user)\n",
    "    for course, score in recommendations:\n",
    "        print(f\"Course: {course}, Score: {score:.4f}\")\n",
    "    \n",
    "    return improved_recommender, history\n",
    "\n",
    "# Run the test\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    df = pd.read_csv('cleaned_dataset.csv')  # Replace with your data path\n",
    "    \n",
    "    # Test the improved model\n",
    "    improved_model, history = test_improved_model(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef8c829c-1c9d-47d3-b84c-880644c556a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting encoders...\n",
      "Number of users: 375220, Number of items: 16\n",
      "Generating negative samples...\n",
      "Final dataset shape: (550510, 3)\n",
      "Epoch 1/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 145ms/step - accuracy: 0.8533 - loss: 36.4986 - val_accuracy: 0.8610 - val_loss: 0.6452 - learning_rate: 5.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 144ms/step - accuracy: 0.8613 - loss: 0.5729 - val_accuracy: 0.8610 - val_loss: 0.4355 - learning_rate: 5.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 143ms/step - accuracy: 0.8618 - loss: 0.4236 - val_accuracy: 0.8610 - val_loss: 0.3959 - learning_rate: 5.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 147ms/step - accuracy: 0.8609 - loss: 0.3963 - val_accuracy: 0.8610 - val_loss: 0.3882 - learning_rate: 5.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 150ms/step - accuracy: 0.8611 - loss: 0.3913 - val_accuracy: 0.8610 - val_loss: 0.3854 - learning_rate: 5.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 153ms/step - accuracy: 0.8621 - loss: 0.3867 - val_accuracy: 0.8610 - val_loss: 0.3836 - learning_rate: 5.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 154ms/step - accuracy: 0.8617 - loss: 0.3863 - val_accuracy: 0.8610 - val_loss: 0.3820 - learning_rate: 5.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 161ms/step - accuracy: 0.8614 - loss: 0.3857 - val_accuracy: 0.8610 - val_loss: 0.3806 - learning_rate: 5.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 182ms/step - accuracy: 0.8623 - loss: 0.3823 - val_accuracy: 0.8610 - val_loss: 0.3796 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 153ms/step - accuracy: 0.8615 - loss: 0.3819 - val_accuracy: 0.8610 - val_loss: 0.3783 - learning_rate: 5.0000e-04\n",
      "Evaluating model...\n",
      "Evaluated 100/38072 users\n",
      "Evaluated 200/38072 users\n",
      "Evaluated 300/38072 users\n",
      "Evaluated 400/38072 users\n",
      "Evaluated 500/38072 users\n",
      "Evaluated 600/38072 users\n",
      "Evaluated 700/38072 users\n",
      "Evaluated 800/38072 users\n",
      "Evaluated 900/38072 users\n",
      "Evaluated 1000/38072 users\n",
      "Evaluated 1100/38072 users\n",
      "Evaluated 1200/38072 users\n",
      "Evaluated 1300/38072 users\n",
      "Evaluated 1400/38072 users\n",
      "Evaluated 1500/38072 users\n",
      "Evaluated 1600/38072 users\n",
      "Evaluated 1700/38072 users\n",
      "Evaluated 1800/38072 users\n",
      "Evaluated 1900/38072 users\n",
      "Evaluated 2000/38072 users\n",
      "Evaluated 2100/38072 users\n",
      "Evaluated 2200/38072 users\n",
      "Evaluated 2300/38072 users\n",
      "Evaluated 2400/38072 users\n",
      "Evaluated 2500/38072 users\n",
      "Evaluated 2600/38072 users\n",
      "Evaluated 2700/38072 users\n",
      "Evaluated 2800/38072 users\n",
      "Evaluated 2900/38072 users\n",
      "Evaluated 3000/38072 users\n",
      "Evaluated 3100/38072 users\n",
      "Evaluated 3200/38072 users\n",
      "Evaluated 3300/38072 users\n",
      "Evaluated 3400/38072 users\n",
      "Evaluated 3500/38072 users\n",
      "Evaluated 3600/38072 users\n",
      "Evaluated 3700/38072 users\n",
      "Evaluated 3800/38072 users\n",
      "Evaluated 3900/38072 users\n",
      "Evaluated 4000/38072 users\n",
      "Evaluated 4100/38072 users\n",
      "Evaluated 4200/38072 users\n",
      "Evaluated 4300/38072 users\n",
      "Evaluated 4400/38072 users\n",
      "Evaluated 4500/38072 users\n",
      "Evaluated 4600/38072 users\n",
      "Evaluated 4700/38072 users\n",
      "Evaluated 4800/38072 users\n",
      "Evaluated 4900/38072 users\n",
      "Evaluated 5000/38072 users\n",
      "Evaluated 5100/38072 users\n",
      "Evaluated 5200/38072 users\n",
      "Evaluated 5300/38072 users\n",
      "Evaluated 5400/38072 users\n",
      "Evaluated 5500/38072 users\n",
      "Evaluated 5600/38072 users\n",
      "Evaluated 5700/38072 users\n",
      "Evaluated 5800/38072 users\n",
      "Evaluated 5900/38072 users\n",
      "Evaluated 6000/38072 users\n",
      "Evaluated 6100/38072 users\n",
      "Evaluated 6200/38072 users\n",
      "Evaluated 6300/38072 users\n",
      "Evaluated 6400/38072 users\n",
      "Evaluated 6500/38072 users\n",
      "Evaluated 6600/38072 users\n",
      "Evaluated 6700/38072 users\n",
      "Evaluated 6800/38072 users\n",
      "Evaluated 6900/38072 users\n",
      "Evaluated 7000/38072 users\n",
      "Evaluated 7100/38072 users\n",
      "Evaluated 7200/38072 users\n",
      "Evaluated 7300/38072 users\n",
      "Evaluated 7400/38072 users\n",
      "Evaluated 7500/38072 users\n",
      "Evaluated 7600/38072 users\n",
      "Evaluated 7700/38072 users\n",
      "Evaluated 7800/38072 users\n",
      "Evaluated 7900/38072 users\n",
      "Evaluated 8000/38072 users\n",
      "Evaluated 8100/38072 users\n",
      "Evaluated 8200/38072 users\n",
      "Evaluated 8300/38072 users\n",
      "Evaluated 8400/38072 users\n",
      "Evaluated 8500/38072 users\n",
      "Evaluated 8600/38072 users\n",
      "Evaluated 8700/38072 users\n",
      "Evaluated 8800/38072 users\n",
      "Evaluated 8900/38072 users\n",
      "Evaluated 9000/38072 users\n",
      "Evaluated 9100/38072 users\n",
      "Evaluated 9200/38072 users\n",
      "Evaluated 9300/38072 users\n",
      "Evaluated 9400/38072 users\n",
      "Evaluated 9500/38072 users\n",
      "Evaluated 9600/38072 users\n",
      "Evaluated 9700/38072 users\n",
      "Evaluated 9800/38072 users\n",
      "Evaluated 9900/38072 users\n",
      "Evaluated 10000/38072 users\n",
      "Evaluated 10100/38072 users\n",
      "Evaluated 10200/38072 users\n",
      "Evaluated 10300/38072 users\n",
      "Evaluated 10400/38072 users\n",
      "Evaluated 10500/38072 users\n",
      "Evaluated 10600/38072 users\n",
      "Evaluated 10700/38072 users\n",
      "Evaluated 10800/38072 users\n",
      "Evaluated 10900/38072 users\n",
      "Evaluated 11000/38072 users\n",
      "Evaluated 11100/38072 users\n",
      "Evaluated 11200/38072 users\n",
      "Evaluated 11300/38072 users\n",
      "Evaluated 11400/38072 users\n",
      "Evaluated 11500/38072 users\n",
      "Evaluated 11600/38072 users\n",
      "Evaluated 11700/38072 users\n",
      "Evaluated 11800/38072 users\n",
      "Evaluated 11900/38072 users\n",
      "Evaluated 12000/38072 users\n",
      "Evaluated 12100/38072 users\n",
      "Evaluated 12200/38072 users\n",
      "Evaluated 12300/38072 users\n",
      "Evaluated 12400/38072 users\n",
      "Evaluated 12500/38072 users\n",
      "Evaluated 12600/38072 users\n",
      "Evaluated 12700/38072 users\n",
      "Evaluated 12800/38072 users\n",
      "Evaluated 12900/38072 users\n",
      "Evaluated 13000/38072 users\n",
      "Evaluated 13100/38072 users\n",
      "Evaluated 13200/38072 users\n",
      "Evaluated 13300/38072 users\n",
      "Evaluated 13400/38072 users\n",
      "Evaluated 13500/38072 users\n",
      "Evaluated 13600/38072 users\n",
      "Evaluated 13700/38072 users\n",
      "Evaluated 13800/38072 users\n",
      "Evaluated 13900/38072 users\n",
      "Evaluated 14000/38072 users\n",
      "Evaluated 14100/38072 users\n",
      "Evaluated 14200/38072 users\n",
      "Evaluated 14300/38072 users\n",
      "Evaluated 14400/38072 users\n",
      "Evaluated 14500/38072 users\n",
      "Evaluated 14600/38072 users\n",
      "Evaluated 14700/38072 users\n",
      "Evaluated 14800/38072 users\n",
      "Evaluated 14900/38072 users\n",
      "Evaluated 15000/38072 users\n",
      "Evaluated 15100/38072 users\n",
      "Evaluated 15200/38072 users\n",
      "Evaluated 15300/38072 users\n",
      "Evaluated 15400/38072 users\n",
      "Evaluated 15500/38072 users\n",
      "Evaluated 15600/38072 users\n",
      "Evaluated 15700/38072 users\n",
      "Evaluated 15800/38072 users\n",
      "Evaluated 15900/38072 users\n",
      "Evaluated 16000/38072 users\n",
      "Evaluated 16100/38072 users\n",
      "Evaluated 16200/38072 users\n",
      "Evaluated 16300/38072 users\n",
      "Evaluated 16400/38072 users\n",
      "Evaluated 16500/38072 users\n",
      "Evaluated 16600/38072 users\n",
      "Evaluated 16700/38072 users\n",
      "Evaluated 16800/38072 users\n",
      "Evaluated 16900/38072 users\n",
      "Evaluated 17000/38072 users\n",
      "Evaluated 17100/38072 users\n",
      "Evaluated 17200/38072 users\n",
      "Evaluated 17300/38072 users\n",
      "Evaluated 17400/38072 users\n",
      "Evaluated 17500/38072 users\n",
      "Evaluated 17600/38072 users\n",
      "Evaluated 17700/38072 users\n",
      "Evaluated 17800/38072 users\n",
      "Evaluated 17900/38072 users\n",
      "Evaluated 18000/38072 users\n",
      "Evaluated 18100/38072 users\n",
      "Evaluated 18200/38072 users\n",
      "Evaluated 18300/38072 users\n",
      "Evaluated 18400/38072 users\n",
      "Evaluated 18500/38072 users\n",
      "Evaluated 18600/38072 users\n",
      "Evaluated 18700/38072 users\n",
      "Evaluated 18800/38072 users\n",
      "Evaluated 18900/38072 users\n",
      "Evaluated 19000/38072 users\n",
      "Evaluated 19100/38072 users\n",
      "Evaluated 19200/38072 users\n",
      "Evaluated 19300/38072 users\n",
      "Evaluated 19400/38072 users\n",
      "Evaluated 19500/38072 users\n",
      "Evaluated 19600/38072 users\n",
      "Evaluated 19700/38072 users\n",
      "Evaluated 19800/38072 users\n",
      "Evaluated 19900/38072 users\n",
      "Evaluated 20000/38072 users\n",
      "Evaluated 20100/38072 users\n",
      "Evaluated 20200/38072 users\n",
      "Evaluated 20300/38072 users\n",
      "Evaluated 20400/38072 users\n",
      "Evaluated 20500/38072 users\n",
      "Evaluated 20600/38072 users\n",
      "Evaluated 20700/38072 users\n",
      "Evaluated 20800/38072 users\n",
      "Evaluated 20900/38072 users\n",
      "Evaluated 21000/38072 users\n",
      "Evaluated 21100/38072 users\n",
      "Evaluated 21200/38072 users\n",
      "Evaluated 21300/38072 users\n",
      "Evaluated 21400/38072 users\n",
      "Evaluated 21500/38072 users\n",
      "Evaluated 21600/38072 users\n",
      "Evaluated 21700/38072 users\n",
      "Evaluated 21800/38072 users\n",
      "Evaluated 21900/38072 users\n",
      "Evaluated 22000/38072 users\n",
      "Evaluated 22100/38072 users\n",
      "Evaluated 22200/38072 users\n",
      "Evaluated 22300/38072 users\n",
      "Evaluated 22400/38072 users\n",
      "Evaluated 22500/38072 users\n",
      "Evaluated 22600/38072 users\n",
      "Evaluated 22700/38072 users\n",
      "Evaluated 22800/38072 users\n",
      "Evaluated 22900/38072 users\n",
      "Evaluated 23000/38072 users\n",
      "Evaluated 23100/38072 users\n",
      "Evaluated 23200/38072 users\n",
      "Evaluated 23300/38072 users\n",
      "Evaluated 23400/38072 users\n",
      "Evaluated 23500/38072 users\n",
      "Evaluated 23600/38072 users\n",
      "Evaluated 23700/38072 users\n",
      "Evaluated 23800/38072 users\n",
      "Evaluated 23900/38072 users\n",
      "Evaluated 24000/38072 users\n",
      "Evaluated 24100/38072 users\n",
      "Evaluated 24200/38072 users\n",
      "Evaluated 24300/38072 users\n",
      "Evaluated 24400/38072 users\n",
      "Evaluated 24500/38072 users\n",
      "Evaluated 24600/38072 users\n",
      "Evaluated 24700/38072 users\n",
      "Evaluated 24800/38072 users\n",
      "Evaluated 24900/38072 users\n",
      "Evaluated 25000/38072 users\n",
      "Evaluated 25100/38072 users\n",
      "Evaluated 25200/38072 users\n",
      "Evaluated 25300/38072 users\n",
      "Evaluated 25400/38072 users\n",
      "Evaluated 25500/38072 users\n",
      "Evaluated 25600/38072 users\n",
      "Evaluated 25700/38072 users\n",
      "Evaluated 25800/38072 users\n",
      "Evaluated 25900/38072 users\n",
      "Evaluated 26000/38072 users\n",
      "Evaluated 26100/38072 users\n",
      "Evaluated 26200/38072 users\n",
      "Evaluated 26300/38072 users\n",
      "Evaluated 26400/38072 users\n",
      "Evaluated 26500/38072 users\n",
      "Evaluated 26600/38072 users\n",
      "Evaluated 26700/38072 users\n",
      "Evaluated 26800/38072 users\n",
      "Evaluated 26900/38072 users\n",
      "Evaluated 27000/38072 users\n",
      "Evaluated 27100/38072 users\n",
      "Evaluated 27200/38072 users\n",
      "Evaluated 27300/38072 users\n",
      "Evaluated 27400/38072 users\n",
      "Evaluated 27500/38072 users\n",
      "Evaluated 27600/38072 users\n",
      "Evaluated 27700/38072 users\n",
      "Evaluated 27800/38072 users\n",
      "Evaluated 27900/38072 users\n",
      "Evaluated 28000/38072 users\n",
      "Evaluated 28100/38072 users\n",
      "Evaluated 28200/38072 users\n",
      "Evaluated 28300/38072 users\n",
      "Evaluated 28400/38072 users\n",
      "Evaluated 28500/38072 users\n",
      "Evaluated 28600/38072 users\n",
      "Evaluated 28700/38072 users\n",
      "Evaluated 28800/38072 users\n",
      "Evaluated 28900/38072 users\n",
      "Evaluated 29000/38072 users\n",
      "Evaluated 29100/38072 users\n",
      "Evaluated 29200/38072 users\n",
      "Evaluated 29300/38072 users\n",
      "Evaluated 29400/38072 users\n",
      "Evaluated 29500/38072 users\n",
      "Evaluated 29600/38072 users\n",
      "Evaluated 29700/38072 users\n",
      "Evaluated 29800/38072 users\n",
      "Evaluated 29900/38072 users\n",
      "Evaluated 30000/38072 users\n",
      "Evaluated 30100/38072 users\n",
      "Evaluated 30200/38072 users\n",
      "Evaluated 30300/38072 users\n",
      "Evaluated 30400/38072 users\n",
      "Evaluated 30500/38072 users\n",
      "Evaluated 30600/38072 users\n",
      "Evaluated 30700/38072 users\n",
      "Evaluated 30800/38072 users\n",
      "Evaluated 30900/38072 users\n",
      "Evaluated 31000/38072 users\n",
      "Evaluated 31100/38072 users\n",
      "Evaluated 31200/38072 users\n",
      "Evaluated 31300/38072 users\n",
      "Evaluated 31400/38072 users\n",
      "Evaluated 31500/38072 users\n",
      "Evaluated 31600/38072 users\n",
      "Evaluated 31700/38072 users\n",
      "Evaluated 31800/38072 users\n",
      "Evaluated 31900/38072 users\n",
      "Evaluated 32000/38072 users\n",
      "Evaluated 32100/38072 users\n",
      "Evaluated 32200/38072 users\n",
      "Evaluated 32300/38072 users\n",
      "Evaluated 32400/38072 users\n",
      "Evaluated 32500/38072 users\n",
      "Evaluated 32600/38072 users\n",
      "Evaluated 32700/38072 users\n",
      "Evaluated 32800/38072 users\n",
      "Evaluated 32900/38072 users\n",
      "Evaluated 33000/38072 users\n",
      "Evaluated 33100/38072 users\n",
      "Evaluated 33200/38072 users\n",
      "Evaluated 33300/38072 users\n",
      "Evaluated 33400/38072 users\n",
      "Evaluated 33500/38072 users\n",
      "Evaluated 33600/38072 users\n",
      "Evaluated 33700/38072 users\n",
      "Evaluated 33800/38072 users\n",
      "Evaluated 33900/38072 users\n",
      "Evaluated 34000/38072 users\n",
      "Evaluated 34100/38072 users\n",
      "Evaluated 34200/38072 users\n",
      "Evaluated 34300/38072 users\n",
      "Evaluated 34400/38072 users\n",
      "Evaluated 34500/38072 users\n",
      "Evaluated 34600/38072 users\n",
      "Evaluated 34700/38072 users\n",
      "Evaluated 34800/38072 users\n",
      "Evaluated 34900/38072 users\n",
      "Evaluated 35000/38072 users\n",
      "Evaluated 35100/38072 users\n",
      "Evaluated 35200/38072 users\n",
      "Evaluated 35300/38072 users\n",
      "Evaluated 35400/38072 users\n",
      "Evaluated 35500/38072 users\n",
      "Evaluated 35600/38072 users\n",
      "Evaluated 35700/38072 users\n",
      "Evaluated 35800/38072 users\n",
      "Evaluated 35900/38072 users\n",
      "Evaluated 36000/38072 users\n",
      "Evaluated 36100/38072 users\n",
      "Evaluated 36200/38072 users\n",
      "Evaluated 36300/38072 users\n",
      "Evaluated 36400/38072 users\n",
      "Evaluated 36500/38072 users\n",
      "Evaluated 36600/38072 users\n",
      "Evaluated 36700/38072 users\n",
      "Evaluated 36800/38072 users\n",
      "Evaluated 36900/38072 users\n",
      "Evaluated 37000/38072 users\n",
      "Evaluated 37100/38072 users\n",
      "Evaluated 37200/38072 users\n",
      "Evaluated 37300/38072 users\n",
      "Evaluated 37400/38072 users\n",
      "Evaluated 37500/38072 users\n",
      "Evaluated 37600/38072 users\n",
      "Evaluated 37700/38072 users\n",
      "Evaluated 37800/38072 users\n",
      "Evaluated 37900/38072 users\n",
      "Evaluated 38000/38072 users\n",
      "\n",
      "Precision@10: 0.1023\n",
      "Recall@10: 0.9082\n",
      "NDCG@10: 0.5898\n",
      "\n",
      "Sample recommendations for user: MHxPC130442623\n",
      "Course: HarvardX/CS50x/2012, Score: 0.9644\n",
      "Course: MITx/6.00x/2012_Fall, Score: 0.9145\n",
      "Course: MITx/6.00x/2013_Spring, Score: 0.8855\n",
      "Course: HarvardX/ER22x/2013_Spring, Score: 0.8836\n",
      "Course: HarvardX/PH207x/2012_Fall, Score: 0.8426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1740\\3719423250.py:267: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  (self.item_encoder.inverse_transform([idx])[0], float(predictions[idx]))\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1740\\3719423250.py:267: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  (self.item_encoder.inverse_transform([idx])[0], float(predictions[idx]))\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1740\\3719423250.py:267: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  (self.item_encoder.inverse_transform([idx])[0], float(predictions[idx]))\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1740\\3719423250.py:267: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  (self.item_encoder.inverse_transform([idx])[0], float(predictions[idx]))\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_1740\\3719423250.py:267: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  (self.item_encoder.inverse_transform([idx])[0], float(predictions[idx]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ImprovedNCF:\n",
    "    def __init__(self):\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.item_encoder = LabelEncoder()\n",
    "        self.model = None\n",
    "        self.n_users = None\n",
    "        self.n_items = None\n",
    "        \n",
    "    def fit_encoders(self, df, user_col='userid_DI', item_col='course_id'):\n",
    "        \"\"\"\n",
    "        Fit the label encoders on training data\n",
    "        \"\"\"\n",
    "        print(\"Fitting encoders...\")\n",
    "        self.user_encoder.fit(df[user_col])\n",
    "        self.item_encoder.fit(df[item_col])\n",
    "        self.n_users = len(self.user_encoder.classes_)\n",
    "        self.n_items = len(self.item_encoder.classes_)\n",
    "        print(f\"Number of users: {self.n_users}, Number of items: {self.n_items}\")\n",
    "\n",
    "    def preprocess_data(self, df, user_col='userid_DI', item_col='course_id'):\n",
    "        \"\"\"\n",
    "        Transform the data using fitted encoders\n",
    "        \"\"\"\n",
    "        df['user_idx'] = self.user_encoder.transform(df[user_col])\n",
    "        df['item_idx'] = self.item_encoder.transform(df[item_col])\n",
    "        return df\n",
    "\n",
    "    def generate_negative_samples(self, df, neg_ratio=0.5):  # Reduced negative ratio\n",
    "        \"\"\"\n",
    "        Generate negative samples more efficiently\n",
    "        \"\"\"\n",
    "        print(\"Generating negative samples...\")\n",
    "        \n",
    "        # Create positive samples dataframe\n",
    "        pos_samples = df[['user_idx', 'item_idx']].copy()\n",
    "        pos_samples['label'] = 1\n",
    "        \n",
    "        # Create set of positive interactions for faster lookup\n",
    "        pos_interactions = set(zip(df['user_idx'], df['item_idx']))\n",
    "        \n",
    "        # Generate negative samples\n",
    "        neg_samples = []\n",
    "        for user in df['user_idx'].unique():\n",
    "            n_neg = min(int(df[df['user_idx'] == user].shape[0] * neg_ratio), 5)  # Reduced max negatives\n",
    "            neg_items = np.random.randint(0, self.n_items, size=n_neg * 2)\n",
    "            valid_negs = [(user, item) for item in neg_items \n",
    "                         if (user, item) not in pos_interactions][:n_neg]\n",
    "            neg_samples.extend(valid_negs)\n",
    "        \n",
    "        # Convert negative samples to dataframe\n",
    "        neg_df = pd.DataFrame(neg_samples, columns=['user_idx', 'item_idx'])\n",
    "        neg_df['label'] = 0\n",
    "        \n",
    "        # Combine positive and negative samples\n",
    "        final_df = pd.concat([pos_samples, neg_df], ignore_index=True)\n",
    "        print(f\"Final dataset shape: {final_df.shape}\")\n",
    "        \n",
    "        return final_df\n",
    "\n",
    "    def build_model(self, embedding_dim=32):\n",
    "        \"\"\"\n",
    "        Build improved NCF model with regularization\n",
    "        \"\"\"\n",
    "        # Input layers\n",
    "        user_input = Input(shape=(1,), name='user_input')\n",
    "        item_input = Input(shape=(1,), name='item_input')\n",
    "        \n",
    "        # Embedding layers with regularization\n",
    "        user_embedding = Embedding(\n",
    "            self.n_users, \n",
    "            embedding_dim,\n",
    "            embeddings_regularizer=l2(0.01),\n",
    "            name='user_embedding'\n",
    "        )(user_input)\n",
    "        \n",
    "        item_embedding = Embedding(\n",
    "            self.n_items, \n",
    "            embedding_dim,\n",
    "            embeddings_regularizer=l2(0.01),\n",
    "            name='item_embedding'\n",
    "        )(item_input)\n",
    "        \n",
    "        # Flatten embeddings\n",
    "        user_flat = Flatten()(user_embedding)\n",
    "        item_flat = Flatten()(item_embedding)\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        concat = Concatenate()([user_flat, item_flat])\n",
    "        \n",
    "        # Dense layers with dropout and regularization\n",
    "        dense1 = Dense(64, \n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=l2(0.01))(concat)\n",
    "        dropout1 = Dropout(0.2)(dense1)\n",
    "        \n",
    "        dense2 = Dense(32, \n",
    "                      activation='relu',\n",
    "                      kernel_regularizer=l2(0.01))(dropout1)\n",
    "        dropout2 = Dropout(0.2)(dense2)\n",
    "        \n",
    "        output = Dense(1, activation='sigmoid')(dropout2)\n",
    "        \n",
    "        model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "        \n",
    "        # Use a lower learning rate\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.0005),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def train(self, train_data, validation_split=0.2, batch_size=2048, epochs=10):\n",
    "        \"\"\"\n",
    "        Train the model with improved callbacks\n",
    "        \"\"\"\n",
    "        # Prepare training data\n",
    "        X = train_data[['user_idx', 'item_idx']].values\n",
    "        y = train_data['label'].values\n",
    "        \n",
    "        # Split training data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=validation_split, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Modified early stopping\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True,\n",
    "            min_delta=0.001\n",
    "        )\n",
    "        \n",
    "        # Add learning rate reduction\n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=0.0001\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            [X_train[:, 0], X_train[:, 1]], \n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=([X_val[:, 0], X_val[:, 1]], y_val),\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self, test_df, train_df, K=10):\n",
    "        \"\"\"\n",
    "        Evaluate the model using Precision@K, Recall@K, and NDCG@K\n",
    "        \"\"\"\n",
    "        print(\"Evaluating model...\")\n",
    "        # Create user-item interactions dictionaries\n",
    "        test_user_items = test_df.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
    "        train_user_items = train_df.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
    "        \n",
    "        # All users in test set\n",
    "        all_users = test_df['user_idx'].unique()\n",
    "        \n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        ndcgs = []\n",
    "        \n",
    "        num_users = len(all_users)\n",
    "        for i, user in enumerate(all_users):\n",
    "            # Items the user interacted with in the test set\n",
    "            true_items = test_user_items.get(user, set())\n",
    "\n",
    "            # Skip if no true items\n",
    "            if len(true_items) == 0:\n",
    "                continue\n",
    "\n",
    "            # Items the user interacted with in the training set\n",
    "            train_items = train_user_items.get(user, set())\n",
    "\n",
    "            # Candidate items are all items not seen in training\n",
    "            candidate_items = list(set(range(self.n_items)) - train_items)\n",
    "\n",
    "            # Predict scores for candidate items\n",
    "            user_array = np.full(len(candidate_items), user)\n",
    "            predictions = self.model.predict([user_array, np.array(candidate_items)], verbose=0)\n",
    "\n",
    "            # Map items to their predicted scores\n",
    "            item_scores = list(zip(candidate_items, predictions.flatten()))\n",
    "\n",
    "            # Sort items by score in descending order\n",
    "            item_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Get top K items and their scores\n",
    "            top_K_items = [item for item, score in item_scores[:K]]\n",
    "\n",
    "            # Calculate Precision@K\n",
    "            num_relevant_items = len(set(top_K_items) & true_items)\n",
    "            precision_at_K = num_relevant_items / K\n",
    "            precisions.append(precision_at_K)\n",
    "            \n",
    "            # Calculate Recall@K\n",
    "            recall_at_K = num_relevant_items / len(true_items)\n",
    "            recalls.append(recall_at_K)\n",
    "            \n",
    "            # Calculate NDCG@K manually\n",
    "            dcg = 0.0\n",
    "            for idx, item in enumerate(top_K_items):\n",
    "                if item in true_items:\n",
    "                    dcg += 1 / np.log2(idx + 2)\n",
    "            # Ideal DCG\n",
    "            ideal_dcg = 0.0\n",
    "            ideal_relevance = min(len(true_items), K)\n",
    "            for idx in range(ideal_relevance):\n",
    "                ideal_dcg += 1 / np.log2(idx + 2)\n",
    "            ndcg_at_K = dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
    "            ndcgs.append(ndcg_at_K)\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Evaluated {i + 1}/{num_users} users\")\n",
    "\n",
    "        avg_precision = np.mean(precisions)\n",
    "        avg_recall = np.mean(recalls)\n",
    "        avg_ndcg = np.mean(ndcgs)\n",
    "\n",
    "        print(f\"\\nPrecision@{K}: {avg_precision:.4f}\")\n",
    "        print(f\"Recall@{K}: {avg_recall:.4f}\")\n",
    "        print(f\"NDCG@{K}: {avg_ndcg:.4f}\")\n",
    "        \n",
    "        return avg_precision, avg_recall, avg_ndcg\n",
    "\n",
    "    def get_recommendations(self, user_id, top_n=5):\n",
    "        \"\"\"\n",
    "        Get recommendations for a specific user\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_encoder.classes_:\n",
    "            return \"User not found in training data\"\n",
    "        \n",
    "        user_idx = self.user_encoder.transform([user_id])[0]\n",
    "        \n",
    "        # Create test data for all items\n",
    "        test_user = np.array([user_idx] * self.n_items)\n",
    "        test_items = np.array(range(self.n_items))\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = self.model.predict([test_user, test_items], verbose=0)\n",
    "        \n",
    "        # Get top N recommendations\n",
    "        top_indices = predictions.flatten().argsort()[-top_n:][::-1]\n",
    "        \n",
    "        # Convert back to original item IDs and include scores\n",
    "        recommendations = [\n",
    "            (self.item_encoder.inverse_transform([idx])[0], float(predictions[idx]))\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Test the improved version\n",
    "def test_improved_model(df):\n",
    "    # Initialize the improved model\n",
    "    improved_recommender = ImprovedNCF()\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Filter test set to only include users and items present in training data\n",
    "    test_df = test_df[test_df['userid_DI'].isin(train_df['userid_DI'].unique())]\n",
    "    test_df = test_df[test_df['course_id'].isin(train_df['course_id'].unique())]\n",
    "    \n",
    "    # Fit encoders on training data\n",
    "    improved_recommender.fit_encoders(train_df)\n",
    "    \n",
    "    # Preprocess data\n",
    "    train_df = improved_recommender.preprocess_data(train_df)\n",
    "    test_df = improved_recommender.preprocess_data(test_df)\n",
    "    \n",
    "    # Generate training data with reduced negative samples\n",
    "    training_data = improved_recommender.generate_negative_samples(train_df)\n",
    "    \n",
    "    # Build and train the model\n",
    "    improved_recommender.build_model()\n",
    "    history = improved_recommender.train(training_data)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    precision, recall, ndcg = improved_recommender.evaluate_model(test_df, train_df, K=10)\n",
    "    \n",
    "    # Get recommendations for a sample user\n",
    "    sample_user = df['userid_DI'].iloc[0]\n",
    "    recommendations = improved_recommender.get_recommendations(sample_user)\n",
    "    \n",
    "    print(\"\\nSample recommendations for user:\", sample_user)\n",
    "    for course, score in recommendations:\n",
    "        print(f\"Course: {course}, Score: {score:.4f}\")\n",
    "    \n",
    "    return improved_recommender, history\n",
    "\n",
    "# Run the test\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    df = pd.read_csv('cleaned_dataset.csv')  \n",
    "    \n",
    "    # Test the improved model\n",
    "    improved_model, history = test_improved_model(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b78f2f-a159-4b9f-8390-417301694a47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
