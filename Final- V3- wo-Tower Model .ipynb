{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7069bb9-0a10-499b-ab5f-09ce2a0074be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 222\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# Replace this with your actual data loading code\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m#df = pd.DataFrame({\u001b[39;00m\n\u001b[0;32m    216\u001b[0m        \u001b[38;5;66;03m# 'user_id': [...],  # Your user IDs\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# Initialize preprocessor\u001b[39;00m\n\u001b[0;32m    221\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m DataPreprocessor()\n\u001b[1;32m--> 222\u001b[0m df_processed, user_features, item_features \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mpreprocess_data(df)\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;66;03m# Split data\u001b[39;00m\n\u001b[0;32m    225\u001b[0m train_df, val_df \u001b[38;5;241m=\u001b[39m train_test_split(df_processed, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "\n",
    "# 1. Data Preprocessing\n",
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.item_encoder = LabelEncoder()\n",
    "        \n",
    "    def preprocess_data(self, df):\n",
    "        # Encode user and item IDs\n",
    "        df['user_id_encoded'] = self.user_encoder.fit_transform(df['user_id'])\n",
    "        df['item_id_encoded'] = self.item_encoder.fit_transform(df['item_id'])\n",
    "        \n",
    "        # Create user features\n",
    "        user_features = pd.DataFrame({\n",
    "            'user_id': df['user_id_encoded'].unique()\n",
    "        })\n",
    "        \n",
    "        # Create item features\n",
    "        item_features = pd.DataFrame({\n",
    "            'item_id': df['item_id_encoded'].unique()\n",
    "        })\n",
    "        \n",
    "        return df, user_features, item_features\n",
    "\n",
    "# 2. Two-Tower Model Definition\n",
    "class TwoTowerModel(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64, hidden_units=[32, 16]):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        \n",
    "        # User tower\n",
    "        self.user_tower = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(1,)),\n",
    "            tf.keras.layers.Embedding(num_users, embedding_dim),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(hidden_units[0], activation='relu'),\n",
    "            tf.keras.layers.Dense(hidden_units[1], activation='relu'),\n",
    "            tf.keras.layers.Dense(embedding_dim),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "        ])\n",
    "        \n",
    "        # Item tower\n",
    "        self.item_tower = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(1,)),\n",
    "            tf.keras.layers.Embedding(num_items, embedding_dim),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(hidden_units[0], activation='relu'),\n",
    "            tf.keras.layers.Dense(hidden_units[1], activation='relu'),\n",
    "            tf.keras.layers.Dense(embedding_dim),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_input, item_input = inputs\n",
    "        user_embedding = self.user_tower(user_input)\n",
    "        item_embedding = self.item_tower(item_input)\n",
    "        \n",
    "        # Compute dot product similarity\n",
    "        return tf.reduce_sum(user_embedding * item_embedding, axis=1)\n",
    "    \n",
    "    def get_user_embeddings(self, user_ids):\n",
    "        return self.user_tower(user_ids)\n",
    "    \n",
    "    def get_item_embeddings(self, item_ids):\n",
    "        return self.item_tower(item_ids)\n",
    "\n",
    "# 3. Training Data Generator\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, batch_size=1024, num_negatives=4):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.num_negatives = num_negatives\n",
    "        self.num_items = df['item_id_encoded'].nunique()\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) * (1 + self.num_negatives) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.batch_size\n",
    "        end_idx = min((idx + 1) * self.batch_size, len(self.df) * (1 + self.num_negatives))\n",
    "        \n",
    "        batch_users = []\n",
    "        batch_items = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        for i in range(start_idx, end_idx):\n",
    "            if i < len(self.df):\n",
    "                # Positive sample\n",
    "                user = self.df.iloc[i]['user_id_encoded']\n",
    "                item = self.df.iloc[i]['item_id_encoded']\n",
    "                batch_users.append(user)\n",
    "                batch_items.append(item)\n",
    "                batch_labels.append(1.0)\n",
    "                \n",
    "                # Negative samples\n",
    "                for _ in range(self.num_negatives):\n",
    "                    neg_item = np.random.randint(0, self.num_items)\n",
    "                    while neg_item in self.df[self.df['user_id_encoded'] == user]['item_id_encoded'].values:\n",
    "                        neg_item = np.random.randint(0, self.num_items)\n",
    "                    batch_users.append(user)\n",
    "                    batch_items.append(neg_item)\n",
    "                    batch_labels.append(0.0)\n",
    "        \n",
    "        return [np.array(batch_users), np.array(batch_items)], np.array(batch_labels)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 4. Training and Evaluation Functions\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, learning_rate=0.001):\n",
    "        self.model = model\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "        self.loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.train_loss = tf.keras.metrics.Mean()\n",
    "        self.train_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model(x)\n",
    "            loss = self.loss_fn(y, logits)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        \n",
    "        self.train_loss(loss)\n",
    "        self.train_accuracy(y, tf.sigmoid(logits))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self, train_generator, val_generator, epochs=10):\n",
    "        history = {\n",
    "            'loss': [],\n",
    "            'accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': [],\n",
    "            'time_per_epoch': []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Training\n",
    "            for batch_idx, (x, y) in enumerate(train_generator):\n",
    "                loss = self.train_step(x, y)\n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(f\"Batch {batch_idx}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            # Validation\n",
    "            val_loss = 0\n",
    "            val_accuracy = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for x_val, y_val in val_generator:\n",
    "                val_logits = self.model(x_val)\n",
    "                val_loss += self.loss_fn(y_val, val_logits)\n",
    "                val_accuracy += tf.keras.metrics.binary_accuracy(y_val, tf.sigmoid(val_logits))\n",
    "                num_batches += 1\n",
    "            \n",
    "            val_loss /= num_batches\n",
    "            val_accuracy /= num_batches\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            \n",
    "            # Store metrics\n",
    "            history['loss'].append(self.train_loss.result().numpy())\n",
    "            history['accuracy'].append(self.train_accuracy.result().numpy())\n",
    "            history['val_loss'].append(val_loss.numpy())\n",
    "            history['val_accuracy'].append(val_accuracy.numpy())\n",
    "            history['time_per_epoch'].append(epoch_time)\n",
    "            \n",
    "            print(f\"Training Loss: {self.train_loss.result():.4f}\")\n",
    "            print(f\"Training Accuracy: {self.train_accuracy.result():.4f}\")\n",
    "            print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "            print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"Time taken: {epoch_time:.2f}s\")\n",
    "            \n",
    "            # Reset metrics\n",
    "            self.train_loss.reset_states()\n",
    "            self.train_accuracy.reset_states()\n",
    "        \n",
    "        return history\n",
    "\n",
    "# 5. Recommendation Generator\n",
    "class RecommendationGenerator:\n",
    "    def __init__(self, model, num_recommendations=5):\n",
    "        self.model = model\n",
    "        self.num_recommendations = num_recommendations\n",
    "    \n",
    "    def generate_recommendations(self, user_id, item_ids):\n",
    "        user_embedding = self.model.get_user_embeddings(tf.constant([user_id]))\n",
    "        item_embeddings = self.model.get_item_embeddings(tf.constant(item_ids))\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = tf.matmul(user_embedding, tf.transpose(item_embeddings))\n",
    "        similarities = tf.squeeze(similarities)\n",
    "        \n",
    "        # Get top k recommendations\n",
    "        _, indices = tf.nn.top_k(similarities, k=self.num_recommendations)\n",
    "        \n",
    "        return indices.numpy(), similarities[indices].numpy()\n",
    "\n",
    "# 6. Main Training Pipeline\n",
    "def main():\n",
    "   df = pd.read_csv('cleaned_dataset.csv')  # Load your data\n",
    "    # Replace this with your actual data loading code\n",
    "    #df = pd.DataFrame({\n",
    "       # 'user_id': [...],  # Your user IDs\n",
    "       #'item_id': [...],  # Your item IDs\n",
    "   # })\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "df_processed, user_features, item_features = preprocessor.preprocess_data(df)\n",
    "    \n",
    "    # Split data\n",
    "train_df, val_df = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create data generators\n",
    "train_generator = DataGenerator(train_df)\n",
    "val_generator = DataGenerator(val_df)\n",
    "    \n",
    "    # Initialize model\n",
    "model = TwoTowerModel(\n",
    "    num_users=len(user_features),\n",
    "    num_items=len(item_features),\n",
    "    embedding_dim=64,\n",
    "    hidden_units=[32, 16]\n",
    ")\n",
    "    \n",
    "    # Initialize trainer\n",
    "trainer = ModelTrainer(model)\n",
    "    \n",
    "    # Train model\n",
    "history = trainer.train(train_generator, val_generator, epochs=10)\n",
    "    \n",
    "    # Initialize recommendation generator\n",
    "recommender = RecommendationGenerator(model)\n",
    "    \n",
    "    # Generate recommendations for a sample user\n",
    "sample_user_id = 0\n",
    "all_item_ids = np.arange(len(item_features))\n",
    "rec_indices, rec_scores = recommender.generate_recommendations(sample_user_id, all_item_ids)\n",
    "    \n",
    "    # Print recommendations\n",
    "print(\"\\nTop 5 recommendations for user\", preprocessor.user_encoder.inverse_transform([sample_user_id])[0])\n",
    "for idx, score in zip(rec_indices, rec_scores):\n",
    "    item_id = preprocessor.item_encoder.inverse_transform([idx])[0]\n",
    "    print(f\"Item {item_id}: Score {score:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d8975-6d95-4b33-bb2e-26e7c6097faa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
